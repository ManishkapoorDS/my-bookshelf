abstract: Sequential decision making problems, such as structured prediction, robotic
  control, and game playing, require a combination of planning policies and generalisation
  of those plans. In this paper, we present Expert Iteration (ExIt), a novel reinforcement
  learning algorithm which decomposes the problem into separate planning and generalisation
  tasks. Planning new policies is performed by tree search, while a deep neural network
  generalises those plans. Subsequently, tree search is improved by using the neural
  network policy to guide search, increasing the strength of new plans. In contrast,
  standard deep Reinforcement Learning algorithms rely on a neural network not only
  to generalise plans, but to discover them too. We show that ExIt outperforms REINFORCE
  for training a neural network to play the board game Hex, and our final tree search
  agent, trained tabula rasa, defeats MoHex 1.0, the most recent Olympiad Champion
  player to be publicly released.
archiveprefix: arXiv
author: Anthony, Thomas and Tian, Zheng and Barber, David
author_list:
- family: Anthony
  given: Thomas
- family: Tian
  given: Zheng
- family: Barber
  given: David
eprint: 1705.08439v4
file: 1705.08439v4.pdf
files:
- anthony-thomas-and-tian-zheng-and-barber-davidthinking-fast-and-slow-with-deep-learning-and-tree-search2017.pdf
month: May
primaryclass: cs.AI
ref: 1705.08439v4
time-added: 2020-10-29-18:39:33
title: Thinking Fast and Slow with Deep Learning and Tree Search
type: article
url: http://arxiv.org/abs/1705.08439v4
year: '2017'
