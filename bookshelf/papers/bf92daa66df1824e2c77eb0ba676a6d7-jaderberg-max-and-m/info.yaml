abstract: Deep reinforcement learning agents have achieved state-of-the-art results
  by directly maximising cumulative reward. However, environments contain a much wider
  variety of possible training signals. In this paper, we introduce an agent that
  also maximises many other pseudo-reward functions simultaneously by reinforcement
  learning. All of these tasks share a common representation that, like unsupervised
  learning, continues to develop in the absence of extrinsic rewards. We also introduce
  a novel mechanism for focusing this representation upon extrinsic rewards, so that
  learning can rapidly adapt to the most relevant aspects of the actual task. Our
  agent significantly outperforms the previous state-of-the-art on Atari, averaging
  880\% expert human performance, and a challenging suite of first-person, three-dimensional
  \emph{Labyrinth} tasks leading to a mean speedup in learning of 10$\times$ and averaging
  87\% expert human performance on Labyrinth.
archiveprefix: arXiv
author: Jaderberg, Max and Mnih, Volodymyr and Czarnecki, Wojciech Marian and Schaul,
  Tom and Leibo, Joel Z and Silver, David and Kavukcuoglu, Koray
author_list:
- family: Jaderberg
  given: Max
- family: Mnih
  given: Volodymyr
- family: Czarnecki
  given: Wojciech Marian
- family: Schaul
  given: Tom
- family: Leibo
  given: Joel Z
- family: Silver
  given: David
- family: Kavukcuoglu
  given: Koray
eprint: 1611.05397v1
file: 1611.05397v1.pdf
files:
- jaderberg-max-and-mnih-volodymyr-and-czarnecki-wojciech-marian-and-schaul-tom-and-leibo-joel-z-and-silver-david-and-kavukcuoglu-korayreinforcem.pdf
month: Nov
primaryclass: cs.LG
ref: 1611.05397v1
tags: reinforcement-learning
title: Reinforcement Learning with Unsupervised Auxiliary Tasks
type: article
url: http://arxiv.org/abs/1611.05397v1
year: '2016'
