abstract: 'A grand challenge in machine learning is the development of computational
  algorithms that match or outperform humans in perceptual inference tasks that are
  complicated by nuisance variation. For instance, visual object recognition involves
  the unknown object position, orientation, and scale in object recognition while
  speech recognition involves the unknown voice pronunciation, pitch, and speed. Recently,
  a new breed of deep learning algorithms have emerged for high-nuisance inference
  tasks that routinely yield pattern recognition systems with near- or super-human
  capabilities. But a fundamental question remains: Why do they work? Intuitions abound,
  but a coherent framework for understanding, analyzing, and synthesizing deep learning
  architectures has remained elusive. We answer this question by developing a new
  probabilistic framework for deep learning based on the Deep Rendering Model: a generative
  probabilistic model that explicitly captures latent nuisance variation. By relaxing
  the generative model to a discriminative one, we can recover two of the current
  leading deep learning systems, deep convolutional neural networks and random decision
  forests, providing insights into their successes and shortcomings, as well as a
  principled route to their improvement.'
archiveprefix: arXiv
author: Patel, Ankit B. and Nguyen, Tan and Baraniuk, Richard G.
author_list:
- family: Patel
  given: Ankit B.
- family: Nguyen
  given: Tan
- family: Baraniuk
  given: Richard G.
eprint: 1504.00641v1
file: 1504.00641v1.pdf
files:
- patel-ankit-b.-and-nguyen-tan-and-baraniuk-richard-g.a-probabilistic-theory-of-deep-learning2015.pdf
month: Apr
primaryclass: stat.ML
ref: 1504.00641v1
time-added: 2020-06-24-12:06:30
title: A Probabilistic Theory of Deep Learning
type: article
url: http://arxiv.org/abs/1504.00641v1
year: '2015'
