<!DOCTYPE html>
<html lang="en" class="no-js">
<head>
    <meta charset="UTF-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="access" content="Yes">

    

    <meta name="journal_id" content="10994"/>

    <meta name="dc.title" content="Spectral learning of weighted automata"/>

    <meta name="dc.source" content="Machine Learning 2013 96:1"/>

    <meta name="dc.format" content="text/html"/>

    <meta name="dc.publisher" content="Springer"/>

    <meta name="dc.date" content="2013-10-03"/>

    <meta name="dc.type" content="OriginalPaper"/>

    <meta name="dc.language" content="En"/>

    <meta name="dc.copyright" content="2013 The Author(s)"/>

    <meta name="dc.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="dc.description" content="In recent years we have seen the development of efficient provably correct algorithms for learning Weighted Finite Automata (WFA). Most of these algorithms avoid the known hardness results by defining parameters beyond the number of states that can be used to quantify the complexity of learning automata under a particular distribution. One such class of methods are the so-called spectral algorithms that measure learning complexity in terms of the smallest singular value of some Hankel matrix. However, despite their simplicity and wide applicability to real problems, their impact in application domains remains marginal to this date. One of the goals of this paper is to remedy this situation by presenting a derivation of the spectral method for learning WFA that&#8212;without sacrificing rigor and mathematical elegance&#8212;puts emphasis on providing intuitions on the inner workings of the method and does not assume a strong background in formal algebraic methods. In addition, our algorithm overcomes some of the shortcomings of previous work and is able to learn from statistics of substrings. To illustrate the approach we present experiments on a real application of the method to natural language parsing."/>

    <meta name="prism.issn" content="1573-0565"/>

    <meta name="prism.publicationName" content="Machine Learning"/>

    <meta name="prism.publicationDate" content="2013-10-03"/>

    <meta name="prism.volume" content="96"/>

    <meta name="prism.number" content="1"/>

    <meta name="prism.section" content="OriginalPaper"/>

    <meta name="prism.startingPage" content="33"/>

    <meta name="prism.endingPage" content="63"/>

    <meta name="prism.copyright" content="2013 The Author(s)"/>

    <meta name="prism.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="prism.url" content="https://link.springer.com/article/10.1007/s10994-013-5416-x"/>

    <meta name="prism.doi" content="doi:10.1007/s10994-013-5416-x"/>

    <meta name="citation_pdf_url" content="https://link.springer.com/content/pdf/10.1007/s10994-013-5416-x.pdf"/>

    <meta name="citation_fulltext_html_url" content="https://link.springer.com/article/10.1007/s10994-013-5416-x"/>

    <meta name="citation_journal_title" content="Machine Learning"/>

    <meta name="citation_journal_abbrev" content="Mach Learn"/>

    <meta name="citation_publisher" content="Springer US"/>

    <meta name="citation_issn" content="1573-0565"/>

    <meta name="citation_title" content="Spectral learning of weighted automata"/>

    <meta name="citation_volume" content="96"/>

    <meta name="citation_issue" content="1"/>

    <meta name="citation_publication_date" content="2014/07"/>

    <meta name="citation_online_date" content="2013/10/03"/>

    <meta name="citation_firstpage" content="33"/>

    <meta name="citation_lastpage" content="63"/>

    <meta name="citation_article_type" content="Article"/>

    <meta name="citation_fulltext_world_readable" content=""/>

    <meta name="citation_language" content="en"/>

    <meta name="dc.identifier" content="doi:10.1007/s10994-013-5416-x"/>

    <meta name="DOI" content="10.1007/s10994-013-5416-x"/>

    <meta name="citation_doi" content="10.1007/s10994-013-5416-x"/>

    <meta name="description" content="In recent years we have seen the development of efficient provably correct algorithms for learning Weighted Finite Automata (WFA). Most of these algorithms"/>

    <meta name="dc.creator" content="Borja Balle"/>

    <meta name="dc.creator" content="Xavier Carreras"/>

    <meta name="dc.creator" content="Franco M. Luque"/>

    <meta name="dc.creator" content="Ariadna Quattoni"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Control, Robotics, Mechatronics"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Simulation and Modeling"/>

    <meta name="dc.subject" content="Natural Language Processing (NLP)"/>

    <meta name="citation_reference" content="citation_title=A spectral algorithm for latent Dirichlet allocation; citation_inbook_title=NIPS; citation_publication_date=2012; citation_pages=926-934; citation_id=CR1; citation_author=A. Anandkumar; citation_author=D. P. Foster; citation_author=D. Hsu; citation_author=S. Kakade; citation_author=Y. K. Liu"/>

    <meta name="citation_reference" content="
Anandkumar, A., Ge, R., Hsu, D., Kakade, S. M., &amp; Telgarsky, M. (2012b). Tensor decompositions for learning latent variable models.
					                      
                    arXiv:1210.7559
                    
                  .
"/>

    <meta name="citation_reference" content="citation_journal_title=Journal of Machine Learning Research&#8212;Proceedings Track; citation_title=A method of moments for mixture models and hidden Markov models; citation_author=A. Anandkumar, D. Hsu, S. M. Kakade; citation_volume=23; citation_publication_date=2012; citation_pages=33.1-33.34; citation_id=CR3"/>

    <meta name="citation_reference" content="citation_journal_title=Journal of Machine Learning Research&#8212;Proceedings Track; citation_title=Quadratic weighted automata: spectral algorithm and likelihood maximization; citation_author=R. Bailly; citation_volume=20; citation_publication_date=2011; citation_pages=147-163; citation_id=CR4"/>

    <meta name="citation_reference" content="citation_title=Grammatical inference as a principal component analysis problem; citation_inbook_title=Proceedings of the 26th international conference on machine learning; citation_publication_date=2009; citation_pages=33-40; citation_id=CR5; citation_author=R. Bailly; citation_author=F. Denis; citation_author=L. Ralaivola; citation_publisher=Omnipress"/>

    <meta name="citation_reference" content="citation_title=A spectral approach for probabilistic grammatical inference on trees; citation_inbook_title=Lecture notes in computer science; citation_publication_date=2010; citation_pages=74-88; citation_id=CR6; citation_author=R. Bailly; citation_author=A. Habrard; citation_author=F. Denis; citation_publisher=Springer"/>

    <meta name="citation_reference" content="citation_title=Trainable grammars for speech recognition; citation_inbook_title=Speech communication papers for the 97th meeting of the Acoustical Society of America; citation_publication_date=1979; citation_pages=547-550; citation_id=CR7; citation_author=J. K. Baker"/>

    <meta name="citation_reference" content="
Balle, B. (2013). Learning finite-state machines: algorithmic and statistical aspects. PhD thesis, Universitat Polit&#232;cnica de Catalunya.
"/>

    <meta name="citation_reference" content="citation_title=Spectral learning of general weighted automata via constrained matrix completion; citation_inbook_title=Advances in neural information processing systems; citation_publication_date=2012; citation_pages=2168-2176; citation_id=CR9; citation_author=B. Balle; citation_author=M. Mohri"/>

    <meta name="citation_reference" content="citation_title=A spectral learning algorithm for finite state transducers; citation_inbook_title=ECML/PKDD (1); citation_publication_date=2011; citation_pages=156-171; citation_id=CR10; citation_author=B. Balle; citation_author=A. Quattoni; citation_author=X. Carreras; citation_publisher=Springer"/>

    <meta name="citation_reference" content="citation_title=Local loss optimization in operator models: a new insight into spectral learning; citation_inbook_title=Proceedings of the 29th international conference on machine learning (ICML-2012), ICML&#8217;12; citation_publication_date=2012; citation_pages=1879-1886; citation_id=CR11; citation_author=B. Balle; citation_author=A. Quattoni; citation_author=X. Carreras; citation_publisher=Omnipress"/>

    <meta name="citation_reference" content="citation_journal_title=Theoretical Computer Science; citation_title=Learning probabilistic automata: a study in state distinguishability; citation_author=B. Balle, J. Castro, R. Gavald&#224;; citation_volume=473; citation_publication_date=2013; citation_pages=46-60; citation_doi=10.1016/j.tcs.2012.10.009; citation_id=CR12"/>

    <meta name="citation_reference" content="citation_journal_title=Journal of the ACM; citation_title=Learning functions represented as multiplicity automata; citation_author=A. Beimel, F. Bergadano, N. Bshouty, E. Kushilevitz, S. Varricchio; citation_volume=47; citation_publication_date=2000; citation_pages=506-530; citation_doi=10.1145/337244.337257; citation_id=CR13"/>

    <meta name="citation_reference" content="citation_title=Rational series and their languages; citation_publication_date=1988; citation_id=CR14; citation_author=J. Berstel; citation_author=C. Reutenauer; citation_publisher=Springer"/>

    <meta name="citation_reference" content="citation_journal_title=The International Journal of Robotics Research; citation_title=Closing the learning planning loop with predictive state representations; citation_author=B. Boots, S. Siddiqi, G. Gordon; citation_volume=30; citation_issue=7; citation_publication_date=2011; citation_pages=954-966; citation_doi=10.1177/0278364911404092; citation_id=CR15"/>

    <meta name="citation_reference" content="citation_journal_title=Journal of Computer and System Sciences; citation_title=Realizations by stochastic finite automata; citation_author=J. W. Carlyle, A. Paz; citation_volume=5; citation_issue=1; citation_publication_date=1971; citation_pages=26-40; citation_doi=10.1016/S0022-0000(71)80005-3; citation_id=CR16"/>

    <meta name="citation_reference" content="citation_title=Towards feasible PAC-learning of probabilistic deterministic finite automata; citation_inbook_title=Proceedings of the 9th international colloquium on grammatical inference (ICGI); citation_publication_date=2008; citation_pages=163-174; citation_id=CR17; citation_author=J. Castro; citation_author=R. Gavald&#224;"/>

    <meta name="citation_reference" content="citation_journal_title=Journal of Machine Learning Research; citation_title=PAC-learnability of probabilistic deterministic finite state automata; citation_author=A. Clark, F. Thollard; citation_volume=5; citation_publication_date=2004; citation_pages=473-497; citation_id=CR18"/>

    <meta name="citation_reference" content="citation_title=Parsing the WSJ using CCG and log-linear models; citation_inbook_title=Proceedings of the 42nd meeting of the association for computational linguistics (ACL&#8217;04), main volume; citation_publication_date=2004; citation_pages=103-110; citation_id=CR19; citation_author=S. Clark; citation_author=J. R. Curran"/>

    <meta name="citation_reference" content="citation_title=Spectral learning of latent-variable PCFGS; citation_inbook_title=Proceedings of the 50th annual meeting of the association for computational linguistics (Volume 1: Long papers); citation_publication_date=2012; citation_pages=223-231; citation_id=CR20; citation_author=S. B. Cohen; citation_author=K. Stratos; citation_author=M. Collins; citation_author=D. P. Foster; citation_author=L. Ungar; citation_publisher=Association for Computational Linguistics"/>

    <meta name="citation_reference" content="citation_title=Experiments with spectral learning of latent-variable pcfgs; citation_inbook_title=Proceedings of the 2013 conference of the North American chapter of the Association for Computational Linguistics: human language technologies; citation_publication_date=2013; citation_pages=148-157; citation_id=CR21; citation_author=S. B. Cohen; citation_author=K. Stratos; citation_author=M. Collins; citation_author=D. P. Foster; citation_author=L. Ungar; citation_publisher=Association for Computational Linguistics"/>

    <meta name="citation_reference" content="citation_journal_title=Journal of the Royal Statistical Society; citation_title=Maximum likelihood from incomplete data via the EM algorithm; citation_author=A. P. Dempster, N. M. Laird, D. B. Rubin; citation_volume=39; citation_issue=1; citation_publication_date=1977; citation_pages=1-38; citation_id=CR22"/>

    <meta name="citation_reference" content="citation_journal_title=Fundamenta Informaticae; citation_title=On rational stochastic languages; citation_author=F. Denis, Y. Esposito; citation_volume=86; citation_issue=1&#8211;2; citation_publication_date=2008; citation_pages=41-77; citation_id=CR23"/>

    <meta name="citation_reference" content="citation_title=Spectral dependency parsing with latent variables; citation_inbook_title=Proceedings of the 2012 joint conference on empirical methods in natural language processing and computational natural language learning; citation_publication_date=2012; citation_pages=205-213; citation_id=CR24; citation_author=P. Dhillon; citation_author=J. Rodu; citation_author=M. Collins; citation_author=D. Foster; citation_author=L. Ungar; citation_publisher=Association for Computational Linguistics"/>

    <meta name="citation_reference" content="citation_title=Bilexical grammars and their cubic-time parsing algorithms; citation_inbook_title=Advances in probabilistic and other parsing technologies; citation_publication_date=2000; citation_pages=29-62; citation_id=CR25; citation_author=J. Eisner; citation_publisher=Kluwer Academic"/>

    <meta name="citation_reference" content="citation_title=Efficient parsing for bilexical context-free grammars and head-automaton grammars; citation_inbook_title=Proceedings of the 37th annual meeting of the Association for Computational Linguistics (ACL), University of Maryland; citation_publication_date=1999; citation_pages=457-464; citation_id=CR26; citation_author=J. Eisner; citation_author=G. Satta"/>

    <meta name="citation_reference" content="citation_title=Favor short dependencies: parsing with soft and hard constraints on dependency length; citation_inbook_title=Trends in parsing technology: dependency parsing, domain adaptation, and deep parsing; citation_publication_date=2010; citation_pages=121-150; citation_id=CR27; citation_author=J. Eisner; citation_author=N. A. Smith; citation_publisher=Springer"/>

    <meta name="citation_reference" content="citation_journal_title=Journal de Math&#233;matiques Pures et Appliqu&#233;es; citation_title=Matrices de Hankel; citation_author=M. Fliess; citation_volume=53; citation_publication_date=1974; citation_pages=197-222; citation_id=CR28"/>

    <meta name="citation_reference" content="citation_title=Parsing algorithms and metrics; citation_inbook_title=Proceedings of the 34th annual meeting of the Association for Computational Linguistics; citation_publication_date=1996; citation_pages=177-183; citation_id=CR29; citation_author=J. Goodman; citation_publisher=Association for Computational Linguistics"/>

    <meta name="citation_reference" content="citation_title=A spectral algorithm for learning hidden Markov models; citation_inbook_title=Proceedings of the annual conference on computational learning theory (COLT); citation_publication_date=2009; citation_id=CR30; citation_author=D. Hsu; citation_author=S. M. Kakade; citation_author=T. Zhang"/>

    <meta name="citation_reference" content="citation_title=Identifiability and unmixing of latent parse trees; citation_publication_date=2012; citation_id=CR31; citation_author=D. Hsu; citation_author=S. M. Kakade; citation_author=P. Liang"/>

    <meta name="citation_reference" content="citation_title=On the learnability of discrete distributions; citation_inbook_title=Proceedings of the twenty-sixth annual ACM symposium on theory of computing; citation_publication_date=1994; citation_pages=273-282; citation_id=CR32; citation_author=M. Kearns; citation_author=Y. Mansour; citation_author=D. Ron; citation_author=R. Rubinfeld; citation_author=R. E. Schapire; citation_author=L. Sellie; citation_publisher=ACM"/>

    <meta name="citation_reference" content="citation_title=Spectral learning for non-deterministic dependency parsing; citation_inbook_title=Proceedings of the 13th conference of the European chapter of the Association for Computational Linguistics; citation_publication_date=2012; citation_pages=409-419; citation_id=CR33; citation_author=F. M. Luque; citation_author=A. Quattoni; citation_author=B. Balle; citation_author=X. Carreras; citation_publisher=Association for Computational Linguistics"/>

    <meta name="citation_reference" content="citation_title=Introduction to information retrieval; citation_publication_date=2008; citation_id=CR34; citation_author=C. D. Manning; citation_author=P. Raghavan; citation_author=H. Sch&#252;tze; citation_publisher=Cambridge University Press"/>

    <meta name="citation_reference" content="citation_journal_title=Computational Linguistics; citation_title=Building a large annotated corpus of English: the Penn Treebank; citation_author=M. P. Marcus, B. Santorini, M. A. Marcinkiewicz; citation_volume=19; citation_publication_date=1993; citation_pages=313-330; citation_id=CR35"/>

    <meta name="citation_reference" content="citation_title=Online learning of approximate dependency parsing algorithms; citation_inbook_title=Proceedings of the 11th conference of the European chapter of the Association for Computational Linguistics; citation_publication_date=2006; citation_pages=81-88; citation_id=CR36; citation_author=R. McDonald; citation_author=F. Pereira"/>

    <meta name="citation_reference" content="citation_title=Non-projective dependency parsing using spanning tree algorithms; citation_inbook_title=Proceedings of human language technology conference and conference on empirical methods in natural language processing; citation_publication_date=2005; citation_pages=523-530; citation_id=CR37; citation_author=R. McDonald; citation_author=F. Pereira; citation_author=K. Ribarov; citation_author=J. Hajic; citation_publisher=Association for Computational Linguistics"/>

    <meta name="citation_reference" content="citation_title=Weighted automata algorithms; citation_inbook_title=Handbook of weighted automata; citation_publication_date=2009; citation_pages=213-254; citation_id=CR38; citation_author=M. Mohri; citation_publisher=Springer"/>

    <meta name="citation_reference" content="citation_title=Learning nonsingular phylogenies and hidden Markov models; citation_inbook_title=Proceedings of the 37th annual ACM symposium on theory of computing (STOC); citation_publication_date=2005; citation_pages=366-375; citation_id=CR39; citation_author=E. Mossel; citation_author=S. Roch"/>

    <meta name="citation_reference" content="citation_journal_title=Theoretical Computer Science; citation_title=PAC-learnability of probabilistic deterministic finite state automata in terms of variation distance; citation_author=N. Palmer, P. W. Goldberg; citation_volume=387; citation_issue=1; citation_publication_date=2007; citation_pages=18-31; citation_doi=10.1016/j.tcs.2007.07.023; citation_id=CR40"/>

    <meta name="citation_reference" content="citation_title=A spectral algorithm for latent tree graphical models; citation_inbook_title=Proceedings of the 28th international conference on machine learning, ICML 2011 (ICML); citation_publication_date=2011; citation_pages=1065-1072; citation_id=CR41; citation_author=A. Parikh; citation_author=L. Song; citation_author=E. Xing"/>

    <meta name="citation_reference" content="citation_journal_title=Journal of Artificial Intelligence Research; citation_title=Complexity results and approximation strategies for map explanations; citation_author=J. D. Park, A. Darwiche; citation_volume=21; citation_publication_date=2004; citation_pages=101-133; citation_id=CR42"/>

    <meta name="citation_reference" content="citation_title=Improved inference for unlexicalized parsing; citation_inbook_title=Human language technologies 2007: the conference of the North American chapter of the Association for Computational Linguistics; citation_publication_date=2007; citation_pages=404-411; citation_id=CR43; citation_author=S. Petrov; citation_author=D. Klein; citation_publisher=Association for Computational Linguistics"/>

    <meta name="citation_reference" content="citation_title=A universal part-of-speech tagset; citation_inbook_title=Proceedings of LREC; citation_publication_date=2012; citation_id=CR44; citation_author=S. Petrov; citation_author=D. Das; citation_author=R. McDonald"/>

    <meta name="citation_reference" content="citation_journal_title=Journal of Computing Systems Science; citation_title=On the learnability and usage of acyclic probabilistic finite automata; citation_author=D. Ron, Y. Singer, N. Tishby; citation_volume=56; citation_issue=2; citation_publication_date=1998; citation_pages=133-152; citation_doi=10.1006/jcss.1997.1555; citation_id=CR45"/>

    <meta name="citation_reference" content="citation_title=Automata-theoretic aspects of formal power series; citation_publication_date=1978; citation_id=CR46; citation_author=A. Salomaa; citation_author=M. Soittola; citation_publisher=Springer"/>

    <meta name="citation_reference" content="citation_journal_title=Information and Control; citation_title=On the definition of a family of automata; citation_author=M. Sch&#252;tzenberger; citation_volume=4; citation_publication_date=1961; citation_pages=245-270; citation_doi=10.1016/S0019-9958(61)80020-X; citation_id=CR47"/>

    <meta name="citation_reference" content="citation_title=Reduced-rank hidden Markov models; citation_inbook_title=Proceedings of the thirteenth international conference on artificial intelligence and statistics (AISTATS); citation_publication_date=2010; citation_pages=741-748; citation_id=CR48; citation_author=S. M. Siddiqi; citation_author=B. Boots; citation_author=G. J. Gordon"/>

    <meta name="citation_reference" content="citation_title=Hilbert space embeddings of hidden Markov models; citation_inbook_title=Proceedings of the 27th international conference on machine learning (ICML-10); citation_publication_date=2010; citation_pages=991-998; citation_id=CR49; citation_author=L. Song; citation_author=S. M. Siddiqi; citation_author=G. Gordon; citation_author=A. Smola; citation_publisher=Omnipress"/>

    <meta name="citation_reference" content="citation_title=Loss minimization in parse reranking; citation_inbook_title=Proceedings of the 2006 conference on empirical methods in natural language processing; citation_publication_date=2006; citation_pages=560-567; citation_id=CR50; citation_author=I. Titov; citation_author=J. Henderson; citation_publisher=Association for Computational Linguistics"/>

    <meta name="citation_reference" content="citation_journal_title=Communications of the ACM; citation_title=A theory of the learnable; citation_author=L. G. Valiant; citation_volume=27; citation_issue=11; citation_publication_date=1984; citation_pages=1134-1142; citation_doi=10.1145/1968.1972; citation_id=CR51"/>

    <meta name="citation_reference" content="citation_title=Learning predictive representations from a history; citation_inbook_title=Proceedings of the 22nd international conference on machine learning; citation_publication_date=2005; citation_pages=964-971; citation_id=CR52; citation_author=E. Wiewiora; citation_publisher=ACM"/>

    <meta name="citation_author" content="Borja Balle"/>

    <meta name="citation_author_email" content="bballe@lsi.upc.edu"/>

    <meta name="citation_author_institution" content="Universitat Polit&#232;cnica de Catalunya, Barcelona, Spain"/>

    <meta name="citation_author" content="Xavier Carreras"/>

    <meta name="citation_author_email" content="carreras@lsi.upc.edu"/>

    <meta name="citation_author_institution" content="Universitat Polit&#232;cnica de Catalunya, Barcelona, Spain"/>

    <meta name="citation_author" content="Franco M. Luque"/>

    <meta name="citation_author_email" content="francolq@famaf.unc.edu.ar"/>

    <meta name="citation_author_institution" content="Universidad Nacional de C&#243;rdoba and CONICET, C&#243;rdoba, Argentina"/>

    <meta name="citation_author" content="Ariadna Quattoni"/>

    <meta name="citation_author_email" content="aquattoni@lsi.upc.edu"/>

    <meta name="citation_author_institution" content="Universitat Polit&#232;cnica de Catalunya, Barcelona, Spain"/>

    <meta name="twitter:card" content="summary"/>

    <meta name="twitter:title" content="Spectral learning of weighted automata"/>

    <meta name="twitter:site" content="@SpringerLink"/>

    <meta name="twitter:description" content="In recent years we have seen the development of efficient provably correct algorithms for learning Weighted Finite Automata (WFA). Most of these algorithms avoid the known hardness results by defining parameters beyond the number of states that can be used to quantify the complexity of learning automata under a particular distribution. One such class of methods are the so-called spectral algorithms that measure learning complexity in terms of the smallest singular value of some Hankel matrix. However, despite their simplicity and wide applicability to real problems, their impact in application domains remains marginal to this date. One of the goals of this paper is to remedy this situation by presenting a derivation of the spectral method for learning WFA that&#8212;without sacrificing rigor and mathematical elegance&#8212;puts emphasis on providing intuitions on the inner workings of the method and does not assume a strong background in formal algebraic methods. In addition, our algorithm overcomes some of the shortcomings of previous work and is able to learn from statistics of substrings. To illustrate the approach we present experiments on a real application of the method to natural language parsing."/>

    <meta name="twitter:image" content="https://static-content.springer.com/cover/journal/10994/96/1.jpg"/>

    <meta name="twitter:image:alt" content="Content cover image"/>

    <meta name="citation_springer_api_url" content="http://api.springer.com/metadata/pam?q=doi:10.1007/s10994-013-5416-x&amp;api_key="/>

    <meta name="format-detection" content="telephone=no"/>

    <meta name="citation_cover_date" content="2014/07/01"/>


    
        <meta property="og:url" content="https://link.springer.com/article/10.1007/s10994-013-5416-x"/>
        <meta property="og:type" content="article"/>
        <meta property="og:site_name" content="Machine Learning"/>
        <meta property="og:title" content="Spectral learning of weighted automata"/>
        <meta property="og:description" content="In recent years we have seen the development of efficient provably correct algorithms for learning Weighted Finite Automata (WFA). Most of these algorithms avoid the known hardness results by defining parameters beyond the number of states that can be used to quantify the complexity of learning automata under a particular distribution. One such class of methods are the so-called spectral algorithms that measure learning complexity in terms of the smallest singular value of some Hankel matrix. However, despite their simplicity and wide applicability to real problems, their impact in application domains remains marginal to this date. One of the goals of this paper is to remedy this situation by presenting a derivation of the spectral method for learning WFA that—without sacrificing rigor and mathematical elegance—puts emphasis on providing intuitions on the inner workings of the method and does not assume a strong background in formal algebraic methods. In addition, our algorithm overcomes some of the shortcomings of previous work and is able to learn from statistics of substrings. To illustrate the approach we present experiments on a real application of the method to natural language parsing."/>
        <meta property="og:image" content="https://media.springernature.com/w110/springer-static/cover/journal/10994.jpg"/>
    

    <title>Spectral learning of weighted automata | SpringerLink</title>

    <link rel="shortcut icon" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16 32x32 48x48" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-16x16-8bd8c1c945.png />
<link rel="icon" sizes="32x32" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-32x32-61a52d80ab.png />
<link rel="icon" sizes="48x48" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-48x48-0ec46b6b10.png />
<link rel="apple-touch-icon" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />
<link rel="apple-touch-icon" sizes="72x72" href=/oscar-static/images/favicons/springerlink/ic_launcher_hdpi-f77cda7f65.png />
<link rel="apple-touch-icon" sizes="76x76" href=/oscar-static/images/favicons/springerlink/app-icon-ipad-c3fd26520d.png />
<link rel="apple-touch-icon" sizes="114x114" href=/oscar-static/images/favicons/springerlink/app-icon-114x114-3d7d4cf9f3.png />
<link rel="apple-touch-icon" sizes="120x120" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@2x-67b35150b3.png />
<link rel="apple-touch-icon" sizes="144x144" href=/oscar-static/images/favicons/springerlink/ic_launcher_xxhdpi-986442de7b.png />
<link rel="apple-touch-icon" sizes="152x152" href=/oscar-static/images/favicons/springerlink/app-icon-ipad@2x-677ba24d04.png />
<link rel="apple-touch-icon" sizes="180x180" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />


    
    <script>(function(H){H.className=H.className.replace(/\bno-js\b/,'js')})(document.documentElement)</script>

    <link rel="stylesheet" href=/oscar-static/app-springerlink/css/core-article-c36432aacc.css media="screen">
    <link rel="stylesheet" id="js-mustard" href=/oscar-static/app-springerlink/css/enhanced-article-b62189e5ed.css media="only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)">
    

    
    <script type="text/javascript">
        window.dataLayer = [{"Country":"IT","doi":"10.1007-s10994-013-5416-x","Journal Title":"Machine Learning","Journal Id":10994,"Keywords":"Spectral learning, Weighted finite automata, Dependency parsing","kwrd":["Spectral_learning","Weighted_finite_automata","Dependency_parsing"],"Labs":"Y","ksg":"Krux.segments","kuid":"Krux.uid","Has Body":"Y","Features":[],"Open Access":"N","hasAccess":"Y","bypassPaywall":"N","user":{"license":{"businessPartnerID":[],"businessPartnerIDString":""}},"Access Type":"subscription","Bpids":"","Bpnames":"","BPID":["1"],"VG Wort Identifier":"pw-vgzm.415900-10.1007-s10994-013-5416-x","Full HTML":"Y","Subject Codes":["SCI","SCI21000","SCT19000","SCI19000","SCI21040"],"pmc":["I","I21000","T19000","I19000","I21040"],"session":{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},"content":{"serial":{"eissn":"1573-0565","pissn":"0885-6125"},"type":"Article","category":{"pmc":{"primarySubject":"Computer Science","primarySubjectCode":"I","secondarySubjects":{"1":"Artificial Intelligence","2":"Control, Robotics, Mechatronics","3":"Artificial Intelligence","4":"Simulation and Modeling","5":"Natural Language Processing (NLP)"},"secondarySubjectCodes":{"1":"I21000","2":"T19000","3":"I21000","4":"I19000","5":"I21040"}},"sucode":"SC6"},"attributes":{"deliveryPlatform":"oscar"}},"Event Category":"Article","GA Key":"UA-26408784-1","DOI":"10.1007/s10994-013-5416-x","Page":"article"}];
        var event = new CustomEvent('dataLayerCreated');
        document.dispatchEvent(event);
    </script>


    


<script src="/oscar-static/js/jquery-220afd743d.js" id="jquery"></script>

<script>
    function OptanonWrapper() {
        dataLayer.push({
            'event' : 'onetrustActive'
        });
    }
</script>


    <script data-test="onetrust-link" type="text/javascript" src="https://cdn.cookielaw.org/consent/6b2ec9cd-5ace-4387-96d2-963e596401c6.js" charset="UTF-8"></script>





    
    
        <!-- Google Tag Manager -->
        <script data-test="gtm-head">
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-WCF9Z9');
        </script>
        <!-- End Google Tag Manager -->
    


    <script class="js-entry">
    (function(w, d) {
        window.config = window.config || {};
        window.config.mustardcut = false;

        var ctmLinkEl = d.getElementById('js-mustard');

        if (ctmLinkEl && w.matchMedia && w.matchMedia(ctmLinkEl.media).matches) {
            window.config.mustardcut = true;

            
            
            
                window.Component = {};
            

            var currentScript = d.currentScript || d.head.querySelector('script.js-entry');

            
            function catchNoModuleSupport() {
                var scriptEl = d.createElement('script');
                return !(scriptEl.hasOwnProperty('noModule')) && scriptEl.hasOwnProperty('onbeforeload');
            }

            var headScripts = [
                { 'src' : '/oscar-static/js/polyfill-es5-bundle-ab77bcf8ba.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es5-bundle-5a82bc1fe4.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es6-bundle-cba6a3f7dc.js', 'async': false, 'module': true}
            ];

            var bodyScripts = [
                { 'src' : '/oscar-static/js/app-es5-bundle-dc94dc76aa.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/app-es6-bundle-0535bd1dc3.js', 'async': false, 'module': true}
                
                
                    , { 'src' : '/oscar-static/js/global-article-es5-bundle-cda72a2f15.js', 'async': false, 'module': false},
                    { 'src' : '/oscar-static/js/global-article-es6-bundle-cfa69aa341.js', 'async': false, 'module': true}
                
            ];

            function createScript(script) {
                var scriptEl = d.createElement('script');
                scriptEl.src = script.src;
                scriptEl.async = script.async;
                if (script.module === true) {
                    scriptEl.type = "module";
                    if (catchNoModuleSupport()) {
                        scriptEl.src = '';
                    }
                } else if (script.module === false) {
                    scriptEl.setAttribute('nomodule', true)
                }
                if (script.charset) {
                    scriptEl.setAttribute('charset', script.charset);
                }

                return scriptEl;
            }

            for (var i=0; i<headScripts.length; ++i) {
                var scriptEl = createScript(headScripts[i]);
                currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);
            }

            d.addEventListener('DOMContentLoaded', function() {
                for (var i=0; i<bodyScripts.length; ++i) {
                    var scriptEl = createScript(bodyScripts[i]);
                    d.body.appendChild(scriptEl);
                }
            });

            // Webfont repeat view
            var config = w.config;
            if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {
                d.documentElement.className += ' webfonts-loaded';
            }
        }
    })(window, document);
</script>

</head>
<body class="shared-article-renderer">
    
    
    
        <!-- Google Tag Manager (noscript) -->
        <noscript data-test="gtm-body">
            <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WCF9Z9"
            height="0" width="0" style="display:none;visibility:hidden"></iframe>
        </noscript>
        <!-- End Google Tag Manager (noscript) -->
    


    <div class="u-vh-full">
        
<div class="c-ad c-ad--LB1" data-test="springer-doubleclick-ad">
    <div class="c-ad c-ad__inner">
        <p class="c-ad__label">Advertisement</p>
        <div id="div-gpt-ad-LB1" data-gpt-unitpath="/270604982/springerlink/10994/article" data-gpt-sizes="728x90" data-gpt-targeting="pos=LB1;articleid=5416;"></div>
    </div>
</div>

<div class="u-position-relative">
    <header class="c-header u-mb-24" data-test="publisher-header">
        <div class="c-header__container">
            <div class="c-header__brand">
                
    <a id="logo" class="u-display-block" href="/" title="Go to homepage" data-test="springerlink-logo">
        <picture>
            <source type="image/svg+xml" srcset=/oscar-static/images/springerlink/svg/springerlink-253e23a83d.svg>
            <img src=/oscar-static/images/springerlink/png/springerlink-1db8a5b8b1.png alt="SpringerLink" width="148" height="30" data-test="header-academic">
        </picture>
        
        
    </a>


            </div>
            <div class="c-header__navigation">
                
    
        <button type="button"
                class="c-header__link u-button-reset u-mr-24"
                data-expander
                data-expander-target="#popup-search"
                data-expander-autofocus="true"
                data-test="header-search-button">
            <span class="u-display-flex u-align-items-center">
                Search
                <svg class="c-icon u-ml-8" width="14" height="14" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </span>
        </button>
        <nav>
            <ul class="c-header__menu">
                
                <li class="c-header__item">
                    <a data-test="login-link" class="c-header__link" href="//link.springer.com/signup-login?previousUrl=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs10994-013-5416-x">Log in</a>
                </li>
                

                
            </ul>
        </nav>
    

    



            </div>
        </div>
    </header>

    
        <div id="popup-search" class="c-popup-search u-mb-16 js-header-search u-js-hide">
            <div class="c-popup-search__content">
                <div class="u-container">
                    <div class="c-popup-search__container" data-test="springerlink-popup-search">
                        <div class="app-search">
    <form role="search" method="GET" action="/search" >
        <label for="search" class="app-search__label">Search SpringerLink</label>
        <div class="app-search__content">
            <input id="search" class="app-search__input" data-search-input autocomplete="off" role="textbox" name="query" type="text" value="">
            <button class="app-search__button" type="submit">
                <span class="u-visually-hidden">Search</span>
                <svg class="c-icon" width="14" height="14" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </button>
            
                <input type="hidden" name="searchType" value="publisherSearch">
            
            
        </div>
    </form>
</div>

                    </div>
                </div>
            </div>
        </div>
    
</div>

        

    <div class="c-banner c-banner--marketing">
	<div class="u-container">
		<p class="u-ma-0">
			We'd like to understand how you use our websites in order to improve them.
			<a class="c-banner__link u-underline"
				href="https://www.surveymonkey.co.uk/r/F2JMQ6L" data-track="click"
				data-track-action="Survey click" data-track-category="Blue Banner"
				data-track-label=10.1007/s10994-013-5416-x>Register your interest.</a>
		</p>
	</div>
</div>
    <div class="u-container u-mt-32 u-mb-32 u-clearfix" id="main-content" data-component="article-container">

        <main class="c-article-main-column u-float-left js-main-column">
            <div id="pdflink-container" class="download-article test-pdf-link">
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10994-013-5416-x.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-category="article body" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    
</div>

            <article itemscope itemtype="http://schema.org/ScholarlyArticle" lang="en">
                <div class="c-article-header">
                    <header>
                        <ul class="c-article-identifiers" data-test="article-identifier">
                            
    
    
    

                            <li class="c-article-identifiers__item"><a href="#article-info" data-track="click" data-track-action="publication date" data-track-category="article body" data-track-label="link">Published: <time datetime="2013-10-03" itemprop="datePublished">03 October 2013</time></a></li>
                        </ul>

                        
                        <h1 class="c-article-title u-h1" data-test="article-title" data-article-title="" itemprop="name headline">Spectral learning of weighted automata</h1><p class="c-article-title__alternate" lang="en">A forward-backward perspective</p>
                        <ul class="c-author-list js-etal-collapsed" data-etal="25" data-etal-small="3" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-category="article body" data-track-label="link" href="#auth-1">Borja Balle</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Universitat Politècnica de Catalunya" /><meta itemprop="address" content="grid.6835.8, Universitat Politècnica de Catalunya, Barcelona, 08034, Spain" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-category="article body" data-track-label="link" href="#auth-2" data-corresp-id="c1">Xavier Carreras<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-email"></use></svg></a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Universitat Politècnica de Catalunya" /><meta itemprop="address" content="grid.6835.8, Universitat Politècnica de Catalunya, Barcelona, 08034, Spain" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-category="article body" data-track-label="link" href="#auth-3">Franco M. Luque</a></span><sup class="u-js-hide"><a href="#Aff2">2</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Universidad Nacional de Córdoba and CONICET" /><meta itemprop="address" content="grid.10692.3c, 0000000101152557, Universidad Nacional de Córdoba and CONICET, X5000HUA, Córdoba, Argentina" /></span></sup> &amp; </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-category="article body" data-track-label="link" href="#auth-4">Ariadna Quattoni</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Universitat Politècnica de Catalunya" /><meta itemprop="address" content="grid.6835.8, Universitat Politècnica de Catalunya, Barcelona, 08034, Spain" /></span></sup> </li></ul>
                        <p class="c-article-info-details" data-container-section="info">
                            
    <a data-test="journal-link" href="/journal/10994"><i data-test="journal-title">Machine Learning</i></a>

                            <b data-test="journal-volume"><span class="u-visually-hidden">volume</span> 96</b>, <span class="u-visually-hidden">pages</span><span itemprop="pageStart">33</span>–<span itemprop="pageEnd">63</span>(<span data-test="article-publication-year">2014</span>)<a href="#citeas" class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-category="article body" data-track-label="link">Cite this article</a>
                        </p>
                        
    

                        <div data-test="article-metrics">
                            <div id="altmetric-container">
    
        <div class="c-article-metrics-bar__wrapper u-clear-both">
            <ul class="c-article-metrics-bar u-list-inline">
                
                    <li class=" c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">1433 <span class="c-article-metrics-bar__label">Accesses</span></p>
                    </li>
                
                
                    <li class="c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">12 <span class="c-article-metrics-bar__label">Citations</span></p>
                    </li>
                
                
                    
                        <li class="c-article-metrics-bar__item">
                            <p class="c-article-metrics-bar__count">1 <span class="c-article-metrics-bar__label">Altmetric</span></p>
                        </li>
                    
                
                <li class="c-article-metrics-bar__item">
                    <p class="c-article-metrics-bar__details"><a href="/article/10.1007%2Fs10994-013-5416-x/metrics" data-track="click" data-track-action="view metrics" data-track-category="article body" data-track-label="link" rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
                </li>
            </ul>
        </div>
    
</div>

                        </div>
                        
                        
                        
                    </header>
                </div>

                <div data-article-body="true" data-track-component="article body" class="c-article-body">
                    <section aria-labelledby="Abs1" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title u-h2 js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><p>In recent years we have seen the development of efficient provably correct algorithms for learning Weighted Finite Automata (WFA). Most of these algorithms avoid the known hardness results by defining parameters beyond the number of states that can be used to quantify the complexity of learning automata under a particular distribution. One such class of methods are the so-called spectral algorithms that measure learning complexity in terms of the smallest singular value of some Hankel matrix. However, despite their simplicity and wide applicability to real problems, their impact in application domains remains marginal to this date. One of the goals of this paper is to remedy this situation by presenting a derivation of the spectral method for learning WFA that—without sacrificing rigor and mathematical elegance—puts emphasis on providing intuitions on the inner workings of the method and does not assume a strong background in formal algebraic methods. In addition, our algorithm overcomes some of the shortcomings of previous work and is able to learn from statistics of substrings. To illustrate the approach we present experiments on a real application of the method to natural language parsing.</p></div></div></section>
                    
    


                    

                    

                    
                        
                            <section aria-labelledby="Sec1"><div class="c-article-section" id="Sec1-section"><h2 class="c-article-section__title u-h2 js-section-title js-c-reading-companion-sections-item" id="Sec1">Introduction</h2><div class="c-article-section__content" id="Sec1-content"><p>Learning finite automata is a fundamental task in Grammatical Inference. Over the years, a multitude of variations on this problem have been studied. For example, several learning models with different degrees of realism have been considered, ranging from query models and the learning in the limit paradigm, to the more challenging PAC learning framework. The main differences between these models are the ways in which learning algorithms can interact with the target machine. But not only the choice of learning model makes a difference in the study of this task, but also the particular kind of target automata that must be learned. These can range from the classical acceptors for regular languages like Deterministic Finite Automata (DFA) and Non-deterministic Finite Automata (NFA), to the more general Weighted Finite Automata (WFA) and Multiplicity Automata (MA), while also considering intermediate case like several classes of Probabilistic Finite Automata (PFA).</p><p>Efficient algorithms for learning all these classes of machines have been proposed in query models where algorithms have access to a minimal adequate teacher. Furthermore, most of these learning problems are also known to have polynomial information-theoretic complexity in the PAC learning model. But despite these encouraging results, it has been known for decades that the most basic problems regarding learnability of automata in the PAC model are computationally untractable under both complexity-theoretic and cryptographic assumptions. Since these general worst-case results preclude the existence of efficient learning algorithms for all machines under all possible probability distributions, lots of efforts have been done in identifying problems involving special cases for which provably efficient learning algorithms can be given. An alternative approach has been to identify additional parameters beyond the number of states that can be used to quantify the complexity of learning a particular automaton under a particular distribution. A paradigmatic example of this line of work are the PAC learning algorithms for PDFA given in Ron et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="&#xA;Ron, D., Singer, Y., &amp; Tishby, N. (1998). On the learnability and usage of acyclic probabilistic finite automata. Journal of Computing Systems Science, 56(2), 133–152.&#xA;" href="/article/10.1007/s10994-013-5416-x#ref-CR45" id="ref-link-section-d84034e359">1998</a>), Clark and Thollard (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="&#xA;Clark, A., &amp; Thollard, F. (2004). PAC-learnability of probabilistic deterministic finite state automata. Journal of Machine Learning Research, 5, 473–497.&#xA;" href="/article/10.1007/s10994-013-5416-x#ref-CR18" id="ref-link-section-d84034e362">2004</a>), Palmer and Goldberg (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="&#xA;Palmer, N., &amp; Goldberg, P. W. (2007). PAC-learnability of probabilistic deterministic finite state automata in terms of variation distance. Theoretical Computer Science, 387(1), 18–31.&#xA;" href="/article/10.1007/s10994-013-5416-x#ref-CR40" id="ref-link-section-d84034e365">2007</a>), Castro and Gavaldà (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="&#xA;Castro, J., &amp; Gavaldà, R. (2008). Towards feasible PAC-learning of probabilistic deterministic finite automata. In Proceedings of the 9th international colloquium on grammatical inference (ICGI) (pp. 163–174).&#xA;" href="/article/10.1007/s10994-013-5416-x#ref-CR17" id="ref-link-section-d84034e368">2008</a>), Balle et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="&#xA;Balle, B., Castro, J., &amp; Gavaldà, R. (2013). Learning probabilistic automata: a study in state distinguishability. Theoretical Computer Science, 473, 46–60.&#xA;" href="/article/10.1007/s10994-013-5416-x#ref-CR12" id="ref-link-section-d84034e371">2013</a>) whose running time depend on a distinguishability parameter quantifying the minimal distance between distributions generated by different states in the target machine.</p><p>Spectral learning methods are a family of algorithms that also fall into this particular line of work. In particular, starting with the seminal works of Hsu et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="&#xA;Hsu, D., Kakade, S. M., &amp; Zhang, T. (2009). A spectral algorithm for learning hidden Markov models. In Proceedings of the annual conference on computational learning theory (COLT).&#xA;" href="/article/10.1007/s10994-013-5416-x#ref-CR30" id="ref-link-section-d84034e377">2009</a>) and Bailly et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="&#xA;Bailly, R., Denis, F., &amp; Ralaivola, L. (2009). Grammatical inference as a principal component analysis problem. In L. Bottou &amp; M. Littman (Eds.), Proceedings of the 26th international conference on machine learning (pp. 33–40). Montreal: Omnipress.&#xA;" href="/article/10.1007/s10994-013-5416-x#ref-CR5" id="ref-link-section-d84034e380">2009</a>), efficient provably correct algorithms for learning non-deterministic machines that define probability distributions over sets of strings have been recently developed. A work-around to the aforementioned hardness results is obtained in this case by including the smallest singular value of some Hankel matrix in the bounds on the running time of spectral algorithms. The initial enthusiasm generated by such algorithms has been corroborated by the appearance of numerous follow-ups devoted to extending the method to more complex probabilistic models. However, despite the fact that these type of algorithms can be used to learn classes of machines widely used in applications like Hidden Markov Models (HMM) and PNFA, the impact of these methods in application domains remains marginal to this date. This remains so even when implementing such methods involves just a few linear algebra operations available in most general mathematical computing software packages. One of the main purposes of this paper is to try to remedy this situation by providing practical intuitions around the foundations of these algorithms and clear guidelines on how to use them in practice.</p><p>In our opinion, a major cause for the gap between the theoretical and practical development of spectral methods is the overwhelmingly theoretical nature of most papers in this area. The state of the art seems to suggest that there is no known workaround to these long mathematical proofs when seeking PAC learning results. However, it is also the case that most of the times the derivations given for these learning algorithms provide no intuitions on why or how one should expect them to work. Thus, obliterating the matter of PAC bounds, our first contribution is to provide a new derivation of the spectral learning algorithm for WFA that stresses the main intuitions behind the method. This yields an efficient algorithm for learning stochastic WFA defining probability distributions over strings. Our second contribution is showing how a simple transformation of this algorithm yields a more sample-efficient learning method that can work with substring statistics in contrast to the usual prefix statistics used in other methods.</p><p>Finite automata can also be used as building blocks for constructing more general context-free grammatical formalisms. In this paper we consider the case of non-deterministic Split Head-Automata Grammars (SHAG). These are a family of hidden-state parsing models that have been successfully used to model the significant amount of non-local phenomena exhibited by dependency structures in natural language. A SHAG is composed by a collection of stochastic automata and can be used to define a probability distribution over dependency structures for a given sentence. Each automaton in a SHAG describes the generation of particular head-modifier sequences. Our third contribution is to apply the spectral method to the problem of learning the constituent automata of a target SHAG. Contrary to previous works where PDFA were used as basic constituent automata for SHAG, using the spectral method allows us to learn SHAG built out of non-deterministic automata.</p><h3 class="c-article__sub-heading u-h3" id="Sec2">Related work</h3><p>In the last years multiple spectral learning algorithms have been proposed for a wide range of models. Many of these models deal with data whose nature is eminently sequential, like the work of Bailly et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="&#xA;Bailly, R., Denis, F., &amp; Ralaivola, L. (2009). Grammatical inference as a principal component analysis problem. In L. Bottou &amp; M. Littman (Eds.), Proceedings of the 26th international conference on machine learning (pp. 33–40). Montreal: Omnipress.&#xA;" href="/article/10.1007/s10994-013-5416-x#ref-CR5" id="ref-link-section-d84034e395">2009</a>) on WFA, or other works on particular subclasses of WFA like HMM (Hsu et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="&#xA;Hsu, D., Kakade, S. M., &amp; Zhang, T. (2009). A spectral algorithm for learning hidden Markov models. In Proceedings of the annual conference on computational learning theory (COLT).&#xA;" href="/article/10.1007/s10994-013-5416-x#ref-CR30" id="ref-link-section-d84034e398">2009</a>) and related extensions (Siddiqi et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="&#xA;Siddiqi, S. M., Boots, B., &amp; Gordon, G. J. (2010). Reduced-rank hidden Markov models. In Proceedings of the thirteenth international conference on artificial intelligence and statistics (AISTATS) (pp. 741–748).&#xA;" href="/article/10.1007/s10994-013-5416-x#ref-CR48" id="ref-link-section-d84034e401">2010</a>; Song et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="&#xA;Song, L., Siddiqi, S. M., Gordon, G., &amp; Smola, A. (2010). Hilbert space embeddings of hidden Markov models. In J. Fürnkranz &amp; T. Joachims (Eds.), Proceedings of the 27th international conference on machine learning (ICML-10) (pp. 991–998). Haifa: Omnipress.&#xA;" href="/article/10.1007/s10994-013-5416-x#ref-CR49" id="ref-link-section-d84034e404">2010</a>), Predictive State Representations (PSR) (Boots et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="&#xA;Boots, B., Siddiqi, S., &amp; Gordon, G. (2011). Closing the learning planning loop with predictive state representations. The International Journal of Robotics Research, 30(7), 954–966.&#xA;" href="/article/10.1007/s10994-013-5416-x#ref-CR15" id="ref-link-section-d84034e407">2011</a>), Finite State Transducers (FST) (Balle et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="&#xA;Balle, B., Quattoni, A., &amp; Carreras, X. (2011). A spectral learning algorithm for finite state transducers. In D. Gunopulos, T. Hofmann, D. Malerba, &amp; M. Vazirgiannis (Eds.), Lecture notes in computer science: Vol. 6911. ECML/PKDD (1) (pp. 156–171). Berlin: Springer.&#xA;" href="/article/10.1007/s10994-013-5416-x#ref-CR10" id="ref-link-section-d84034e411">2011</a>), and Quadratic Weighted Automata (QWA) (Bailly <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="&#xA;Bailly, R. (2011). Quadratic weighted automata: spectral algorithm and likelihood maximization. Journal of Machine Learning Research—Proceedings Track, 20, 147–163.&#xA;" href="/article/10.1007/s10994-013-5416-x#ref-CR4" id="ref-link-section-d84034e414">2011</a>). Besides direct applications of the spectral algorithm to different classes of sequential models, the method has also been combined with convex optimization algorithms in Balle et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="&#xA;Balle, B., Quattoni, A., &amp; Carreras, X. (2012). Local loss optimization in operator models: a new insight into spectral learning. In J. Langford &amp; J. Pineau (Eds.), Proceedings of the 29th international conference on machine learning (ICML-2012), ICML’12 (pp. 1879–1886). New York: Omnipress.&#xA;" href="/article/10.1007/s10994-013-5416-x#ref-CR11" id="ref-link-section-d84034e417">2012</a>), Balle and Mohri (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="&#xA;Balle, B., &amp; Mohri, M. (2012). Spectral learning of general weighted automata via constrained matrix completion. In Advances in neural information processing systems (Vol. 25, pp. 2168–2176).&#xA;" href="/article/10.1007/s10994-013-5416-x#ref-CR9" id="ref-link-section-d84034e420">2012</a>).</p><p>Despite this overwhelming diversity, to our knowledge the only previous work that has considered spectral learning for the general class of probabilistic weighted automata is due to Bailly et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="&#xA;Bailly, R., Denis, F., &amp; Ralaivola, L. (2009). Grammatical inference as a principal component analysis problem. In L. Bottou &amp; M. Littman (Eds.), Proceedings of the 26th international conference on machine learning (pp. 33–40). Montreal: Omnipress.&#xA;" href="/article/10.1007/s10994-013-5416-x#ref-CR5" id="ref-link-section-d84034e426">2009</a>). In spirit, their technique for deriving the spectral method is similar to ours. However, their elegant mathematical derivations are presented assuming a target audience with a strong background on formal algebraic methods. As such their presentation lacks the intuitions necessary to make the work accessible to a more general audience of machine learning practitioners. In contrast—without sacrificing rigor and mathematical elegance—our derivations put emphasis on providing intuitions on the inner working of the spectral method.</p><p>Besides sequential models, spectral learning algorithms for tree-like structures appearing in context-free grammatical models and probabilistic graphical models have also been considered (Bailly et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="&#xA;Bailly, R., Habrard, A., &amp; Denis, F. (2010). A spectral approach for probabilistic grammatical inference on trees. In M. Hutter, F. Stephan, V. Vovk, &amp; T. Zeugmann (Eds.), Lecture notes in computer science (Vol. 6331, pp. 74–88). Berlin: Springer.&#xA;" href="/article/10.1007/s10994-013-5416-x#ref-CR6" id="ref-link-section-d84034e432">2010</a>; Parikh et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="&#xA;Parikh, A., Song, L., &amp; Xing, E. (2011). A spectral algorithm for latent tree graphical models. In Proceedings of the 28th international conference on machine learning, ICML 2011 (ICML) (pp. 1065–1072).&#xA;" href="/article/10.1007/s10994-013-5416-x#ref-CR41" id="ref-link-section-d84034e435">2011</a>; Luque et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="&#xA;Luque, F. M., Quattoni, A., Balle, B., &amp; Carreras, X. (2012). Spectral learning for non-deterministic dependency parsing. In Proceedings of the 13th conference of the European chapter of the Association for Computational Linguistics (pp. 409–419). Avignon: Association for Computational Linguistics.&#xA;" href="/article/10.1007/s10994-013-5416-x#ref-CR33" id="ref-link-section-d84034e438">2012</a>; Cohen et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="&#xA;Cohen, S. B., Stratos, K., Collins, M., Foster, D. P., &amp; Ungar, L. (2012). Spectral learning of latent-variable PCFGS. In Proceedings of the 50th annual meeting of the association for computational linguistics (Volume 1: Long papers) (pp. 223–231). Jeju Island: Association for Computational Linguistics.&#xA;" href="/article/10.1007/s10994-013-5416-x#ref-CR20" id="ref-link-section-d84034e441">2012</a>; Dhillon et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="&#xA;Dhillon, P., Rodu, J., Collins, M., Foster, D., &amp; Ungar, L. (2012). Spectral dependency parsing with latent variables. In Proceedings of the 2012 joint conference on empirical methods in natural language processing and computational natural language learning (pp. 205–213). Jeju Island: Association for Computational Linguistics.&#xA;" href="/article/10.1007/s10994-013-5416-x#ref-CR24" id="ref-link-section-d84034e444">2012</a>). In Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10994-013-5416-x#Sec22">6.4</a> we give a more detailed comparison between our work on SHAG and related methods that learn tree-shaped models. The spectral method has been applied as well to other classes of probabilistic mixture models (Anandkumar et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012c" title="&#xA;Anandkumar, A., Hsu, D., &amp; Kakade, S. M. (2012c). A method of moments for mixture models and hidden Markov models. Journal of Machine Learning Research—Proceedings Track, 23, 33.1–33.34.&#xA;" href="/article/10.1007/s10994-013-5416-x#ref-CR3" id="ref-link-section-d84034e451">2012c</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference a" title="&#xA;Anandkumar, A., Foster, D. P., Hsu, D., Kakade, S., &amp; Liu, Y. K. (2012a). A spectral algorithm for latent Dirichlet allocation. In P. L. Bartlett, F. C. N. Pereira, C. J. C. Burges, L. Bottou, &amp; K. Q. Weinberger (Eds.), NIPS (pp. 926–934).&#xA;" href="/article/10.1007/s10994-013-5416-x#ref-CR1" id="ref-link-section-d84034e454">a</a>).</p></div></div></section><section aria-labelledby="Sec3"><div class="c-article-section" id="Sec3-section"><h2 class="c-article-section__title u-h2 js-section-title js-c-reading-companion-sections-item" id="Sec3">Weighted automata and Hankel matrices</h2><div class="c-article-section__content" id="Sec3-content"><p>In this section we present Weighted Finite Automata (WFA), the finite state machine formulations that will be used throughout the paper. We begin by introducing some notation for dealing with functions from strings to real numbers and then proceed to define Hankel matrices. These matrices will play a very important role in the derivation of the spectral learning algorithm given in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10994-013-5416-x#Sec8">4</a>. Then we proceed to describe the algebraic formulation of WFA and its relation to Hankel matrices. Finally, we discuss some special properties of stochastic WFA realizing probability distributions over strings. These properties will allow us to use the spectral method to learn from substring statistics, thus yielding more sample-efficient methods than other approaches based on string or prefix statistics.</p><h3 class="c-article__sub-heading u-h3" id="Sec4">Functions on strings and their Hankel matrices</h3><p>Let <i>Σ</i> be a finite alphabet. We use <i>σ</i> to denote an arbitrary symbol in <i>Σ</i>. The set of all finite strings over <i>Σ</i> is denoted by <i>Σ</i>
                           <sup>⋆</sup>, where we write <i>λ</i> for the empty string. We use bold letters to represent vectors <b>v</b> and matrices <b>M</b>. We use <b>M</b>
                           <sup>+</sup> to denote the <i>Moore–Penrose pseudoinverse</i> of some matrix <b>M</b>.</p><p>Let <span class="mathjax-tex">\(f : \varSigma^{\star}\rightarrow\mathbb{R}\)</span> be a function over strings. The <i>Hankel matrix</i> of <i>f</i> is a bi-infinite matrix <span class="mathjax-tex">\(\mathbf {H}_{f} \in \mathbb{R} ^{\varSigma^{\star}\times\varSigma^{\star}}\)</span> whose entries are defined as <b>H</b>
                           <sub>
                    <i>f</i>
                  </sub>(<i>u</i>,<i>v</i>)=<i>f</i>(<i>uv</i>) for any <i>u</i>,<i>v</i>∈<i>Σ</i>
                           <sup>⋆</sup>. That is, rows are indexed by prefixes and columns by suffixes. Note that the Hankel matrix of a function <i>f</i> is a very redundant way to represent <i>f</i>. In particular, the value <i>f</i>(<i>x</i>) appears |<i>x</i>|+1 times in <b>H</b>
                           <sub>
                    <i>f</i>
                  </sub>, and we have <i>f</i>(<i>x</i>)=<b>H</b>
                           <sub>
                    <i>f</i>
                  </sub>(<i>x</i>,<i>λ</i>)=<b>H</b>
                           <sub>
                    <i>f</i>
                  </sub>(<i>λ</i>,<i>x</i>). An obvious observation is that a matrix <span class="mathjax-tex">\(\mathbf {M}\in\mathbb{R}^{\varSigma ^{\star}\times \varSigma^{\star}}\)</span> satisfying <b>M</b>(<i>u</i>
                           <sub>1</sub>,<i>v</i>
                           <sub>1</sub>)=<b>M</b>(<i>u</i>
                           <sub>2</sub>,<i>v</i>
                           <sub>2</sub>) for any <i>u</i>
                           <sub>1</sub>
                           <i>v</i>
                           <sub>1</sub>=<i>u</i>
                           <sub>2</sub>
                           <i>v</i>
                           <sub>2</sub> is the Hankel matrix of some function <span class="mathjax-tex">\(f : \varSigma^{\star}\rightarrow\mathbb{R}\)</span>.</p><p>We will be considering (finite) sub-blocks of a bi-infinite Hankel matrix <b>H</b>
                           <sub>
                    <i>f</i>
                  </sub>. An easy way to define such sub-blocks is using a <i>basis</i>
					                      <span class="mathjax-tex">\(\mathcal{B}= (\mathcal{P},\mathcal{S})\)</span>, where <span class="mathjax-tex">\(\mathcal{P}\subseteq\varSigma^{\star}\)</span> is a set of prefixes and <span class="mathjax-tex">\(\mathcal{S}\subseteq \varSigma^{\star}\)</span> a set of suffixes. We write <span class="mathjax-tex">\(p = |\mathcal{P}|\)</span> and <span class="mathjax-tex">\(s = |\mathcal{S}|\)</span>. The sub-block of <b>H</b>
                           <sub>
                    <i>f</i>
                  </sub> defined by <span class="mathjax-tex">\(\mathcal{B}\)</span> is the <i>p</i>×<i>s</i> matrix <span class="mathjax-tex">\(\mathbf {H}_{\mathcal{B}}\in \mathbb{R}^{\mathcal{P}\times\mathcal{S}}\)</span> with <span class="mathjax-tex">\(\mathbf {H}_{\mathcal{B}}(u,v) = \mathbf {H}_{f}(u,v) = f(uv)\)</span> for any <span class="mathjax-tex">\(u \in\mathcal{P}\)</span> and <span class="mathjax-tex">\(v \in\mathcal{S}\)</span>. We may just write <b>H</b> if the basis <span class="mathjax-tex">\(\mathcal{B}\)</span> is arbitrary or obvious from the context.</p><p>Not all bases will be equally useful for our purposes. In particular, we will be interested in so-called closed basis. Let <span class="mathjax-tex">\(\mathcal{B}= (\mathcal{P},\mathcal{S})\)</span> be a basis and write <i>Σ</i>′=<i>Σ</i>∪{<i>λ</i>}. The <i>prefix-closure</i>
                           <sup><a href="#Fn1"><span class="u-visually-hidden">Footnote </span>1</a></sup> of <span class="mathjax-tex">\(\mathcal{B}\)</span> is the basis <span class="mathjax-tex">\(\mathcal{B}^{\prime}= (\mathcal{P}^{\prime},\mathcal {S})\)</span>, where <span class="mathjax-tex">\(\mathcal{P}^{\prime}= \mathcal{P} \varSigma^{\prime}\)</span>. Equivalently, a basis <span class="mathjax-tex">\(\mathcal{B}= (\mathcal{P},\mathcal{S})\)</span> is said to be <i>p-closed</i> if <span class="mathjax-tex">\(\mathcal{P}= \mathcal{P}^{\prime}\varSigma^{\prime}\)</span> for some <span class="mathjax-tex">\(\mathcal {P}^{\prime}\)</span> called the <i>root</i> of <span class="mathjax-tex">\(\mathcal{P}\)</span>. It turns out that a Hankel matrix over a p-closed basis can be partitioned into |<i>Σ</i>|+1 blocks of the same size. This partition will be central to our results. Let <b>H</b>
                           <sub>
                    <i>f</i>
                  </sub> be a Hankel matrix and <span class="mathjax-tex">\(\mathcal{B}= (\mathcal{P},\mathcal{S})\)</span> a basis. For any <i>σ</i>∈<i>Σ</i>′ we write <b>H</b>
                           <sub>
                    <i>σ</i>
                  </sub> to denote the sub-block of <b>H</b>
                           <sub>
                    <i>f</i>
                  </sub> over the basis <span class="mathjax-tex">\((\mathcal{P}\sigma, \mathcal{S})\)</span>. That is, the sub-block <span class="mathjax-tex">\(\mathbf {H}_{\sigma}\in\mathbb{R}^{\mathcal{P}\sigma \times\mathcal{S}}\)</span> of <b>H</b>
                           <sub>
                    <i>f</i>
                  </sub> is the <i>p</i>×<i>s</i> matrix defined by <b>H</b>
                           <sub>
                    <i>σ</i>
                  </sub>(<i>u</i>,<i>v</i>)=<b>H</b>
                           <sub>
                    <i>f</i>
                  </sub>(<i>uσ</i>,<i>v</i>). Thus, if <span class="mathjax-tex">\(\mathcal{B}^{\prime}\)</span> is the prefix-closure of <span class="mathjax-tex">\(\mathcal {B}\)</span>, then for a particular ordering of the strings in <span class="mathjax-tex">\(\mathcal{P}^{\prime}\)</span>, we have </p><div id="Equa" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex"> $$\begin{aligned} \mathbf {H}_{\mathcal{B}^\prime}^\top= \bigl[ \; \mathbf {H}_\lambda^\top\; \bigm|\; \mathbf {H}_{\sigma _1}^\top\;\bigm|\; \cdots\;\bigm|\; \mathbf {H}_{\sigma_{|\varSigma|}}^\top\; \bigr] . \end{aligned}$$ </span></div></div>
                        <p>The <i>rank</i> of a function <span class="mathjax-tex">\(f : \varSigma^{\star}\rightarrow\mathbb {R}\)</span> is defined as the rank of its Hankel matrix: <span class="mathjax-tex">\(\operatorname{rank}(f) = \operatorname {rank}( \mathbf {H}_{f})\)</span>. The rank of a sub-block of <b>H</b>
                           <sub>
                    <i>f</i>
                  </sub> cannot exceed <span class="mathjax-tex">\(\operatorname {rank}(f)\)</span>, and we will be specially interested on sub-blocks with full rank. We say that a basis <span class="mathjax-tex">\(\mathcal{B}= (\mathcal{P},\mathcal{S})\)</span> is <i>complete</i> for <i>f</i> if the sub-block <span class="mathjax-tex">\(\mathbf {H}_{\mathcal{B}}\)</span> has full rank: <span class="mathjax-tex">\(\operatorname{rank}(\mathbf {H}_{\mathcal{B}}) = \operatorname{rank}( \mathbf {H}_{f})\)</span>. In this case we say that <span class="mathjax-tex">\(\mathbf {H}_{\mathcal{B}}\)</span> is a <i>complete sub-block</i> of <b>H</b>
                           <sub>
                    <i>f</i>
                  </sub>. It turns out that the rank of <i>f</i> is related to the number of states needed to compute <i>f</i> with a weighted automaton, and that the prefix-closure of a complete sub-block of <b>H</b>
                           <sub>
                    <i>f</i>
                  </sub> contains enough information to compute this automaton. These two results will provide the basis for the learning algorithm presented in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10994-013-5416-x#Sec8">4</a>.</p><h3 class="c-article__sub-heading u-h3" id="Sec5">Weighted finite automata</h3><p>A widely used class of functions mapping strings to real numbers is that of functions defined by <i>weighted finite automata</i> (WFA) or in short <i>weighted automata</i> (Mohri <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="&#xA;Mohri, M. (2009). Weighted automata algorithms. In M. Droste, W. Kuich, &amp; H. Vogler (Eds.), Monographs in theoretical computer science. An EATCS series. Handbook of weighted automata (pp. 213–254). Berlin: Springer.&#xA;" href="/article/10.1007/s10994-013-5416-x#ref-CR38" id="ref-link-section-d84034e1296">2009</a>). These functions are also known as <i>rational power series</i> (Salomaa and Soittola <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1978" title="&#xA;Salomaa, A., &amp; Soittola, M. (1978). Automata-theoretic aspects of formal power series. New York: Springer.&#xA;" href="/article/10.1007/s10994-013-5416-x#ref-CR46" id="ref-link-section-d84034e1302">1978</a>; Berstel and Reutenauer <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1988" title="&#xA;Berstel, J., &amp; Reutenauer, C. (1988). Rational series and their languages. Berlin: Springer.&#xA;" href="/article/10.1007/s10994-013-5416-x#ref-CR14" id="ref-link-section-d84034e1306">1988</a>). A WFA over <i>Σ</i> with <i>n</i> states can be defined as a tuple <i>A</i>=〈<b><i>α</i></b>
                           <sub>1</sub>,<b><i>α</i></b>
                           <sub>∞</sub>,{<b>A</b>
                           <sub>
                    <i>σ</i>
                  </sub>}〉, where <span class="mathjax-tex">\(\boldsymbol{\alpha}_{1}, \boldsymbol {\alpha }_{\infty }\in\mathbb{R}^{n}\)</span> are the <i>initial</i> and <i>final weight</i> vectors, and <span class="mathjax-tex">\(\mathbf {A}_{\sigma}\in\mathbb{R}^{n\times n}\)</span> the transition matrix associated to each alphabet symbol <i>σ</i>∈<i>Σ</i>. The function <i>f</i>
                           <sub>
                    <i>A</i>
                  </sub> realized by a WFA <i>A</i> is defined by </p><div id="Equb" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex"> $$\begin{aligned} f_A(x) = \boldsymbol{\alpha}_1^\top \mathbf {A}_{x_1} \cdots \mathbf {A}_{x_t} \boldsymbol {\alpha }_{\infty }= \boldsymbol{ \alpha}_1^\top \mathbf {A}_x \boldsymbol {\alpha }_{\infty }, \end{aligned}$$ </span></div></div><p> for any string <i>x</i>=<i>x</i>
                           <sub>1</sub>⋯<i>x</i>
                           <sub>
                    <i>t</i>
                  </sub>∈<i>Σ</i>
                           <sup>⋆</sup> with <i>t</i>=|<i>x</i>| and <i>x</i>
                           <sub>
                    <i>i</i>
                  </sub>∈<i>Σ</i> for all 1≤<i>i</i>≤<i>t</i>. We will write |<i>A</i>| to denote the number of states of a WFA. The following characterization of the set of functions <span class="mathjax-tex">\(f : \varSigma ^{\star}\rightarrow \mathbb{R}\)</span> realizable by WFA in terms of the rank of their Hankel matrix <span class="mathjax-tex">\(\operatorname{rank} ( \mathbf {H}_{f})\)</span> was given in Carlyle and Paz (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1971" title="&#xA;Carlyle, J. W., &amp; Paz, A. (1971). Realizations by stochastic finite automata. Journal of Computer and System Sciences, 5(1), 26–40.&#xA;" href="/article/10.1007/s10994-013-5416-x#ref-CR16" id="ref-link-section-d84034e1471">1971</a>), Fliess (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1974" title="&#xA;Fliess, M. (1974). Matrices de Hankel. Journal de Mathématiques Pures et Appliquées, 53, 197–222.&#xA;" href="/article/10.1007/s10994-013-5416-x#ref-CR28" id="ref-link-section-d84034e1474">1974</a>). We also note that the construction of an equivalent WFA with the minimal number of states from a given WFA was first given in Schützenberger (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1961" title="&#xA;Schützenberger, M. (1961). On the definition of a family of automata. Information and Control, 4, 245–270.&#xA;" href="/article/10.1007/s10994-013-5416-x#ref-CR47" id="ref-link-section-d84034e1477">1961</a>).</p>
                  <h3 class="c-article__sub-heading u-h3">Theorem 1</h3>
                  <p>(Carlyle and Paz <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1971" title="&#xA;Carlyle, J. W., &amp; Paz, A. (1971). Realizations by stochastic finite automata. Journal of Computer and System Sciences, 5(1), 26–40.&#xA;" href="/article/10.1007/s10994-013-5416-x#ref-CR16" id="ref-link-section-d84034e1487">1971</a>; Fliess <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1974" title="&#xA;Fliess, M. (1974). Matrices de Hankel. Journal de Mathématiques Pures et Appliquées, 53, 197–222.&#xA;" href="/article/10.1007/s10994-013-5416-x#ref-CR28" id="ref-link-section-d84034e1490">1974</a>)</p>
                  <p>
                              <i>A function</i>
						                        <span class="mathjax-tex">\(f : \varSigma^{\star}\rightarrow\mathbb{R}\)</span>
						                        <i>can be defined by a WFA iff</i>
						                        <span class="mathjax-tex">\(\operatorname{rank}( \mathbf {H}_{f})\)</span>
						                        <i>is finite</i>, <i>and in that case</i>
						                        <span class="mathjax-tex">\(\operatorname{rank}( \mathbf {H}_{f})\)</span>
						                        <i>is the minimal number of states of any WFA</i>
						                        <i>A</i>
						                        <i>such that</i>
						                        <i>f</i>=<i>f</i>
                              <sub>
                      <i>A</i>
                    </sub>.</p>
                <p>In view of this result, we will say that <i>A</i> is <i>minimal</i> for <i>f</i> if <i>f</i>
                           <sub>
                    <i>A</i>
                  </sub>=<i>f</i> and <span class="mathjax-tex">\(|A| = \operatorname{rank}(f)\)</span>.</p><p>Another useful fact about WFA is their invariance under change of basis. It follows from the definition of <i>f</i>
                           <sub>
                    <i>A</i>
                  </sub> that if <span class="mathjax-tex">\(\mathbf {M} \in\mathbb{R}^{n \times n}\)</span> is an invertible matrix, then the WFA <i>B</i>=〈<b>M</b>
                           <sup>⊤</sup>
                           <b><i>α</i></b>
                           <sub>1</sub>,<b>M</b>
                           <sup>−1</sup>
                           <b><i>α</i></b>
                           <sub>∞</sub>,{<b>M</b>
                           <sup>−1</sup>
                           <b>A</b>
                           <sub>
                    <i>σ</i>
                  </sub>
                           <b>M</b>}〉 satisfies <i>f</i>
                           <sub>
                    <i>B</i>
                  </sub>=<i>f</i>
                           <sub>
                    <i>A</i>
                  </sub>. Sometimes <i>B</i> will be denoted by <b>M</b>
                           <sup>−1</sup>
                           <i>A</i>
                           <b>M</b>. This fact will prove very useful when we consider the problem of learning a WFA realizing a certain function.</p><p>Weighted automata are related to other finite state computational models. In particular, WFA can also be defined more generally over an arbitrary semi-ring instead of the field of real numbers, in which case there are sometimes called <i>multiplicity automata</i> (MA) (e.g. Beimel et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="&#xA;Beimel, A., Bergadano, F., Bshouty, N., Kushilevitz, E., &amp; Varricchio, S. (2000). Learning functions represented as multiplicity automata. Journal of the ACM, 47, 506–530.&#xA;" href="/article/10.1007/s10994-013-5416-x#ref-CR13" id="ref-link-section-d84034e1707">2000</a>). It is well known that using weights over an arbitrary semi-ring more computational power is obtained. However, in this paper we will only consider WFA with real weights. It is easy to see that several other models of automata (DFA, PDFA, PNFA) can be cast as special cases of WFA.</p><h4 class="c-article__sub-heading u-h3 c-article__sub-heading--light" id="Sec6">Example</h4><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10994-013-5416-x#Fig1">1</a> shows an example of a weighted automaton <i>A</i>=〈<b><i>α</i></b>
                              <sub>1</sub>,<b><i>α</i></b>
                              <sub>∞</sub>,{<b>A</b>
                              <sub>
                      <i>σ</i>
                    </sub>}〉 with two states defined over the alphabet <i>Σ</i>={<i>a</i>,<i>b</i>}, with both its algebraic representation (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10994-013-5416-x#Fig1">1</a>(b)) in terms of vectors and matrices and the equivalent graph representation (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10994-013-5416-x#Fig1">1</a>(a)) useful for a variety of WFA algorithms (Mohri <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="&#xA;Mohri, M. (2009). Weighted automata algorithms. In M. Droste, W. Kuich, &amp; H. Vogler (Eds.), Monographs in theoretical computer science. An EATCS series. Handbook of weighted automata (pp. 213–254). Berlin: Springer.&#xA;" href="/article/10.1007/s10994-013-5416-x#ref-CR38" id="ref-link-section-d84034e1759">2009</a>). Letting <span class="mathjax-tex">\(\mathcal{W} = \{\epsilon, a, b\}\)</span>, then <span class="mathjax-tex">\(\mathcal{B}= (\mathcal{W} \varSigma',\mathcal{W})\)</span> is a p-closed basis. The following is the Hankel matrix of <i>A</i> on this basis shown with two-digit precision entries: </p><div id="Equc" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex"> $$\begin{aligned} \mathbf {H}_\mathcal{B}^\top= \bordermatrix{ ~ &amp; \epsilon&amp; a &amp; b &amp; aa &amp; ab &amp; ba &amp; bb \cr \epsilon&amp; 0.00 &amp; 0.20 &amp; 0.14 &amp; 0.22 &amp; 0.15 &amp; 0.45 &amp; 0.31 \cr a &amp; 0.20 &amp; 0.22 &amp; 0.45 &amp; 0.19 &amp; 0.29 &amp; 0.45 &amp; 0.85 \cr b &amp; 0.14 &amp; 0.15 &amp; 0.31 &amp; 0.13 &amp; 0.20 &amp; 0.32 &amp; 0.58 \cr } \end{aligned}$$ </span></div></div>
						                        <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1"><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 1</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-category="article body" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10994-013-5416-x/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10994-013-5416-x/MediaObjects/10994_2013_5416_Fig1_HTML.gif?as=webp"></source><img aria-describedby="figure-1-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10994-013-5416-x/MediaObjects/10994_2013_5416_Fig1_HTML.gif" alt="figure1" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p>Example of a weighted automaton over <i>Σ</i>={<i>a</i>,<i>b</i>} with 2 states: (<b>a</b>) graph representation; (<b>b</b>) algebraic representation</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-category="article body" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10994-013-5416-x/figures/1" data-track-dest="link:Figure1 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           </div></div></section><section aria-labelledby="Sec7"><div class="c-article-section" id="Sec7-section"><h2 class="c-article-section__title u-h2 js-section-title js-c-reading-companion-sections-item" id="Sec7">Observables in stochastic weighted automata</h2><div class="c-article-section__content" id="Sec7-content"><p>Previous section introduces the class of WFA in a general setting. As we will see in next section, in order to learn an automata realizing (an approximation of) a function <span class="mathjax-tex">\(f : \varSigma^{\star}\rightarrow\mathbb {R}\)</span> using a spectral algorithm, we will need to compute (an estimate) of a sub-block of the Hankel matrix <b>H</b>
                        <sub>
                  <i>f</i>
                </sub>. In general such sub-blocks may be hard to obtain. However, in the case when <i>f</i> computes a probability distribution over <i>Σ</i>
                        <sup>⋆</sup> and we have access to a sample of i.i.d. examples from this distribution, estimates of sub-blocks of <b>H</b>
                        <sub>
                  <i>f</i>
                </sub> can be obtained efficiently. In this section we discuss some properties of WFA which realize probability distributions. In particular, we are interested in showing how different kinds of statistics that can be computed from a sample of strings induce functions on <i>Σ</i>
                        <sup>⋆</sup> realized by similar WFA.</p><p>We say that a WFA <i>A</i> is <i>stochastic</i> if the function <i>f</i>=<i>f</i>
                        <sub>
                  <i>A</i>
                </sub> is a probability distribution over <i>Σ</i>
                        <sup>⋆</sup>. That is, if <i>f</i>(<i>x</i>)≥0 for all <i>x</i>∈<i>Σ</i>
                        <sup>⋆</sup> and <span class="mathjax-tex">\(\sum_{x \in\varSigma^{\star}} f(x) = 1\)</span>. To make it clear that <i>f</i> represents a probability distribution we may sometimes write it as <span class="mathjax-tex">\(f(x) = \mathbb{P}[x]\)</span>.</p><p>An interesting fact about distributions over <i>Σ</i>
                        <sup>⋆</sup> is that given an i.i.d. sample generated from that distribution one can compute an estimation <span class="mathjax-tex">\(\hat{ \mathbf {H}}_{f}\)</span> of its Hankel matrix, or of any finite sub-block <span class="mathjax-tex">\(\hat {\mathbf {H}}_{\mathcal{B}}\)</span>. When the sample is large enough, these estimates will converge to the true Hankel matrices. In particular, suppose <i>S</i>=(<i>x</i>
                        <sup>1</sup>,…,<i>x</i>
                        <sup><i>m</i></sup>) is a sample containing <i>m</i> i.i.d. strings from some distribution <span class="mathjax-tex">\(\mathbb{P}\)</span> over <i>Σ</i>
                        <sup>⋆</sup> and let us write <span class="mathjax-tex">\(\hat{\mathbb{P}}_{S}(x)\)</span> for the empirical frequency of <i>x</i> in <i>S</i>. Then, for a fixed basis <span class="mathjax-tex">\(\mathcal{B}\)</span>, if we compute the <i>empirical Hankel matrix</i> given by <span class="mathjax-tex">\(\hat{ \mathbf {H}}_{\mathcal{B}}(u,v) = \hat{\mathbb{P}}_{S}[u v]\)</span>, one can show using McDiarmid’s inequality that with high probability the following holds (Hsu et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="&#xA;Hsu, D., Kakade, S. M., &amp; Zhang, T. (2009). A spectral algorithm for learning hidden Markov models. In Proceedings of the annual conference on computational learning theory (COLT).&#xA;" href="/article/10.1007/s10994-013-5416-x#ref-CR30" id="ref-link-section-d84034e2062">2009</a>): </p><div id="Equd" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex"> $$\begin{aligned} \| \mathbf {H}_\mathcal{B}- \hat{ \mathbf {H}}_\mathcal{B}\|_F \leq O \biggl(\frac {1}{\sqrt{m}} \biggr) . \end{aligned}$$ </span></div></div><p> This is one of the pillars on which the finite sample analysis of the spectral method lies. We will discuss this further in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10994-013-5416-x#Sec11">4.2.1</a>.</p><p>Note that when <i>f</i> realizes a distribution over <i>Σ</i>
                        <sup>⋆</sup>, one can think of computing other probabilistic quantities besides probabilities of strings <span class="mathjax-tex">\(\mathbb{P}[x]\)</span>. For example, one can define the function <i>f</i>
                        <sub>p</sub> that computes probabilities of prefixes; that is, <span class="mathjax-tex">\(f_{\mathrm{p}}(x) = \mathbb{P}[x \varSigma^{\star}]\)</span>. Another probabilistic function that can be computed from a distribution over <i>Σ</i>
                        <sup>⋆</sup> is the expected number of times a particular string appears as a substring of random strings; we use <i>f</i>
                        <sub>s</sub> to denote this function. More formally, given two strings <i>w</i>,<i>x</i>∈<i>Σ</i>
                        <sup>⋆</sup> let |<i>w</i>|<sub>
                  <i>x</i>
                </sub> denote the number of times that <i>x</i> appears in <i>w</i> as a substring. Then we can write <span class="mathjax-tex">\(f_{\mathrm{s}}(x) = \mathbb{E}[|w|_{x}]\)</span>, where the expectation is with respect to <i>w</i> sampled from <i>f</i>: <span class="mathjax-tex">\(\mathbb{E}[|w|_{x}] = \sum_{w \in \varSigma^{\star}} |w|_{x} \mathbb{P}[w]\)</span>.</p><p>In general the class of stochastic WFA may include some pathological examples with states that are not connected to any terminating state. In order to avoid such cases we introduce the following technical condition. Given a stochastic WFA <i>A</i>=〈<b><i>α</i></b>
                        <sub>1</sub>,<b><i>α</i></b>
                        <sub>∞</sub>,{<b>A</b>
                        <sub>
                  <i>σ</i>
                </sub>}〉 let <b>A</b>=∑<sub>
                           <i>σ</i>∈<i>Σ</i>
                        </sub>
                        <b>A</b>
                        <sub>
                  <i>σ</i>
                </sub>. We say that <i>A</i> is <i>irredundant</i> if ∥<b>A</b>∥&lt;1 for some submultiplicative matrix norm ∥⋅∥. Note that a necessary condition for this to happen is that the spectral radius of <b>A</b> is less than one: <i>ρ</i>(<b>A</b>)&lt;1. In particular, irredundancy implies that the sum ∑<sub>
                           <i>k</i>≥0</sub>
                        <b>A</b>
                        <sup><i>k</i></sup> converges to (<b>I</b>−<b>A</b>)<sup>−1</sup>. An interesting property of irredundant stochastic WFA is that both <i>f</i>
                        <sub>p</sub> and <i>f</i>
                        <sub>s</sub> can also be computed by WFA as shown by the following result.</p>
                <h3 class="c-article__sub-heading u-h3">Lemma 1</h3>
                <p>
                           <i>Let</i> 〈<b><i>α</i></b>
                           <sub>1</sub>,<b><i>α</i></b>
                           <sub>∞</sub>,{<b>A</b>
                           <sub>
                    <i>σ</i>
                  </sub>}〉 <i>be an irredundant stochastic WFA and write</i>: <span class="mathjax-tex">\(\mathbf {A} = \sum _{\sigma\in\varSigma} \mathbf {A_{\sigma}}\)</span>, <span class="mathjax-tex">\({\tilde{\boldsymbol{\alpha}}_{1}}^{\top}= \boldsymbol{\alpha}_{1}^{\top}(\mathbf {I} - \mathbf {A})^{-1}\)</span>, <i>and</i>
					                      <span class="mathjax-tex">\({\tilde{\boldsymbol{\alpha}}_{\infty}} = (\mathbf {I} - \mathbf {A})^{-1} \boldsymbol {\alpha }_{\infty }\)</span>. <i>Suppose</i>
					                      <span class="mathjax-tex">\(f : \varSigma^{\star}\rightarrow\mathbb{R}\)</span>
					                      <i>is a probability distribution such that</i>
					                      <span class="mathjax-tex">\(f(x) = \mathbb{P}[x]\)</span>
					                      <i>and define functions</i>
					                      <span class="mathjax-tex">\(f_{\mathrm{p}}(x) = \mathbb{P}[x \varSigma^{\star}]\)</span>
					                      <i>and</i>
					                      <span class="mathjax-tex">\(f_{\mathrm{s}}(x) = \mathbb{E}[|w|_{x}]\)</span>. <i>Then</i>, <i>the following are equivalent</i>: </p><ol class="u-list-style-none">
                    <li>
                      <span class="u-custom-list-number">1.</span>
                      
                        <p>
                                       <i>A</i>=〈<b><i>α</i></b>
                                       <sub>1</sub>,<b><i>α</i></b>
                                       <sub>∞</sub>,{<b>A</b>
                                       <sub>
                            <i>σ</i>
                          </sub>}〉 <i>realizes</i>
									                              <i>f</i>,</p>
                      
                    </li>
                    <li>
                      <span class="u-custom-list-number">2.</span>
                      
                        <p>
                                       <span class="mathjax-tex">\(A_{\mathrm{p}} = \langle\boldsymbol{\alpha}_{1}, {\tilde {\boldsymbol{\alpha}}_{\infty}},\{ \mathbf {A}_{\sigma}\} \rangle\)</span>
									                              <i>realizes</i>
									                              <i>f</i>
                                       <sub>p</sub>,</p>
                      
                    </li>
                    <li>
                      <span class="u-custom-list-number">3.</span>
                      
                        <p>
                                       <span class="mathjax-tex">\(A_{\mathrm{s}} = \langle{\tilde{\boldsymbol{\alpha }}_{1}},{\tilde{\boldsymbol{\alpha}}_{\infty}}, \{ \mathbf {A}_{\sigma}\}\rangle\)</span>
									                              <i>realizes</i>
									                              <i>f</i>
                                       <sub>s</sub>.</p>
                      
                    </li>
                  </ol>
                        
              
                <h3 class="c-article__sub-heading u-h3">Proof</h3>
                <p>In the first place we note that because <i>A</i> is irredundant we have </p><div id="Eque" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex"> $$\begin{aligned} {\tilde{\boldsymbol{\alpha}}_1}^\top= \boldsymbol{ \alpha}_1^\top\sum_{k \geq0} \mathbf {A}^k = \sum_{x \in \varSigma^\star} \boldsymbol{ \alpha}_1^\top \mathbf {A}_x , \end{aligned}$$ </span></div></div><p> where the second equality follows from a term reordering. Similarly, we have <span class="mathjax-tex">\({\tilde{\boldsymbol{\alpha}}_{\infty}} = \sum_{x \in \varSigma^{\star}} \mathbf {A}_{x} \boldsymbol {\alpha }_{\infty }\)</span>. The rest of the proof follows from checking several implications.</p>
                <p>(1 ⇒ 2) Using <span class="mathjax-tex">\(f(x) = \boldsymbol{\alpha}_{1}^{\top} \mathbf {A}_{x} \boldsymbol {\alpha }_{\infty }\)</span> and the definition of <span class="mathjax-tex">\({\tilde{\boldsymbol{\alpha}}_{\infty}}\)</span> we have: </p><div id="Equf" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex"> $$\begin{aligned} \mathbb{P}\bigl[x \varSigma^\star\bigr] = \sum _{y \in\varSigma^\star} \mathbb{P}[x y] = \sum_{y \in\varSigma^\star} \boldsymbol{\alpha}_1^\top \mathbf {A}_x \mathbf {A}_y \boldsymbol {\alpha }_{\infty }= \boldsymbol{\alpha}_1^\top \mathbf {A}_x {\tilde{\boldsymbol{\alpha}}_{\infty}} . \end{aligned}$$ </span></div></div><p> (2 ⇒ 1) It follows from <span class="mathjax-tex">\(\mathbb{P}[x \varSigma^{+}] = \sum_{\sigma\in\varSigma} \mathbb{P}[x \sigma\varSigma^{\star}]\)</span> that </p><div id="Equg" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex"> $$\begin{aligned} \mathbb{P}[x] = \mathbb{P}\bigl[x \varSigma^\star\bigr] - \mathbb{P} \bigl[x \varSigma^+\bigr] = \boldsymbol{\alpha}_1^\top \mathbf {A}_x {\tilde{\boldsymbol{\alpha}}_{\infty} } - \boldsymbol{ \alpha}_1^\top \mathbf {A}_x \mathbf {A}{\tilde{\boldsymbol{\alpha }}_{\infty}} = \boldsymbol{\alpha}_1^\top \mathbf {A}_x (\mathbf {I}- \mathbf {A}) {\tilde{\boldsymbol{\alpha}}_{\infty}} . \end{aligned}$$ </span></div></div><p> (1 ⇒ 3) Since we can write <span class="mathjax-tex">\(\sum_{w \in\varSigma^{\star}} \mathbb{P}[w] |w|_{x} = \mathbb{P}[\varSigma ^{\star}x \varSigma^{\star}]\)</span>, it follows that </p><div id="Equh" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex"> $$\begin{aligned} \mathbb{E}[|w|_x] = \sum_{w \in\varSigma^\star} \mathbb{P}[w] |w|_x = \sum_{u, v \in\varSigma^\star} \mathbb{P}[u x v] = \sum _{u, v \in \varSigma^\star} \boldsymbol{\alpha}_1 ^\top \mathbf {A}_u \mathbf {A}_x \mathbf {A}_v \boldsymbol {\alpha }_{\infty }= {\tilde{\boldsymbol{ \alpha}}_1}^\top \mathbf {A}_x {\tilde{\boldsymbol{ \alpha}}_{\infty}} . \end{aligned}$$ </span></div></div><p> (3 ⇒ 1) Using similar arguments as before we observe that </p><div id="Equi" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex"> $$\begin{aligned} \mathbb{P}[x] =&amp; \mathbb{P}\bigl[\varSigma^\star x \varSigma^\star \bigr] + \mathbb{P}\bigl[\varSigma^+ x \varSigma^+\bigr] - \mathbb{P} \bigl [\varSigma^+ x \varSigma^\star\bigr] - \mathbb{P}\bigl[\varSigma^\star x \varSigma^+ \bigr] \\ =&amp; {\tilde{\boldsymbol{\alpha}}_1}^\top \mathbf {A}_x { \tilde{\boldsymbol{\alpha}}_{\infty}} + {\tilde{\boldsymbol{ \alpha}}_1}^\top \mathbf {A} \mathbf {A}_x \mathbf {A}{\tilde{\boldsymbol{ \alpha}}_{\infty} } - {\tilde{\boldsymbol{\alpha}}_1}^\top \mathbf {A} \mathbf {A}_x {\tilde{\boldsymbol{\alpha}}_{\infty}} - {\tilde{ \boldsymbol{\alpha}}_1}^\top \mathbf {A}_x \mathbf {A}{\tilde{ \boldsymbol{\alpha}}_{\infty}} \\ =&amp; {\tilde{\boldsymbol{\alpha}}_1}^\top(\mathbf {I} - \mathbf {A}) \mathbf {A}_x (\mathbf {I} - \mathbf {A}) {\tilde{\boldsymbol{\alpha}}_{\infty}} . \end{aligned}$$ </span></div></div><p> □</p>
              <p>A direct consequence of this constructive result is that given a WFA realizing a probability distribution <span class="mathjax-tex">\(\mathbb{P}[x]\)</span> we can easily compute WFA realizing the functions <i>f</i>
                        <sub>p</sub> and <i>f</i>
                        <sub>s</sub>; and the converse holds as well. Lemma 1 also implies the following result, which characterizes the rank of <i>f</i>
                        <sub>p</sub> and <i>f</i>
                        <sub>s</sub>.</p>
                <h3 class="c-article__sub-heading u-h3">Corollary 1</h3>
                <p>
                           <i>Suppose</i>
					                      <span class="mathjax-tex">\(f : \varSigma^{\star}\rightarrow\mathbb{R}\)</span>
					                      <i>is stochastic and admits a minimal irredundant WFA</i>. <i>Then</i>
					                      <span class="mathjax-tex">\(\operatorname{rank}(f) = \operatorname {rank}(f_{\mathrm{p}}) = \operatorname{rank} (f_{\mathrm{s}})\)</span>.</p>
              
                <h3 class="c-article__sub-heading u-h3">Proof</h3>
                <p>Since all the constructions of Lemma 1 preserve the number of states, the result follows from considering minimal WFA for <i>f</i>, <i>f</i>
                           <sub>p</sub>, and <i>f</i>
                           <sub>s</sub>. □</p>
              <p>From the point of view of learning, Lemma 1 provides us with tools for proving two-sided reductions between the problems of learning <i>f</i>, <i>f</i>
                        <sub>p</sub>, and <i>f</i>
                        <sub>s</sub>. Since for all these problems the corresponding empirical Hankel matrices can be easily computed, this implies that for each particular task we can use the statistics which better suit its needs. For example, if we are interested in learning a model that predicts the next symbol in a string we might learn the function <i>f</i>
                        <sub>p</sub>. On the other hand, if we want to predict missing symbols in the middle of string we might learn the distribution <i>f</i> itself. Using Lemma 1 we see that both could be learned from substring statistics.</p></div></div></section><section aria-labelledby="Sec8"><div class="c-article-section" id="Sec8-section"><h2 class="c-article-section__title u-h2 js-section-title js-c-reading-companion-sections-item" id="Sec8">Duality, spectral learning, and forward-backward decompositions</h2><div class="c-article-section__content" id="Sec8-content"><p>In this section we give a derivation of the spectral learning algorithm. Our approach follows from a duality result between minimal WFA and factorizations of Hankel matrices. We begin by presenting this duality result and some of its consequences. Afterwards we proceed to describe the spectral method, which is just an efficient implementation of the arguments used in the proof of the duality result. Finally we give an interpretation of this method from the point of view of forward and backward recursions in finite automata. This provides extra intuitions about the method and stresses the role played by factorizations in its derivation.</p><h3 class="c-article__sub-heading u-h3" id="Sec9">Duality and minimal weighted automata</h3><p>Let <i>f</i> be a real function on strings and <b>H</b>
                           <sub>
                    <i>f</i>
                  </sub> its Hankel matrix. In this section we consider factorizations of <b>H</b>
                           <sub>
                    <i>f</i>
                  </sub> and minimal WFA for <i>f</i>. We will show that there exists an interesting relation between these two concepts. This relation will motivate the algorithm presented on next section that factorizes a (sub-block of a) Hankel matrix in order to learn a WFA for some unknown function.</p><p>Our initial observation is that a WFA <i>A</i>=〈<b><i>α</i></b>
                           <sub>1</sub>,<b><i>α</i></b>
                           <sub>∞</sub>,{<b>A</b>
                           <sub>
                    <i>σ</i>
                  </sub>}〉 for <i>f</i> with <i>n</i> states induces a factorization of <b>H</b>
                           <sub>
                    <i>f</i>
                  </sub>. Let <span class="mathjax-tex">\(\mathbf {P} \in\mathbb{R}^{\varSigma^{\star}\times n}\)</span> be a matrix whose <i>u</i>th row equals <span class="mathjax-tex">\(\boldsymbol{\alpha}_{1}^{\top} \mathbf {A}_{u}\)</span> for any <i>u</i>∈<i>Σ</i>
                           <sup>⋆</sup>. Furthermore, let <span class="mathjax-tex">\(\mathbf {S} \in\mathbb{R}^{n \times\varSigma^{\star}}\)</span> be a matrix whose columns are of the form <b>A</b>
                           <sub>
                    <i>v</i>
                  </sub>
                           <b><i>α</i></b>
                           <sub>∞</sub> for all <i>v</i>∈<i>Σ</i>
                           <sup>⋆</sup>. It is trivial to check that one has <b>H</b>
                           <sub>
                    <i>f</i>
                  </sub>=<b>PS</b>. The same happens for sub-blocks: if <span class="mathjax-tex">\(\mathbf {H}_{\mathcal{B}}\)</span> is a sub-block of <b>H</b>
                           <sub>
                    <i>f</i>
                  </sub> defined over an arbitrary basis <span class="mathjax-tex">\(\mathcal{B}= (\mathcal{P},\mathcal{S})\)</span>, then the corresponding restrictions <span class="mathjax-tex">\(\mathbf {P}_{\mathcal{B}}\in\mathbb{R}^{\mathcal{P}\times n}\)</span> and <span class="mathjax-tex">\(\mathbf {S}_{\mathcal{B}}\in\mathbb{R}^{n \times \mathcal{S}}\)</span> of <b>P</b> and <b>S</b> induce the factorization <span class="mathjax-tex">\(\mathbf {H}_{\mathcal{B}}= \mathbf {P}_{\mathcal{B}} \mathbf {S}_{\mathcal{B}}\)</span>. Furthermore, if <b>H</b>
                           <sub>
                    <i>σ</i>
                  </sub> is a sub-block of the matrix <span class="mathjax-tex">\(\mathbf {H}_{\mathcal{B} ^{\prime}}\)</span> corresponding to the prefix-closure of <span class="mathjax-tex">\(\mathbf {H}_{\mathcal{B}}\)</span>, then we also have the factorization <span class="mathjax-tex">\(\mathbf {H}_{\sigma}= \mathbf {P}_{\mathcal{B}} \mathbf {A}_{\sigma} \mathbf {S}_{\mathcal{B}}\)</span>.</p><p>An interesting consequence of this construction is that if <i>A</i> is minimal for <i>f</i>—i.e. <span class="mathjax-tex">\(n = \operatorname{rank}(f)\)</span>—then the factorization <b>H</b>
                           <sub>
                    <i>f</i>
                  </sub>=<b>PS</b> is in fact a <i>rank factorization</i>. Since in general <span class="mathjax-tex">\(\operatorname{rank}( \mathbf {H}_{\mathcal{B}}) \leq n\)</span>, in this case the factorization <span class="mathjax-tex">\(\mathbf {H}_{\mathcal{B}} = \mathbf {P}_{\mathcal{B}}\mathbf {S}_{\mathcal{B}}\)</span> is a rank factorization if and only if <span class="mathjax-tex">\(\mathbf {H}_{\mathcal{B}}\)</span> is a complete sub-block. Thus, we see that a minimal WFA that realizes a function <i>f</i> induces a rank factorization on any complete sub-block of <b>H</b>
                           <sub>
                    <i>f</i>
                  </sub>. The converse is even more interesting: give a rank factorization of a complete sub-block of <b>H</b>
                           <sub>
                    <i>f</i>
                  </sub>, one can compute a minimal WFA for <i>f</i>.</p><p>Let <b>H</b> be a complete sub-block of <b>H</b>
                           <sub>
                    <i>f</i>
                  </sub> defined by the basis <span class="mathjax-tex">\(\mathcal{B}= (\mathcal{P},\mathcal{S})\)</span> and let <b>H</b>
                           <sub>
                    <i>σ</i>
                  </sub> denote the sub-block of the prefix-closure of <b>H</b> corresponding to the basis <span class="mathjax-tex">\((\mathcal{P}\sigma, \mathcal{S})\)</span>. Let <span class="mathjax-tex">\(\mathbf {h}_{\mathcal {P},\lambda }\in\mathbb{R}^{\mathcal{P}}\)</span> denote the <i>p</i>-dimensional vector with coordinates <span class="mathjax-tex">\(\mathbf {h}_{\mathcal {P},\lambda }(u) = f(u)\)</span>, and <span class="mathjax-tex">\(\mathbf {h}_{\lambda ,\mathcal {S}}\in \mathbb{R}^{\mathcal{S}}\)</span> the <i>s</i>-dimensional vector with coordinates <span class="mathjax-tex">\(\mathbf {h}_{\lambda ,\mathcal {S}}(v) = f(v)\)</span>. Now we can state our result.</p>
                  <h3 class="c-article__sub-heading u-h3">Lemma 2</h3>
                  <p>
                              <i>If</i>
						                        <b>H</b>=<b>PS</b>
						                        <i>is a rank factorization</i>, <i>then the WFA</i>
						                        <i>A</i>=〈<b><i>α</i></b>
                              <sub>1</sub>,<b><i>α</i></b>
                              <sub>∞</sub>,{<b>A</b>
                              <sub>
                      <i>σ</i>
                    </sub>}〉 <i>with</i>
						                        <span class="mathjax-tex">\(\boldsymbol{\alpha}_{1}^{\top}= \mathbf {h}_{\lambda ,\mathcal {S}}^{\top} \mathbf {S}^{+}\)</span>, <span class="mathjax-tex">\(\boldsymbol {\alpha }_{\infty }= \mathbf {P}^{+} \mathbf {h}_{\mathcal {P},\lambda }\)</span>, <i>and</i>
						                        <b>A</b>
                              <sub>
                      <i>σ</i>
                    </sub>=<b>P</b>
                              <sup>+</sup>
                              <b>H</b>
                              <sub>
                      <i>σ</i>
                    </sub>
                              <b>S</b>
                              <sup>+</sup>, <i>is minimal for</i>
						                        <i>f</i>.</p>
                
                  <h3 class="c-article__sub-heading u-h3">Proof</h3>
                  <p>Let <span class="mathjax-tex">\(A^{\prime}= \langle \boldsymbol {\alpha }_{1}', \boldsymbol {\alpha }_{\infty }', \{\mathbf {A}'_{\sigma}\} \rangle \)</span> be a minimal WFA for <i>f</i> that induces a rank factorization <b>H</b>=<b>P</b>′<b>S</b>′. It suffices to show that there exists an invertible <b>M</b> such that <b>M</b>
                              <sup>−1</sup>
                              <i>A</i>′<b>M</b>=<i>A</i>. Define <b>M</b>=<b>S</b>′<b>S</b>
                              <sup>+</sup> and note that <b>P</b>
                              <sup>+</sup>
                              <b>P</b>′<b>S</b>′<b>S</b>
                              <sup>+</sup>=<b>P</b>
                              <sup>+</sup>
                              <b>HS</b>
                              <sup>+</sup>=<b>I</b> implies that <b>M</b> is invertible with <b>M</b>
                              <sup>−1</sup>=<b>P</b>
                              <sup>+</sup>
                              <b>P</b>′. Now we check that the operators of <i>A</i> correspond to the operators of <i>A</i>′ under this change of basis. First we see that <span class="mathjax-tex">\(\mathbf {A}_{\sigma}= \mathbf {P}^{+} \mathbf {H}_{\sigma} \mathbf {S}^{+} = \mathbf {P}^{+} \mathbf {P}^{\prime} \mathbf {A}_{\sigma}^{\prime} \mathbf {S}^{\prime} \mathbf {S}^{+} = \mathbf {M}^{-1} \mathbf {A}_{\sigma}^{\prime} \mathbf {M}\)</span>. Now observe that by the construction of <b>S</b>′ and <b>P</b>′ we have <span class="mathjax-tex">\({\boldsymbol{\alpha}_{1}^{\prime}}^{\top} \mathbf {S}^{\prime}= \mathbf {h}_{\lambda ,\mathcal {S}}\)</span>, and <span class="mathjax-tex">\(\mathbf {P}^{\prime} \boldsymbol {\alpha }_{\infty }^{\prime}= \mathbf {h}_{\mathcal {P},\lambda }\)</span>. Thus, it follows that <span class="mathjax-tex">\(\boldsymbol{\alpha}_{1}^{\top}= {\boldsymbol {\alpha}_{1}^{\prime}}^{\top} \mathbf {M}\)</span> and <span class="mathjax-tex">\(\boldsymbol {\alpha }_{\infty }= \mathbf {M}^{-1} \boldsymbol {\alpha }_{\infty }^{\prime}\)</span>. □</p>
                <p>This result shows that there exists a duality between rank factorizations of complete sub-blocks of <b>H</b>
                           <sub>
                    <i>f</i>
                  </sub> and minimal WFA for <i>f</i>. A consequence of this duality is that all minimal WFA for a function <i>f</i> are related via some change of basis. In other words, modulo change of basis, there exists a unique minimal WFA for any function <i>f</i> of finite rank.</p>
                  <h3 class="c-article__sub-heading u-h3">Corollary 2</h3>
                  <p>
                              <i>Let</i>
						                        <i>A</i>=〈<b><i>α</i></b>
                              <sub>1</sub>,<b><i>α</i></b>
                              <sub>∞</sub>,{<b>A</b>
                              <sub>
                      <i>σ</i>
                    </sub>}〉 <i>and</i>
						                        <span class="mathjax-tex">\(A^{\prime}= \langle \boldsymbol {\alpha }_{1}', \boldsymbol {\alpha }_{\infty }', \{\mathbf {A}'_{\sigma}\} \rangle \)</span>
						                        <i>be minimal WFA for some</i>
						                        <i>f</i>
						                        <i>of rank</i>
						                        <i>n</i>. <i>Then there exists an invertible matrix</i>
						                        <span class="mathjax-tex">\(\mathbf {M} \in\mathbb{R}^{n \times n}\)</span>
						                        <i>such that</i>
						                        <i>A</i>=<b>M</b>
                              <sup>−1</sup>
                              <i>A</i>′<b>M</b>.</p>
                
                  <h3 class="c-article__sub-heading u-h3">Proof</h3>
                  <p>Suppose that <b>H</b>
                              <sub>
                      <i>f</i>
                    </sub>=<b>PS</b>=<b>P</b>′<b>S</b>′ are the rank factorizations induced by <i>A</i> and <i>A</i>′ respectively. Then, by the same arguments used in Lemma 2, the matrix <b>M</b>=<b>S</b>′<b>S</b>
                              <sup>+</sup> is invertible and satisfies the equation <i>A</i>=<b>M</b>
                              <sup>−1</sup>
                              <i>A</i>′<b>M</b>. □</p>
                <h3 class="c-article__sub-heading u-h3" id="Sec10">A spectral learning algorithm</h3><p>The spectral method is basically an efficient algorithm that implements the ideas in the proof of Lemma 2 to find a rank factorization of a complete sub-block <b>H</b> of <b>H</b>
                           <sub>
                    <i>f</i>
                  </sub> and obtain from it a minimal WFA for <i>f</i>. The term <i>spectral</i> comes from the fact that it uses SVD, a type of spectral decomposition. We describe the algorithm in detail in this section and give a complete set of experiments that explores the practical behavior of this method in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10994-013-5416-x#Sec14">5</a>.</p><p>Suppose <span class="mathjax-tex">\(f : \varSigma^{\star}\rightarrow\mathbb{R}\)</span> is an unknown function of finite rank <i>n</i> and we want to compute a minimal WFA for it. Let us assume that we know that <span class="mathjax-tex">\(\mathcal{B}= (\mathcal{P},\mathcal {S})\)</span> is a complete basis for <i>f</i>. Our algorithm receives as input: the basis <span class="mathjax-tex">\(\mathcal{B}\)</span> and the values of <i>f</i> on a set of strings <span class="mathjax-tex">\(\mathcal{W}\)</span>. In particular, we assume that <span class="mathjax-tex">\(\mathcal{P} \varSigma^{\prime}\mathcal{S}\cup\mathcal{P}\cup\mathcal{S}\subseteq\mathcal{W}\)</span>. It is clear that using these values of <i>f</i> the algorithm can compute sub-blocks <b>H</b>
                           <sub>
                    <i>σ</i>
                  </sub> for <i>σ</i>∈<i>Σ</i>′ of <b>H</b>
                           <sub>
                    <i>f</i>
                  </sub>. Furthermore, it can compute the vectors <span class="mathjax-tex">\(\mathbf {h}_{\lambda ,\mathcal {S}}\)</span> and <span class="mathjax-tex">\(\mathbf {h}_{\mathcal {P},\lambda }\)</span>. Thus, the algorithm only needs a rank factorization of <b>H</b>
                           <sub>
                    <i>λ</i>
                  </sub> to be able to apply the formulas given in Lemma 2.</p><p>Recall that the <i>compact SVD</i> of a <i>p</i>×<i>s</i> matrix <b>H</b>
                           <sub>
                    <i>λ</i>
                  </sub> of rank <i>n</i> is given by the expression <span class="mathjax-tex">\(\mathbf {H}_{\lambda}= \mathbf {U} \mathbf {\varLambda} \mathbf {V}^{\top}\)</span>, where <span class="mathjax-tex">\(\mathbf {U} \in\mathbb{R}^{p \times n}\)</span> and <span class="mathjax-tex">\(\mathbf {V} \in\mathbb{R} ^{s \times n}\)</span> are orthogonal matrices, and <span class="mathjax-tex">\(\mathbf {\varLambda} \in\mathbb{R}^{n \times n}\)</span> is a diagonal matrix containing the singular values of <b>H</b>
                           <sub>
                    <i>λ</i>
                  </sub>. The most interesting property of compact SVD for our purposes is that <span class="mathjax-tex">\(\mathbf {H}_{\lambda}= (\mathbf {U} \mathbf {\varLambda}) \mathbf {V}^{\top}\)</span> is a rank factorization. We will use this factorization in the algorithm, but write it in a different way. Note that since <b>V</b> is orthogonal we have <b>V</b>
                           <sup>⊤</sup>
                           <b>V</b>=<b>I</b>, and in particular <b>V</b>
                           <sup>+</sup>=<b>V</b>
                           <sup>⊤</sup>. Thus, the factorization above is equivalent to <b>H</b>
                           <sub>
                    <i>λ</i>
                  </sub>=(<b>H</b>
                           <sub>
                    <i>λ</i>
                  </sub>
                           <b>V</b>)<b>V</b>
                           <sup>⊤</sup>.</p><p>With this factorization, equations from Lemma 2 are written as follows: </p><div id="Equj" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex"> $$\begin{aligned} \boldsymbol{\alpha}_1^\top =&amp; \mathbf {h}_{\lambda ,\mathcal {S}}^\top \mathbf {V} , \\ \boldsymbol {\alpha }_{\infty } =&amp; ( \mathbf {H}_\lambda \mathbf {V})^+ \mathbf {h}_{\mathcal {P},\lambda }, \\ \mathbf {A}_\sigma = &amp;( \mathbf {H}_\lambda \mathbf {V})^+ \mathbf {H}_\sigma \mathbf {V} . \end{aligned}$$ </span></div></div><p> These equations define what we call from now on the <i>spectral learning</i> algorithm. The running time of the algorithm can be bound as follows. Note that the cost of computing a compact SVD and the pseudo-inverse is <span class="mathjax-tex">\(O(|\mathcal{P}|~|\mathcal{S}| n)\)</span>, and the cost of computing the operators is <span class="mathjax-tex">\(O(|\varSigma|~|\mathcal{P}| n^{2})\)</span>. To this we need to add the time required in order to compute the Hankel matrices given to the algorithm. In the particular case of stochastic WFA described in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10994-013-5416-x#Sec7">3</a>, approximate Hankel matrices can be computed from a sample <i>S</i> containing <i>m</i> examples in time <i>O</i>(<i>m</i>)—note that the running time of all linear algebra operations is independent of the sample size. Thus, we get a total running time of <span class="mathjax-tex">\(O(m + n |\mathcal{P}| |\mathcal {S}| + n^{2} |\mathcal{P} |~|\varSigma|)\)</span> for the spectral algorithm applied to learn any stochastic function of the type described in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10994-013-5416-x#Sec7">3</a>.</p><h4 class="c-article__sub-heading u-h3 c-article__sub-heading--light" id="Sec11">Sample complexity of spectral learning</h4><p>The spectral algorithm we just described can be used even when <b>H</b> and <b>H</b>
                              <sub>
                      <i>σ</i>
                    </sub> are not known exactly, but approximations <span class="mathjax-tex">\(\hat{ \mathbf {H}}\)</span> and <span class="mathjax-tex">\(\hat{ \mathbf {H}}_{\sigma}\)</span> are available. In this context, an approximation means that we have an estimate for each entry in these matrices; that is, we know an estimate of <i>f</i> for every string in <span class="mathjax-tex">\(\mathcal{W}\)</span>. A different concept of approximation could be that one knows <i>f</i> exactly in some, but not all strings in <span class="mathjax-tex">\(\mathcal{W}\)</span>. In this context, one can still apply the spectral method after a preliminary matrix completion step; see Balle and Mohri (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="&#xA;Balle, B., &amp; Mohri, M. (2012). Spectral learning of general weighted automata via constrained matrix completion. In Advances in neural information processing systems (Vol. 25, pp. 2168–2176).&#xA;" href="/article/10.1007/s10994-013-5416-x#ref-CR9" id="ref-link-section-d84034e4140">2012</a>) for details. When the goal is to learn a probability distribution over strings—or prefixes, or substrings—we are always in the first of these two settings. In these cases we can apply the spectral algorithm directly using empirical estimations <span class="mathjax-tex">\(\hat{ \mathbf {H}}\)</span> and <span class="mathjax-tex">\(\hat{ \mathbf {H}}_{\sigma}\)</span>. A natural question is then how close to <i>f</i> is the approximate function <span class="mathjax-tex">\(\hat{f}\)</span> computed by the learned automaton <span class="mathjax-tex">\(\hat{A}\)</span>. Experiments described in the following sections explore this question from an empirical perspective and compare the performance of spectral learning with other approaches. Here we give a very brief outline of what is known about the <i>sample complexity</i> of spectral learning. Since an in-depth discussion of these results and the techniques used in their proofs is outside the scope of this paper, for further details we refer the reader to papers where these bounds were originally presented (Hsu et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="&#xA;Hsu, D., Kakade, S. M., &amp; Zhang, T. (2009). A spectral algorithm for learning hidden Markov models. In Proceedings of the annual conference on computational learning theory (COLT).&#xA;" href="/article/10.1007/s10994-013-5416-x#ref-CR30" id="ref-link-section-d84034e4194">2009</a>; Bailly et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="&#xA;Bailly, R., Denis, F., &amp; Ralaivola, L. (2009). Grammatical inference as a principal component analysis problem. In L. Bottou &amp; M. Littman (Eds.), Proceedings of the 26th international conference on machine learning (pp. 33–40). Montreal: Omnipress.&#xA;" href="/article/10.1007/s10994-013-5416-x#ref-CR5" id="ref-link-section-d84034e4198">2009</a>; Siddiqi et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="&#xA;Siddiqi, S. M., Boots, B., &amp; Gordon, G. J. (2010). Reduced-rank hidden Markov models. In Proceedings of the thirteenth international conference on artificial intelligence and statistics (AISTATS) (pp. 741–748).&#xA;" href="/article/10.1007/s10994-013-5416-x#ref-CR48" id="ref-link-section-d84034e4201">2010</a>; Bailly <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="&#xA;Bailly, R. (2011). Quadratic weighted automata: spectral algorithm and likelihood maximization. Journal of Machine Learning Research—Proceedings Track, 20, 147–163.&#xA;" href="/article/10.1007/s10994-013-5416-x#ref-CR4" id="ref-link-section-d84034e4204">2011</a>; Balle <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="&#xA;Balle, B. (2013). Learning finite-state machines: algorithmic and statistical aspects. PhD thesis, Universitat Politècnica de Catalunya.&#xA;" href="/article/10.1007/s10994-013-5416-x#ref-CR8" id="ref-link-section-d84034e4207">2013</a>).</p><p>All known results about learning stochastic WFA with spectral methods fall into the well-known <i>PAC-learning</i> framework (Valiant <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1984" title="&#xA;Valiant, L. G. (1984). A theory of the learnable. Communications of the ACM, 27(11), 1134–1142.&#xA;" href="/article/10.1007/s10994-013-5416-x#ref-CR51" id="ref-link-section-d84034e4216">1984</a>; Kearns et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1994" title="&#xA;Kearns, M., Mansour, Y., Ron, D., Rubinfeld, R., Schapire, R. E., &amp; Sellie, L. (1994). On the learnability of discrete distributions. STOC ’94. In Proceedings of the twenty-sixth annual ACM symposium on theory of computing (pp. 273–282). New York: ACM.&#xA;" href="/article/10.1007/s10994-013-5416-x#ref-CR32" id="ref-link-section-d84034e4219">1994</a>). In particular, assuming that a large enough sample of i.i.d. strings drawn from some distribution <i>f</i> over <i>Σ</i>
                              <sup>⋆</sup> realized by a WFA is given to the spectral learning algorithm, we know that with high probability the output WFA computes a function <span class="mathjax-tex">\(\hat{f}\)</span> that is close to <i>f</i>. Sample bounds in this type of results usually depend polynomially on the usual PAC parameters—accuracy <i>ε</i> and confidence <i>δ</i>—as well as other parameters depending on the target <i>f</i>: the size of the alphabet <i>Σ</i>, the number of states <i>n</i> of a minimal WFA realizing <i>f</i>, the size of the basis <span class="mathjax-tex">\(\mathcal{B}\)</span>, and the smallest singular values of <b>H</b> and other related matrices.</p><p>These results come in different flavors, depending on what assumptions are made on the automaton computing <i>f</i> and what criteria is used to measure how close <span class="mathjax-tex">\(\hat{f}\)</span> is to <i>f</i>. When <i>f</i> can be realized by a Hidden Markov Model (HMM), Hsu et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="&#xA;Hsu, D., Kakade, S. M., &amp; Zhang, T. (2009). A spectral algorithm for learning hidden Markov models. In Proceedings of the annual conference on computational learning theory (COLT).&#xA;" href="/article/10.1007/s10994-013-5416-x#ref-CR30" id="ref-link-section-d84034e4301">2009</a>) proved a PAC-learning result under the L<sub>1</sub> distance restricted to strings in <i>Σ</i>
                              <sup><i>t</i></sup> for some <i>t</i>≥0—their bound depends polynomially in <i>t</i>. A similar result was obtained in Siddiqi et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="&#xA;Siddiqi, S. M., Boots, B., &amp; Gordon, G. J. (2010). Reduced-rank hidden Markov models. In Proceedings of the thirteenth international conference on artificial intelligence and statistics (AISTATS) (pp. 741–748).&#xA;" href="/article/10.1007/s10994-013-5416-x#ref-CR48" id="ref-link-section-d84034e4320">2010</a>) for Reduced Rank HMM. For targets <i>f</i> computed by a general stochastic WFA, Bailly et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="&#xA;Bailly, R., Denis, F., &amp; Ralaivola, L. (2009). Grammatical inference as a principal component analysis problem. In L. Bottou &amp; M. Littman (Eds.), Proceedings of the 26th international conference on machine learning (pp. 33–40). Montreal: Omnipress.&#xA;" href="/article/10.1007/s10994-013-5416-x#ref-CR5" id="ref-link-section-d84034e4327">2009</a>) gave a similar results under the milder L<sub>∞</sub> distance. When <i>f</i> can be computed by a Quadratic WFA one can obtain L<sub>1</sub> bounds over all <i>Σ</i>
                              <sup>⋆</sup>; see Bailly (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="&#xA;Bailly, R. (2011). Quadratic weighted automata: spectral algorithm and likelihood maximization. Journal of Machine Learning Research—Proceedings Track, 20, 147–163.&#xA;" href="/article/10.1007/s10994-013-5416-x#ref-CR4" id="ref-link-section-d84034e4343">2011</a>). The case where the function can be computed by a Probabilistic WFA was analyzed in Balle (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="&#xA;Balle, B. (2013). Learning finite-state machines: algorithmic and statistical aspects. PhD thesis, Universitat Politècnica de Catalunya.&#xA;" href="/article/10.1007/s10994-013-5416-x#ref-CR8" id="ref-link-section-d84034e4346">2013</a>), where L<sub>1</sub> bounds over strings in <i>Σ</i>
                              <sup>≤<i>t</i></sup> are given. It is important to note that, with the exception of Bailly (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="&#xA;Bailly, R. (2011). Quadratic weighted automata: spectral algorithm and likelihood maximization. Journal of Machine Learning Research—Proceedings Track, 20, 147–163.&#xA;" href="/article/10.1007/s10994-013-5416-x#ref-CR4" id="ref-link-section-d84034e4360">2011</a>), none of these methods is guaranteed to return a stochastic WFA. That is, though the hypothesis <span class="mathjax-tex">\(\hat{f}\)</span> is close to a probability distribution in L<sub>1</sub> distance, it does not necessarily assign a non-negative number to each strings, much less adds up to one when summed over all strings—though both properties are satisfied <i>in the limit</i>. In practice this is a problem when trying to evaluate these methods using perplexity-like accuracy measures. We do not face this difficulty in our experiments because we use WER-like accuracy measures. See the discussion in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10994-013-5416-x#Sec29">8</a> for pointers to some attempts to solve this problem.</p><p>Despite their formal differences, all these PAC-learning results rely on similar proof techniques. Roughly speaking, the following three principles lay at the bottom of these results: </p><ol class="u-list-style-none">
                      <li>
                        <span class="u-custom-list-number">1.</span>
                        
                          <p>Convergence of empirical estimates <span class="mathjax-tex">\(\hat{ \mathbf {H}}\)</span> and <span class="mathjax-tex">\(\hat{\mathbf {H}}_{\sigma}\)</span> to their true values at a rate of <i>O</i>(<i>m</i>
                                          <sup>−1/2</sup>) in terms of Frobenius norms; here <i>m</i> is the sample size.</p>
                        
                      </li>
                      <li>
                        <span class="u-custom-list-number">2.</span>
                        
                          <p>Stability of linear algebra operations—SVD, pseudoinverse and matrix multiplication—under small perturbations. This implies that when the errors in empirical Hankel matrices are small, we get operators <span class="mathjax-tex">\(\hat {\boldsymbol{\alpha}}_{1}\)</span>, <span class="mathjax-tex">\(\hat{\boldsymbol{\alpha}}_{\infty}\)</span>, and <span class="mathjax-tex">\(\hat{ \mathbf {A}}_{\sigma}\)</span> which are close to their true values, modulo a change of basis.</p>
                        
                      </li>
                      <li>
                        <span class="u-custom-list-number">3.</span>
                        
                          <p>Mild aggregation of errors when computing <span class="mathjax-tex">\(\sum |f(x) - \hat {f}(x)|\)</span> over large sets of strings.</p>
                        
                      </li>
                    </ol><p> We note here that the first of these points, which we already mentioned in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10994-013-5416-x#Sec7">3</a>, is enough to show the statistical consistency of spectral learning. The other two points are rather technical and lie at the core of finite-sample analyses of spectral learning of stochastic WFA.</p><h4 class="c-article__sub-heading u-h3 c-article__sub-heading--light" id="Sec12">Choosing the parameters</h4><p>When run with approximate data <span class="mathjax-tex">\(\hat{ \mathbf {H}}_{\lambda}\)</span>, <span class="mathjax-tex">\(\hat{\mathbf {H}}_{\sigma}\)</span> for <i>σ</i> ∈<i>Σ</i>, <span class="mathjax-tex">\(\hat{\mathbf {h}}_{\lambda,\mathcal{S}}\)</span>, and <span class="mathjax-tex">\(\hat{\mathbf {h}}_{\mathcal{P},\lambda}\)</span>, the algorithm also receives as input the number of states <i>n</i> of the target WFA. That is because the rank of <span class="mathjax-tex">\(\hat{ \mathbf {H}}_{\lambda}\)</span> may be different from the rank of <b>H</b>
                              <sub>
                      <i>λ</i>
                    </sub> due to the noise, and in this case the algorithm may need to ignore some of the smallest singular values of <span class="mathjax-tex">\(\hat{ \mathbf {H}}_{\lambda}\)</span>, which just correspond to zeros in the original matrix that have been corrupted by noise. This is done by just computing a <i>truncated SVD</i> of <span class="mathjax-tex">\(\hat{\mathbf {H}}_{\lambda}\)</span> up to dimension <i>n</i>—we note that the cost of this computation is the same as the computation of a compact SVD on a matrix of rank <i>n</i>. It was shown in Bailly (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="&#xA;Bailly, R. (2011). Quadratic weighted automata: spectral algorithm and likelihood maximization. Journal of Machine Learning Research—Proceedings Track, 20, 147–163.&#xA;" href="/article/10.1007/s10994-013-5416-x#ref-CR4" id="ref-link-section-d84034e4616">2011</a>) that when empirical Hankel matrices are sufficiently accurate, inspection of the singular values of <span class="mathjax-tex">\(\hat{ \mathbf {H}}\)</span> can yield accurate estimates of the number of states <i>n</i> in the target. In practice one usually chooses the number of states via some sort of cross-validation procedure. We will get back to this issue in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10994-013-5416-x#Sec14">5</a>.</p><p>The other important parameter to choose when using the spectral algorithm is the basis. It is easy to show that for functions of rank <i>n</i> there always exist complete basis with <span class="mathjax-tex">\(|\mathcal{P}| = |\mathcal{S}| = n\)</span>. In general there exist infinitely many complete basis and it is safe to assume in theoretical results that at least one is given to the algorithm. However, choosing a basis in practice turns out to be a complex task. A common choice are basis of the form <span class="mathjax-tex">\(\mathcal{P}= \mathcal{S}= \varSigma^{\leq k}\)</span> for some <i>k</i>&gt;0 (Hsu et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="&#xA;Hsu, D., Kakade, S. M., &amp; Zhang, T. (2009). A spectral algorithm for learning hidden Markov models. In Proceedings of the annual conference on computational learning theory (COLT).&#xA;" href="/article/10.1007/s10994-013-5416-x#ref-CR30" id="ref-link-section-d84034e4668">2009</a>; Siddiqi et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="&#xA;Siddiqi, S. M., Boots, B., &amp; Gordon, G. J. (2010). Reduced-rank hidden Markov models. In Proceedings of the thirteenth international conference on artificial intelligence and statistics (AISTATS) (pp. 741–748).&#xA;" href="/article/10.1007/s10994-013-5416-x#ref-CR48" id="ref-link-section-d84034e4672">2010</a>). Another approach, is to choose a basis that contains the most frequent elements observed in the sample, which depending on the particular target model can be either strings, prefixes, suffixes, or substrings. This approach is motivated by the theoretical results from Balle et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="&#xA;Balle, B., Quattoni, A., &amp; Carreras, X. (2012). Local loss optimization in operator models: a new insight into spectral learning. In J. Langford &amp; J. Pineau (Eds.), Proceedings of the 29th international conference on machine learning (ICML-2012), ICML’12 (pp. 1879–1886). New York: Omnipress.&#xA;" href="/article/10.1007/s10994-013-5416-x#ref-CR11" id="ref-link-section-d84034e4675">2012</a>). It is shown there that a random sampling strategy will succeed with high probability in finding a complete basis when given a large enough sample. This suggests that including frequent prefixes and suffixes might be a good heuristic. This approach is much faster than the greedy heuristic presented in Wiewiora (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="&#xA;Wiewiora, E. (2005). Learning predictive representations from a history. In Proceedings of the 22nd international conference on machine learning (pp. 964–971). New York: ACM.&#xA;" href="/article/10.1007/s10994-013-5416-x#ref-CR52" id="ref-link-section-d84034e4678">2005</a>), which for each prefix added to the basis makes a computation taking exponential time in the number of states <i>n</i>. Other authors suggest using the largest Hankel matrix that can be estimated using the given sample; that is, build a basis that includes every prefix and suffix seen in the sample (Bailly et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="&#xA;Bailly, R., Denis, F., &amp; Ralaivola, L. (2009). Grammatical inference as a principal component analysis problem. In L. Bottou &amp; M. Littman (Eds.), Proceedings of the 26th international conference on machine learning (pp. 33–40). Montreal: Omnipress.&#xA;" href="/article/10.1007/s10994-013-5416-x#ref-CR5" id="ref-link-section-d84034e4684">2009</a>). While the statistical properties of such estimation remain unclear, this approach becomes computationally unfeasible for large samples because in this case the size of the basis <i>does</i> grow with the number of examples <i>m</i>. All in all, designing an efficient algorithm for obtaining an optimal sample-dependent basis is an open problem. In our experiments we decided to adopt the simplest sample-dependent strategy: choosing the most frequent prefixes and suffixes in the sample. See Sects. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10994-013-5416-x#Sec14">5</a> and <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10994-013-5416-x#Sec23">7</a> for details.</p><h3 class="c-article__sub-heading u-h3" id="Sec13">The forward-backward interpretation</h3><p>We say that a WFA <i>A</i>=〈<b><i>α</i></b>
                           <sub>1</sub>,<b><i>α</i></b>
                           <sub>∞</sub>,{<b>A</b>
                           <sub>
                    <i>σ</i>
                  </sub>}〉 with <i>n</i> states is <i>probabilistic</i> if the following are satisfied: </p><ol class="u-list-style-none">
                    <li>
                      <span class="u-custom-list-number">1.</span>
                      
                        <p>All parameters are non-negative. That is, for all <i>σ</i>∈<i>Σ</i> and all <i>i</i>,<i>j</i>∈[<i>n</i>]: <b>A</b>
                                       <sub>
                            <i>σ</i>
                          </sub>(<i>i</i>,<i>j</i>)≥0, <b><i>α</i></b>
                                       <sub>1</sub>(<i>i</i>)≥0, and <b><i>α</i></b>
                                       <sub>∞</sub>(<i>i</i>)≥0.</p>
                      
                    </li>
                    <li>
                      <span class="u-custom-list-number">2.</span>
                      
                        <p>Initial weights add up to one: ∑<sub>
                                          <i>i</i>∈[<i>n</i>]</sub>
                                       <b><i>α</i></b>
                                       <sub>1</sub>(<i>i</i>)=1.</p>
                      
                    </li>
                    <li>
                      <span class="u-custom-list-number">3.</span>
                      
                        <p>Transition and final weights from each state add up to one. That is, for all <i>i</i>∈[<i>n</i>]: <b><i>α</i></b>
                                       <sub>∞</sub>(<i>i</i>)+∑<sub>
                                          <i>σ</i>∈<i>Σ</i>
                                       </sub>∑<sub>
                                          <i>j</i>∈[<i>n</i>]</sub>
                                       <b>A</b>
                                       <sub>
                            <i>σ</i>
                          </sub>(<i>i</i>,<i>j</i>)=1.</p>
                      
                    </li>
                  </ol><p> This model is also called in the literature <i>probabilistic finite automata</i> (PFA) or <i>probabilistic non-deterministic finite automata</i> (PNFA). It is obvious that probabilistic WFA are also stochastic, since <i>f</i>
                           <sub>
                    <i>A</i>
                  </sub>(<i>x</i>) is the probability of generating <i>x</i> using the given automaton.</p><p>It turns out that when a probabilistic WFA <i>A</i>=〈<b><i>α</i></b>
                           <sub>1</sub>,<b><i>α</i></b>
                           <sub>∞</sub>,{<b>A</b>
                           <sub>
                    <i>σ</i>
                  </sub>}〉 is considered, the factorization induced on <b>H</b> has a nice probabilistic interpretation. Analyzing the spectral algorithm from this perspective yields additional insights which are useful to keep in mind.</p><p>Let <b>H</b>
                           <sub>
                    <i>f</i>
                  </sub>=<b>PS</b> be the factorization induced by a probabilistic WFA with <i>n</i> states on the Hankel matrix of <span class="mathjax-tex">\(f_{A}(x) = f(x) = \mathbb{P}[x]\)</span>. Then, for any prefix <i>u</i>∈<i>Σ</i>
                           <sup>⋆</sup>, the <i>u</i>th row of <b>P</b> is given by the following <i>n</i>-dimensional vector: </p><div id="Equk" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex"> $$\begin{aligned} \mathbf {P}_u(i) = \mathbb{P}[u \;,\; s_{|u|+1} = i] \quad i \in[n] . \end{aligned}$$ </span></div></div><p> That is, the probability that the probabilistic transition system given by <i>A</i> generates the prefix <i>u</i> and ends up in state <i>i</i>. The coordinates of these vectors are usually called <i>forward probabilities</i>. Similarly, the column of <b>S</b> given by suffix <i>v</i>∈<i>Σ</i>
                           <sup>⋆</sup> is the <i>n</i>-dimensional vector given by: </p><div id="Equl" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex"> $$\begin{aligned} \mathbf {S}_v(i) = \mathbb{P}[v \;|\; s = i] \quad i \in[n] . \end{aligned}$$ </span></div></div><p> This is the probability of generating a suffix <i>s</i> when <i>A</i> is started from state <i>i</i>. These are usually called <i>backward probabilities</i>.</p><p>The same interpretation applies to the factorization induced on a sub-block <span class="mathjax-tex">\(\mathbf {H}_{\mathcal{B}} = \mathbf {P}_{\mathcal{B}}\mathbf {S}_{\mathcal{B}}\)</span>. Therefore, assuming there exists a minimal WFA for <span class="mathjax-tex">\(f(x) = \mathbb{P}[x]\)</span> which is probabilistic,<sup><a href="#Fn2"><span class="u-visually-hidden">Footnote </span>2</a></sup> Lemma 2 says that a WFA for <i>f</i> can be learned from information about the forward and backward probabilities over a small set of prefixes and suffixes. Teaming this basic observation with the spectral method and invariance under change of basis one can show an interesting fact: forward and backward (empirical) probabilities for a probabilistic WFA can be recovered (modulo a change of basis) by computing an SVD on (empirical) string probabilities. In other words, though state probabilities are <i>non-observable</i>, they can be recovered (modulo a linear transformation) from <i>observable</i> quantities.</p></div></div></section><section aria-labelledby="Sec14"><div class="c-article-section" id="Sec14-section"><h2 class="c-article-section__title u-h2 js-section-title js-c-reading-companion-sections-item" id="Sec14">Experiments on learning PNFA</h2><div class="c-article-section__content" id="Sec14-content"><p>In this section we present some experiments that illustrate the behavior of the spectral learning algorithm at learning weighted automata under different configurations. We also present a comparison to alternative methods for learning WFA, namely to baseline unigram and bigram methods, and to an Expectation Maximization algorithm for learning PNFA (Dempster et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1977" title="&#xA;Dempster, A. P., Laird, N. M., &amp; Rubin, D. B. (1977). Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society, 39(1), 1–38.&#xA;" href="/article/10.1007/s10994-013-5416-x#ref-CR22" id="ref-link-section-d84034e5101">1977</a>).</p><p>The data we use are sequences of part-of-speech tags of English sentences, hence the weighted automata we learn will model this type of sequential data. In Natural Language Processing, such sequential models are a central building block in methods for part-of-speech tagging. The data we used is from the Penn Treebank (Marcus et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1993" title="&#xA;Marcus, M. P., Santorini, B., &amp; Marcinkiewicz, M. A. (1993). Building a large annotated corpus of English: the Penn Treebank. Computational Linguistics, 19, 313–330.&#xA;" href="/article/10.1007/s10994-013-5416-x#ref-CR35" id="ref-link-section-d84034e5107">1993</a>), where the part-of-speech tagset consists of 45 symbols. To test the learning algorithms under different conditions, we also did experiments with a simplified tagset of 12 tags, using the mapping by Petrov et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="&#xA;Petrov, S., Das, D., &amp; McDonald, R. (2012). A universal part-of-speech tagset. In Proceedings of LREC.&#xA;" href="/article/10.1007/s10994-013-5416-x#ref-CR44" id="ref-link-section-d84034e5110">2012</a>). We used the standard partitions for training (Sects. 2 to 21, with 39,832 sequences with an average length of 23.9) and validation (Sect. 24, with 1,700 sequences with an average length of 23.6); we did not use the standard test set. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10994-013-5416-x#Fig2">2</a> shows an example sequence from the training set. </p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2"><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 2</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-category="article body" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10994-013-5416-x/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10994-013-5416-x/MediaObjects/10994_2013_5416_Fig2_HTML.gif?as=webp"></source><img aria-describedby="figure-2-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10994-013-5416-x/MediaObjects/10994_2013_5416_Fig2_HTML.gif" alt="figure2" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p>An example sentence from the training set. The bottom row is are the words, which we do not model. The top row are the part-of-speech tags using the original tagset of 45 tags. The middle row are the simplified part-of-speech tags, using a tagset of 12 symbols</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-category="article body" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10994-013-5416-x/figures/2" data-track-dest="link:Figure2 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                     <p>As a measure of error, we compute the <i>word error rate (WER)</i> on the validation set. WER computes the error at predicting the symbol that most likely follows a given prefix sequence, or predicting a special <span class="u-small-caps">stop</span> symbol if the given prefix is most likely to be a complete sequence. If <i>w</i> is a validation sequence of length <i>t</i>, we evaluate <i>t</i>+1 events, one per each symbol <i>w</i>
                        <sub>
                  <i>i</i>
                </sub> given the prefix <i>w</i>
                        <sub>1:<i>i</i>−1</sub> and one for the stopping event; note that each event is independent of the others, and that we always use the correct prefix to condition on. WER is the percentage of errors averaged over all events in the validation set.</p><p>We would like to remind the reader that a WFA learned by the spectral method is only guaranteed to realize a probabilistic distribution on <i>Σ</i>
                        <sup>∗</sup> when we use an <i>exact</i> complete sub-block of the Hankel of a stochastic function. In experiments, we only have access to a finite sample, and even though the SVD is robust to noise, we in fact observe that the WFA we obtain do not define distributions. Hence, standard evaluation metrics for probabilistic language models such as perplexity are not well defined here, and we prefer to use an error metric such as WER that does not require normalized predictions. We also avoid saying that these WFA compute probabilities over strings, and we will just say they compute scores.</p><h3 class="c-article__sub-heading u-h3" id="Sec15">Methods compared</h3><p>We now describe the weighted automata we compare, and give some details about how they were estimated and used to make predictions.</p>
                  <h3 class="c-article__sub-heading u-h3">Unigram model</h3>
                  <p>A WFA with a single state, that emits symbols according to their frequency in training data. When evaluating WER, this method will always predict the most likely symbol (in our data <span class="u-small-caps">NN</span>, which stands for singular noun).</p>
                
                  <h3 class="c-article__sub-heading u-h3">Bigram model</h3>
                  <p>A deterministic WFA with |<i>Σ</i>|+1 states, namely one special start state <i>λ</i> and one state per symbol <i>σ</i>, and the following operators: </p><ul class="u-list-style-dash">
                      <li>
                        <p>
                                       <b><i>α</i></b>
                                       <sub>1</sub>(<i>λ</i>)=1 and <b><i>α</i></b>
                                       <sub>1</sub>(<i>σ</i>)=0 for <i>σ</i>∈<i>Σ</i>
                                    </p>
                      </li>
                      <li>
                        <p>
                                       <b>A</b>
                                       <sub>
                            <i>σ</i>
                          </sub>(<i>i</i>,<i>j</i>)=0 if <i>σ</i>≠<i>j</i>
                                    </p>
                      </li>
                      <li>
                        <p>For each state <i>i</i>, <b>A</b>
                                       <sub>
                            <i>σ</i>
                          </sub>(<i>i</i>,<i>σ</i>) for all <i>σ</i> and <b><i>α</i></b>
                                       <sub>∞</sub>(<i>i</i>) is a distribution estimated from training counts, without smoothing.</p>
                      </li>
                    </ul>
                           
                
                  <h3 class="c-article__sub-heading u-h3">EM model</h3>
                  <p>A non-deterministic WFA with <i>n</i> states trained with Expectation Maximization (EM), where <i>n</i> is a parameter of the method. The learning algorithm initializes the WFA randomly, and then it proceeds iteratively by computing expected counts of state transitions on training sequences, and re-setting the parameters of the WFA by maximum likelihood given the expected counts. On validation data, we use a special operator <span class="mathjax-tex">\(\tilde{\boldsymbol{\alpha}}_{\infty}= \mathbf {1}\)</span> to compute prefix probabilities, and we use the <b><i>α</i></b>
                              <sub>∞</sub> resulting from EM to compute probabilities of complete sequences.</p>
                
                  <h3 class="c-article__sub-heading u-h3">Spectral model</h3>
                  <p>A non-deterministic WFA with <i>n</i> states trained with the spectral method, where the parameters of the method are a basis <span class="mathjax-tex">\((\mathcal {P},\mathcal{S})\)</span> of prefixes and suffixes, and the number of states <i>n</i>. We experiment with two ways of setting the basis: <dl class="c-abbreviation_list"><dt class="c-abbreviation_list__term"><dfn>
                                       <i>Σ</i> Basis::</dfn></dt><dd class="c-abbreviation_list__description">
                          <p>We consider one prefix/suffix for each symbol in the alphabet, that is <span class="mathjax-tex">\(\mathcal{P}= \mathcal{S}= \varSigma\)</span>. This is the setting analyzed by Hsu et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="&#xA;Hsu, D., Kakade, S. M., &amp; Zhang, T. (2009). A spectral algorithm for learning hidden Markov models. In Proceedings of the annual conference on computational learning theory (COLT).&#xA;" href="/article/10.1007/s10994-013-5416-x#ref-CR30" id="ref-link-section-d84034e5393">2009</a>) in their theoretical work. In this case, the statistics gathered at training to estimate the automaton will correspond to unigram, bigram and trigram statistics.</p>
                        </dd><dt class="c-abbreviation_list__term"><dfn>Top-<i>k</i> Basis::</dfn></dt><dd class="c-abbreviation_list__description">
                          <p>In this setting we set the prefixes and suffixes to be frequent subsequences of the training set. In particular, we consider all subsequences of symbols up to length 4, and sort them by frequency in the training set. We then set <span class="mathjax-tex">\(\mathcal{P}\)</span> and <span class="mathjax-tex">\(\mathcal{S}\)</span> to be the most frequent <i>k</i> subsequences, where <i>k</i> is a parameter of the model.</p>
                        </dd></dl> Since the training sequences are quite long, relative to the size of the sequences in the basis, we choose to estimate from the training sample a Hankel sub-block for the function <span class="mathjax-tex">\(f_{\mathrm {s}}(x) = \mathbb{E}[|w|_{x}]\)</span>. Hence, the spectral method will return <span class="mathjax-tex">\(A_{\mathrm {s}} = \langle \tilde{\boldsymbol{\alpha}}_{1},\tilde{\boldsymbol{\alpha }}_{\infty}, \{ \mathbf {A}_{\sigma}\}\rangle \)</span> as defined in Lemma 1. We use Lemma 1 to transform <i>A</i>
                              <sub>s</sub> into <i>A</i> and then into <i>A</i>
                              <sub>s</sub>. To calculate WER on validation data, we use <i>A</i>
                              <sub>s</sub> to compute scores of prefix sequences, and <i>A</i> to compute scores of complete sequences.</p>
                  <p>As a final detail, when computing next-symbol predictions with WFA we kept normalizing the state vector. That is, if we are given a prefix sequence <i>w</i>
                              <sub>1,<i>i</i>
                              </sub> we compute <span class="mathjax-tex">\(\boldsymbol{\alpha}^{i\ \top} A_{\sigma}\tilde{\boldsymbol{\alpha}}_{\infty}\)</span> as the score for symbol <i>σ</i> and <b><i>α</i></b>
                              <sup><i>i</i> ⊤</sup>
                              <b><i>α</i></b>
                              <sub>∞</sub> as the score for stopping, where <b><i>α</i></b>
                              <sup><i>i</i></sup> is a normalized state vector at position <i>i</i>. It is recursively computed as <b><i>α</i></b>
                              <sup>1</sup>=<b><i>α</i></b>
                              <sub>1</sub> and <span class="mathjax-tex">\(\boldsymbol{\alpha}^{i+1} = \frac{\boldsymbol{\alpha}^{i\ \top} A_{w_{i}}}{ \boldsymbol{\alpha}^{i\ \top} A_{w_{i}} \tilde{\boldsymbol{\alpha }}_{\infty}}\)</span>. This normalization should not change the predictions, but it helps avoiding numerical precision problems when validation sequences are relatively long.</p>
                <h3 class="c-article__sub-heading u-h3" id="Sec16">Results</h3><p>We trained all types of models for the two sets of tags, namely the simplified set of 12 tags and the original tagset of 45 tags. For the simplified set, the unigram model obtained a WER of 69.4 % on validation data and the bigram improved to 66.6 %. For the original tagset, the unigram and bigram WER were of 87.2 % and 69.4 %.</p><p>We then evaluated spectral models trained with the <i>Σ</i> basis. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10994-013-5416-x#Fig3">3</a> plots the WER of this method as a function of the number of states, for the simplified tagset (left) and the original one (right). We can see that the spectral method improves the bigram baseline when the number of states is 6–8 for the simplified tagset and 8–11 for the original tagset. While the improvements are not huge, one interpretation of this result is that the spectral method is able compress a bigram-based deterministic WFA with |<i>Σ</i>|+1 states into a non-deterministic WFA with less states. The same plot also shows curves of performance for the spectral method, where the basis corresponds to the most frequent <i>k</i> subsequences in training, for several <i>k</i>. We clearly can see that as <i>k</i> grows the performance improves significantly. We also can see that the choice of the number of states is less critical than with the <i>Σ</i> basis. </p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-3"><figure><figcaption><b id="Fig3" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 3</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-category="article body" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10994-013-5416-x/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10994-013-5416-x/MediaObjects/10994_2013_5416_Fig3_HTML.gif?as=webp"></source><img aria-describedby="figure-3-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10994-013-5416-x/MediaObjects/10994_2013_5416_Fig3_HTML.gif" alt="figure3" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc"><p>Performance of the spectral method in terms of WER relative to the number of states, compared to the baseline performance of an unigram and a bigram model. The <i>left plot</i> corresponds to the simplified tagset of 12 symbols, while the <i>right plot</i> corresponds to the tagset of 45 symbols. For the spectral method, we show a curve corresponding to the <i>Σ</i> basis, and curves for the extended that use the <i>k</i> most frequent training subsequences</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-category="article body" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10994-013-5416-x/figures/3" data-track-dest="link:Figure3 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>We now comment on the performance of EM, which is presented in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10994-013-5416-x#Fig4">4</a>. The top plots present the WER as a function of the number of states, for both tagsets. Clearly, the performance significantly improves with the number of states, even for large number of states up to 150. The bottom plots show convergence curves of WER in terms of the number of EM iterations, for some selected number of states. The performance of EM improves the bigram baseline after 20 iterations, and gets somewhat stable (in terms of WER) at about 60 iterations. Note that the cost of one EM iteration requires to compute expectations on all training sequences, a computation that takes quadratic time with the number of states. </p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-4"><figure><figcaption><b id="Fig4" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 4</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-category="article body" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10994-013-5416-x/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10994-013-5416-x/MediaObjects/10994_2013_5416_Fig4_HTML.gif?as=webp"></source><img aria-describedby="figure-4-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10994-013-5416-x/MediaObjects/10994_2013_5416_Fig4_HTML.gif" alt="figure4" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc"><p>
                                       <i>Top plots</i>: performance of EM with respect to the number of states, where each model was run for 100 iterations. <i>Bottom</i>: convergence of EM in terms of WER at validation. <i>Left plots</i> correspond to the simplified tagset of 12 tags, while <i>right plots</i> correspond to the original tagset of 45 symbols</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-category="article body" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10994-013-5416-x/figures/4" data-track-dest="link:Figure4 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10994-013-5416-x#Fig5">5</a> summarizes the best curves of all methods, for the two tagsets. For machines up to 20 states in the simplified tagset, and 10 states in the original tagset, the performance of the spectral method with extended basis is comparable of that of EM. Yet, the EM algorithm is able to improve the results when increasing the number of states. We should note that in our implementation, the runtime of a single EM iteration is at least twice of the total runtime of learning a model with spectral method. </p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-5"><figure><figcaption><b id="Fig5" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 5</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-category="article body" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10994-013-5416-x/figures/5" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10994-013-5416-x/MediaObjects/10994_2013_5416_Fig5_HTML.gif?as=webp"></source><img aria-describedby="figure-5-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10994-013-5416-x/MediaObjects/10994_2013_5416_Fig5_HTML.gif" alt="figure5" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-5-desc"><p>Comparison of different methods in terms of WER on validation data with respect to number of states. The <i>left plot</i> corresponds to the simplified tagset of 12 symbols, while the <i>right plot</i> corresponds to the tagset of 45 symbols</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-category="article body" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10994-013-5416-x/figures/5" data-track-dest="link:Figure5 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        </div></div></section><section aria-labelledby="Sec17"><div class="c-article-section" id="Sec17-section"><h2 class="c-article-section__title u-h2 js-section-title js-c-reading-companion-sections-item" id="Sec17">Non-deterministic split head-automata grammars</h2><div class="c-article-section__content" id="Sec17-content"><p>In this section we develop an application of the spectral method for WFA to the problem of learning split head-automata grammars (SHAG) (Eisner and Satta <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="&#xA;Eisner, J., &amp; Satta, G. (1999). Efficient parsing for bilexical context-free grammars and head-automaton grammars. In Proceedings of the 37th annual meeting of the Association for Computational Linguistics (ACL), University of Maryland (pp. 457–464).&#xA;" href="/article/10.1007/s10994-013-5416-x#ref-CR26" id="ref-link-section-d84034e5702">1999</a>; Eisner <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="&#xA;Eisner, J. (2000). Bilexical grammars and their cubic-time parsing algorithms. In H. Bunt &amp; A. Nijholt (Eds.), Advances in probabilistic and other parsing technologies (pp. 29–62). Norwell: Kluwer Academic.&#xA;" href="/article/10.1007/s10994-013-5416-x#ref-CR25" id="ref-link-section-d84034e5705">2000</a>), a context-free grammatical formalism whose derivations are dependency trees. A dependency tree is a type of syntactic structure where the basic element is a dependency, a syntactic relation between two words of a sentence represented as a directed arc in the tree. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10994-013-5416-x#Fig6">6</a> shows a dependency tree for an English sentence. In our application, we will assume that the training set will be in the form of sentences (i.e. input sequences) paired with their dependency tree. From this type of data, we will learn probabilistic SHAG models using the spectral method that will be then used to predict the most likely dependency tree for test sentences. In the rest of this section we first define SHAG formally. We then describe how the spectral method can be used to learn a SHAG, and finally we describe how we parse sentences with our SHAG models. Then, in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10994-013-5416-x#Sec23">7</a> of this article we present experiments. </p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-6"><figure><figcaption><b id="Fig6" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 6</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-category="article body" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10994-013-5416-x/figures/6" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10994-013-5416-x/MediaObjects/10994_2013_5416_Fig6_HTML.gif?as=webp"></source><img aria-describedby="figure-6-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10994-013-5416-x/MediaObjects/10994_2013_5416_Fig6_HTML.gif" alt="figure6" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-6-desc"><p>An example of a dependency tree. Each arc in the dependency tree represents a syntactic relation between a head token (the origin of each arc) and one of its modifier tokens (the arc destination). The special root token is represented by ⋆. For each token, we print the part-of-speech, the word itself and its position, though our head-automata grammars only model sequences of part of speech tags. The <i>table below the tree</i> prints all head-modifier sequences of the tree. The subscript number next to each tag is the position of the corresponding token. Note that for a sentence of <i>n</i> tokens there are always 2(<i>n</i>+1) sequences, even though most of them are empty</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-category="article body" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10994-013-5416-x/figures/6" data-track-dest="link:Figure6 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                     <h3 class="c-article__sub-heading u-h3" id="Sec18">SHAG</h3><p>We will use <i>x</i>
                           <sub>
                              <i>i</i>:<i>j</i>
                           </sub>=<i>x</i>
                           <sub>
                    <i>i</i>
                  </sub>
                           <i>x</i>
                           <sub>
                              <i>i</i>+1</sub>⋯<i>x</i>
                           <sub>
                    <i>j</i>
                  </sub> to denote a sequence of symbols <i>x</i>
                           <sub>
                    <i>t</i>
                  </sub> with <i>i</i>≤<i>t</i>≤<i>j</i>. A SHAG generates sentences <i>s</i>
                           <sub>0:<i>N</i>
                           </sub>, where symbols <i>s</i>
                           <sub>
                    <i>t</i>
                  </sub>∈<i>Σ</i> with 1≤<i>t</i>≤<i>N</i> are regular words and <i>s</i>
                           <sub>0</sub>=⋆∉<i>Σ</i> is a special root symbol. Let <span class="mathjax-tex">\(\bar{\varSigma} = \varSigma \cup \{\star\}\)</span>. A derivation <i>y</i>, i.e. a dependency tree, is a collection of <i>head-modifier</i> sequences 〈<i>h</i>,<i>d</i>,<i>x</i>
                           <sub>1:<i>T</i>
                           </sub>〉, where <span class="mathjax-tex">\(h \in\bar{\varSigma}\)</span> is a word, <i>d</i>∈{<span class="u-small-caps">left</span>,<span class="u-small-caps">right</span>} is a direction, and <i>x</i>
                           <sub>1:<i>T</i>
                           </sub> is a sequence of <i>T</i> words, where each <i>x</i>
                           <sub>
                    <i>t</i>
                  </sub>∈<i>Σ</i> is a <i>modifier</i> of <i>h</i> in direction <i>d</i>. We say that <i>h</i> is the <i>head</i> of each <i>x</i>
                           <sub>
                    <i>t</i>
                  </sub>. Modifier sequences <i>x</i>
                           <sub>1:<i>T</i>
                           </sub> are ordered head-outwards, i.e. among <i>x</i>
                           <sub>1:<i>T</i>
                           </sub>, <i>x</i>
                           <sub>1</sub> is the word closest to <i>h</i> in the derived sentence, and <i>x</i>
                           <sub>
                    <i>T</i>
                  </sub> is the furthest. A derivation <i>y</i> of a sentence <i>s</i>
                           <sub>0:<i>N</i>
                           </sub> consists of a <span class="u-small-caps">left</span> and a <span class="u-small-caps">right</span> head-modifier sequence for each <i>s</i>
                           <sub>
                    <i>t</i>
                  </sub>, i.e. there are always two sequences per symbol in the sentence. As special cases, the <span class="u-small-caps">left</span> sequence of the root symbol is always empty, while the <span class="u-small-caps">right</span> one consists of a single word corresponding to the head of the sentence. We denote by <span class="mathjax-tex">\(\mathcal{Y}\)</span> the set of all valid derivations. See Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10994-013-5416-x#Fig6">6</a> to see the head-modifier sequences associated with an example dependency tree.</p><p>Assume a derivation <i>y</i> contains 〈<i>h</i>,<span class="u-small-caps">left</span>,<i>x</i>
                           <sub>1:<i>T</i>
                           </sub>〉 and <img src="//media.springernature.com/full/springer-static/image/art%3A10.1007%2Fs10994-013-5416-x/MediaObjects/10994_2013_5416_IEq167_HTML.gif" />. Let <span class="mathjax-tex">\(\mathcal{L}(y,h)\)</span> be the derived sentence <i>headed</i> by <i>h</i>, which can be expressed as </p><div id="Equm" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex"> $$\begin{aligned} \mathcal{L}(y,x_T) \cdots\mathcal{L}(y,x_1) \ h \ \mathcal{L}\bigl(y,x'_1\bigr) \cdots\mathcal{L} \bigl(y,x'_{T'}\bigr). \end{aligned}$$ </span></div></div><p> The language generated by a SHAG are the strings <span class="mathjax-tex">\(\mathcal{L}(y, \star)\)</span> for any <span class="mathjax-tex">\(y \in\mathcal{Y}\)</span>.<sup><a href="#Fn3"><span class="u-visually-hidden">Footnote </span>3</a></sup>
                        </p><h4 class="c-article__sub-heading u-h3 c-article__sub-heading--light" id="Sec19">Probabilistic SHAG</h4><p>In this article we use probabilistic versions of SHAG where probabilities of head-modifier sequences in a derivation are independent of each other: </p><div id="Equ1" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex"> $$\begin{aligned} \mathbb{P}(y) = \prod_{\langle h, d, x_{1:T}\rangle\in y} \mathbb {P}(x_{1:T}| h, d). \end{aligned}$$ </span></div><div class="c-article-equation__number">
                    (1)
                </div></div><p> In the literature, standard <i>arc-factored</i> models further assume that </p><div id="Equn" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex"> $$\begin{aligned} \mathbb{P}(x_{1:T}| h, d) = \prod_{t=1}^{T+1} \mathbb{P}(x_t | h, d, \sigma_t) , \end{aligned}$$ </span></div></div><p> where <i>x</i>
                              <sub>
                                 <i>T</i>+1</sub> is always a special <span class="u-small-caps">stop</span> word, and <i>σ</i>
                              <sub>
                      <i>t</i>
                    </sub> is the state of a deterministic automaton generating <i>x</i>
                              <sub>1:<i>T</i>+1</sub>. For example, setting <i>σ</i>
                              <sub>1</sub>=<span class="u-small-caps">first</span> and <i>σ</i>
                              <sub>
                                 <i>t</i>&gt;1</sub>=<span class="u-small-caps">rest</span> corresponds to first-order models, while setting <i>σ</i>
                              <sub>1</sub>=<span class="u-small-caps">null</span> and <i>σ</i>
                              <sub>
                                 <i>t</i>&gt;1</sub>=<i>x</i>
                              <sub>
                                 <i>t</i>−1</sub> corresponds to sibling models (Eisner <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="&#xA;Eisner, J. (2000). Bilexical grammars and their cubic-time parsing algorithms. In H. Bunt &amp; A. Nijholt (Eds.), Advances in probabilistic and other parsing technologies (pp. 29–62). Norwell: Kluwer Academic.&#xA;" href="/article/10.1007/s10994-013-5416-x#ref-CR25" id="ref-link-section-d84034e6234">2000</a>; McDonald et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="&#xA;McDonald, R., Pereira, F., Ribarov, K., &amp; Hajic, J. (2005). Non-projective dependency parsing using spanning tree algorithms. In Proceedings of human language technology conference and conference on empirical methods in natural language processing (pp. 523–530). Vancouver: Association for Computational Linguistics.&#xA;" href="/article/10.1007/s10994-013-5416-x#ref-CR37" id="ref-link-section-d84034e6237">2005</a>; McDonald and Pereira <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="&#xA;McDonald, R., &amp; Pereira, F. (2006). Online learning of approximate dependency parsing algorithms. In Proceedings of the 11th conference of the European chapter of the Association for Computational Linguistics (pp. 81–88).&#xA;" href="/article/10.1007/s10994-013-5416-x#ref-CR36" id="ref-link-section-d84034e6240">2006</a>).</p><p>We will define a SHAG using a collection of weighted automata to compute probabilities. Assume that for each possible head <i>h</i> in the vocabulary <span class="mathjax-tex">\(\bar{\varSigma}\)</span> and each direction <i>d</i>∈{<span class="u-small-caps">left</span>,<span class="u-small-caps">right</span>} we have a weighted automaton that computes probabilities of modifier sequences as follows: </p><div id="Equo" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex"> $$\begin{aligned} \mathbb{P}(x_{1:T}| h, d) = \bigl(\boldsymbol{\alpha}_1^{h,d} \bigr)^\top \mathbf {A}_{x_1}^{h,d} \cdots \mathbf {A}_{x_t}^{h,d} \boldsymbol {\alpha }_{\infty }^{h,d} . \end{aligned}$$ </span></div></div><p> Then, this collection of weighted automata defines an <i>non-deterministic SHAG</i> that assigns a probability to each <span class="mathjax-tex">\(y \in\mathcal{Y}\)</span> according to (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10994-013-5416-x#Equ1">1</a>).</p><h3 class="c-article__sub-heading u-h3" id="Sec20">Learning SHAG</h3><p>A property of our non-deterministic SHAG models is that the probability of a derivation factors into the probability of each head-modifier sequence. In other words, the state processes only model horizontal structure of the tree, and different WFA do not interact in a derivation. In addition, in this article we make the assumption that training sequences come paired with dependency trees, i.e. we assume a supervised setting. Hence, we do not deal with the hard problem of inducing grammars from sequences.</p><p>These two facts make the application of the spectral method for WFA almost trivial. From the training set, we can decompose each dependency tree into sequences of modifiers, and create a training set for each head of direction containing the corresponding sequences of modifiers. Then, for each head and direction, we can learn WFA by direct application of the spectral method.</p><h3 class="c-article__sub-heading u-h3" id="Sec21">Parsing with non-deterministic SHAG</h3><p>Given a sentence <i>s</i>
                           <sub>0:<i>N</i>
                           </sub> we would like to find its most likely derivation, </p><div id="Equp" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex"> $$\begin{aligned} \hat{y} = \mathop {\mathrm {argmax}}_{y \in\mathcal{Y}(s_{0:N}) } \mathbb{P}(y). \end{aligned}$$ </span></div></div><p> This problem, known as MAP inference, is known to be intractable for hidden-state structure prediction models, as it involves finding the most likely tree structure while summing out over hidden states. We use a common approximation to MAP based on first computing posterior marginals of tree edges (i.e. dependencies) and then maximizing over the tree structure (see Park and Darwiche (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="&#xA;Park, J. D., &amp; Darwiche, A. (2004). Complexity results and approximation strategies for map explanations. Journal of Artificial Intelligence Research, 21, 101–133.&#xA;" href="/article/10.1007/s10994-013-5416-x#ref-CR42" id="ref-link-section-d84034e6336">2004</a>) for complexity of general MAP inference and approximations). For parsing, this strategy is sometimes known as MBR decoding; previous work has shown that empirically it gives good performance (Goodman <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1996" title="&#xA;Goodman, J. (1996). Parsing algorithms and metrics. In Proceedings of the 34th annual meeting of the Association for Computational Linguistics (pp. 177–183). Santa Cruz: Association for Computational Linguistics.&#xA;" href="/article/10.1007/s10994-013-5416-x#ref-CR29" id="ref-link-section-d84034e6339">1996</a>; Clark and Curran <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="&#xA;Clark, S., &amp; Curran, J. R. (2004). Parsing the WSJ using CCG and log-linear models. In Proceedings of the 42nd meeting of the association for computational linguistics (ACL’04), main volume, Barcelona, Spain (pp. 103–110).&#xA;" href="/article/10.1007/s10994-013-5416-x#ref-CR19" id="ref-link-section-d84034e6343">2004</a>; Titov and Henderson <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="&#xA;Titov, I., &amp; Henderson, J. (2006). Loss minimization in parse reranking. In Proceedings of the 2006 conference on empirical methods in natural language processing (pp. 560–567). Sydney: Association for Computational Linguistics.&#xA;" href="/article/10.1007/s10994-013-5416-x#ref-CR50" id="ref-link-section-d84034e6346">2006</a>; Petrov and Klein <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="&#xA;Petrov, S., &amp; Klein, D. (2007). Improved inference for unlexicalized parsing. Proceedings of the main conference, Association for Computational Linguistics. In Human language technologies 2007: the conference of the North American chapter of the Association for Computational Linguistics (pp. 404–411). Rochester: Association for Computational Linguistics.&#xA;" href="/article/10.1007/s10994-013-5416-x#ref-CR43" id="ref-link-section-d84034e6349">2007</a>). In our case, we use the non-deterministic SHAG to compute posterior marginals of dependencies. We first explain the general strategy of MBR decoding, and then present an algorithm to compute marginals.</p><p>Let (<i>s</i>
                           <sub>
                    <i>i</i>
                  </sub>,<i>s</i>
                           <sub>
                    <i>j</i>
                  </sub>) denote a dependency between head word <i>i</i> and modifier word <i>j</i>. The posterior or <i>marginal probability</i> of a dependency (<i>s</i>
                           <sub>
                    <i>i</i>
                  </sub>,<i>s</i>
                           <sub>
                    <i>j</i>
                  </sub>) given a sentence <i>s</i>
                           <sub>0:<i>N</i>
                           </sub> is defined as </p><div id="Equq" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex"> $$\begin{aligned} \mu_{i,j} = \mathbb{P}\bigl((s_i, s_j) \bigm| s_{0:N}\bigr) = \sum_{\substack{y \in\mathcal{Y}(s_{0:N}) \ :\ (s_i,s_j) \in y}} \mathbb{P}(y) . \end{aligned}$$ </span></div></div><p> To compute marginals, the sum over derivations can be decomposed into a product of inside and outside quantities (Baker <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1979" title="&#xA;Baker, J. K. (1979). Trainable grammars for speech recognition. In D. H. Klatt &amp; J. J. Wolf (Eds.), Speech communication papers for the 97th meeting of the Acoustical Society of America (pp. 547–550).&#xA;" href="/article/10.1007/s10994-013-5416-x#ref-CR7" id="ref-link-section-d84034e6422">1979</a>). In Appendix <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10994-013-5416-x#Sec30">A</a> we describe an inside-outside algorithm for non-deterministic SHAG. Given a sentence <i>s</i>
                           <sub>0:<i>N</i>
                           </sub> and marginal scores <i>μ</i>
                           <sub>
                              <i>i</i>,<i>j</i>
                           </sub>, we compute the parse tree for <i>s</i>
                           <sub>0:<i>N</i>
                           </sub> as </p><div id="Equ2" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex"> $$\begin{aligned} \hat{y} = \mathop {\mathrm {argmax}}_{y \in\mathcal{Y}(s_{0:N}) } \sum _{(s_i, s_j) \in y} \log\mu_{i,j} \end{aligned}$$ </span></div><div class="c-article-equation__number">
                    (2)
                </div></div><p> using the standard projective parsing algorithm for arc-factored models (Eisner <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="&#xA;Eisner, J. (2000). Bilexical grammars and their cubic-time parsing algorithms. In H. Bunt &amp; A. Nijholt (Eds.), Advances in probabilistic and other parsing technologies (pp. 29–62). Norwell: Kluwer Academic.&#xA;" href="/article/10.1007/s10994-013-5416-x#ref-CR25" id="ref-link-section-d84034e6473">2000</a>). Overall we use a two-pass parsing process, first to compute marginals and then to compute the best tree.</p><h3 class="c-article__sub-heading u-h3" id="Sec22">Related work</h3><p>There have been a number of works that apply spectral learning methods to tree structures. Dhillon et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="&#xA;Dhillon, P., Rodu, J., Collins, M., Foster, D., &amp; Ungar, L. (2012). Spectral dependency parsing with latent variables. In Proceedings of the 2012 joint conference on empirical methods in natural language processing and computational natural language learning (pp. 205–213). Jeju Island: Association for Computational Linguistics.&#xA;" href="/article/10.1007/s10994-013-5416-x#ref-CR24" id="ref-link-section-d84034e6485">2012</a>) present a latent-variable model for dependency parsing, where the state process models vertical interactions between heads and modifiers, such that hidden states pass information from the root of the tree to each leaf. In their model, given the state of a head, the modifiers are independent of each other. In contrast, in our case the hidden states model interactions between the children of a head, but hidden states do not pass information vertically. In our case the application of the spectral method is straightforward, while the vertical case requires taking into account that at each node the sequence from the root to the node branches out into multiple children.</p><p>There have been extensions by Bailly et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="&#xA;Bailly, R., Habrard, A., &amp; Denis, F. (2010). A spectral approach for probabilistic grammatical inference on trees. In M. Hutter, F. Stephan, V. Vovk, &amp; T. Zeugmann (Eds.), Lecture notes in computer science (Vol. 6331, pp. 74–88). Berlin: Springer.&#xA;" href="/article/10.1007/s10994-013-5416-x#ref-CR6" id="ref-link-section-d84034e6491">2010</a>) and Cohen et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="&#xA;Cohen, S. B., Stratos, K., Collins, M., Foster, D. P., &amp; Ungar, L. (2012). Spectral learning of latent-variable PCFGS. In Proceedings of the 50th annual meeting of the association for computational linguistics (Volume 1: Long papers) (pp. 223–231). Jeju Island: Association for Computational Linguistics.&#xA;" href="/article/10.1007/s10994-013-5416-x#ref-CR20" id="ref-link-section-d84034e6494">2012</a>) of the spectral method for probabilistic context-free grammars (PCFG), a formalism that includes SHAG. In this case the state process can model horizontal and vertical interactions simultaneously, by making use of tensor operators associated to the rules of the grammar. Recently, Cohen et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="&#xA;Cohen, S. B., Stratos, K., Collins, M., Foster, D. P., &amp; Ungar, L. (2013). Experiments with spectral learning of latent-variable pcfgs. In Proceedings of the 2013 conference of the North American chapter of the Association for Computational Linguistics: human language technologies (pp. 148–157). Atlanta: Association for Computational Linguistics.&#xA;" href="/article/10.1007/s10994-013-5416-x#ref-CR21" id="ref-link-section-d84034e6497">2013</a>) have presented experiments to learn phrase-structure models using the a spectral method.</p><p>The works mentioned so far model a joint distribution over trees of different sizes, which is the suitable setting for models like natural language parsing. In contrast, Parikh et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="&#xA;Parikh, A., Song, L., &amp; Xing, E. (2011). A spectral algorithm for latent tree graphical models. In Proceedings of the 28th international conference on machine learning, ICML 2011 (ICML) (pp. 1065–1072).&#xA;" href="/article/10.1007/s10994-013-5416-x#ref-CR41" id="ref-link-section-d84034e6503">2011</a>) presented a spectral method to learn distributions over labelings of a fixed (though arbitrary) tree topology.</p><p>In all these cases, the learning setting is supervised, in the sense that training sequences are paired with their tree structure, and the spectral algorithm is used to induce the hidden state process. A more ambitious problem is that of grammatical inference, where the goal is to induce the model only from sequences. Regarding spectral methods, Mossel and Roch (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="&#xA;Mossel, E., &amp; Roch, S. (2005). Learning nonsingular phylogenies and hidden Markov models. In Proceedings of the 37th annual ACM symposium on theory of computing (STOC) (pp. 366–375).&#xA;" href="/article/10.1007/s10994-013-5416-x#ref-CR39" id="ref-link-section-d84034e6509">2005</a>) study the induction of the topology of a phylogenetic tree-shaped model, and Hsu et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="&#xA;Hsu, D., Kakade, S. M., &amp; Liang, P. (2012). Identifiability and unmixing of latent parse trees. Advances in neural information processing systems (NIPS).&#xA;" href="/article/10.1007/s10994-013-5416-x#ref-CR31" id="ref-link-section-d84034e6512">2012</a>) discuss spectral techniques to induce PCFG, with dependency grammars as a special case.</p></div></div></section><section aria-labelledby="Sec23"><div class="c-article-section" id="Sec23-section"><h2 class="c-article-section__title u-h2 js-section-title js-c-reading-companion-sections-item" id="Sec23">Experiments on learning SHAG</h2><div class="c-article-section__content" id="Sec23-content"><p>In this section we present experiments with SHAG. We learn non-deterministic SHAG using different versions of the spectral algorithm, and compare them to non-deterministic SHAG learned with EM and to some baseline deterministic SHAG.</p><p>Our experiments involve fully unlexicalized models, i.e. parsing part-of-speech tag sequences. While this setting falls behind the state-of-the-art, it is nonetheless valid to analyze empirically the effect of incorporating hidden states via weighted automata, which results in large improvements. At the end, we present some analysis of the automaton learned by the spectral algorithm to see the information that is captured in the hidden state space.</p><p>All the experiments were done with the dependency version of the English WSJ Penn Treebank, using the standard partitions for training and validation (see Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10994-013-5416-x#Sec14">5</a>). The models were trained using the modifier sequences extracted from the training dependency trees, and they were evaluated parsing the validation set and computing the Unlabeled Attachment Score (UAS). UAS is an accuracy measure that accounts for the percentage of tokens that were assigned the correct head word (note that in a dependency tree, each word modifies exactly one head).</p><h3 class="c-article__sub-heading u-h3" id="Sec24">Methods compared</h3><p>As a SHAG is a collection of automata, each one has its own alphabet <i>Σ</i>
                           <sup><i>h</i>,<i>d</i></sup>, defined as the set of symbols occurring in the training modifier sequences for that head <i>h</i> and direction <i>d</i>. We compare the following models:</p>
                  <h3 class="c-article__sub-heading u-h3">Baseline models</h3>
                  <p>Deterministic SHAG with a fixed global DFA structure. The PDFA transition probabilities for each head and direction are estimated using the training modifier sequences. We define two concrete baselines depending on the DFA structure: <dl class="c-abbreviation_list"><dt class="c-abbreviation_list__term"><dfn>
                                       <span class="u-small-caps">Det</span>::</dfn></dt><dd class="c-abbreviation_list__description">
                          <p>A single state DFA as in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10994-013-5416-x#Fig7">7</a>(a). </p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-7"><figure><figcaption><b id="Fig7" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 7</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-category="article body" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10994-013-5416-x/figures/7" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10994-013-5416-x/MediaObjects/10994_2013_5416_Fig7_HTML.gif?as=webp"></source><img aria-describedby="figure-7-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10994-013-5416-x/MediaObjects/10994_2013_5416_Fig7_HTML.gif" alt="figure7" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-7-desc"><p>Unlexicalized DFAs illustrating the features encoded in the deterministic baselines. For clarity, on each automata we added a separate final state, and a special ending symbol STOP. (<b>a</b>) <span class="u-small-caps">Det</span>. (<b>b</b>) <span class="u-small-caps">Det+F</span>
                                                    </p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-category="article body" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10994-013-5416-x/figures/7" data-track-dest="link:Figure7 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                                       
                        </dd><dt class="c-abbreviation_list__term"><dfn>
                                       <span class="u-small-caps">Det+F</span>::</dfn></dt><dd class="c-abbreviation_list__description">
                          <p>Two states, one emitting the first modifier of a sequence, and another emitting the rest, as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10994-013-5416-x#Fig7">7</a>(b) (see Eisner and Smith (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="&#xA;Eisner, J., &amp; Smith, N. A. (2010). Favor short dependencies: parsing with soft and hard constraints on dependency length. In H. Bunt, P. Merlo, &amp; J. Nivre (Eds.), Trends in parsing technology: dependency parsing, domain adaptation, and deep parsing (Vol. 8, pp. 121–150). Berlin: Springer.&#xA;" href="/article/10.1007/s10994-013-5416-x#ref-CR27" id="ref-link-section-d84034e6627">2010</a>) for a similar deterministic baseline).</p>
                        </dd></dl>
                           </p>
                
                  <h3 class="c-article__sub-heading u-h3">EM model</h3>
                  <p>A non-deterministic SHAG with <i>n</i> states trained with Expectation Maximization (EM) as in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10994-013-5416-x#Sec14">5</a>.</p>
                
                  <h3 class="c-article__sub-heading u-h3">Spectral models</h3>
                  <p>Non-deterministic SHAG where the WFA are trained with the spectral algorithm. As in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10994-013-5416-x#Sec14">5</a>, we use substring expectation estimations and then we use Lemma 1 to obtain WFA that approximate full sequence distributions. The number of states for each WFA is min(|<i>Σ</i>
                              <sup><i>h</i>,<i>d</i></sup>|,<i>n</i>), where <i>n</i> is a parameter of the model. We do experiments with two variants of the spectral method: <dl class="c-abbreviation_list"><dt class="c-abbreviation_list__term"><dfn>
                                       <i>Σ</i>′ basis::</dfn></dt><dd class="c-abbreviation_list__description">
                          <p>The basis for each WFA is <span class="mathjax-tex">\(\mathcal{P}^{h,d} = \mathcal{S}^{h,d} = (\varSigma^{h,d})^{\prime}= \varSigma^{h,d} \cup \{ \lambda\}\)</span>. For this model, we use an additional parameter <i>m</i>, a minimal mass used to discard states. In each WFA, we discard the states with proportional singular value &lt;<i>m</i>.</p>
                        </dd><dt class="c-abbreviation_list__term"><dfn>Extended basis::</dfn></dt><dd class="c-abbreviation_list__description">
                          <p>
                                          <i>f</i> is a parameter of the model, namely a <i>cut factor</i> that defines the size of the basis as follows. For each WFA, we use as basis <span class="mathjax-tex">\(\mathcal{P}^{h,d}\)</span> and <span class="mathjax-tex">\(\mathcal {S}^{h,d}\)</span> the set of |<i>Σ</i>
                                          <sup><i>h</i>,<i>d</i></sup>|<i>f</i> most frequent training subsequences of symbols (up to length 4). Hence, <i>f</i> is a relative size of the basis for a WFA, proportional to the size of its alphabet. We always include the empty sequence <i>λ</i> in the basis.</p>
                        </dd></dl>
                           </p>
                <h3 class="c-article__sub-heading u-h3" id="Sec25">Results</h3><p>The results for the deterministic baselines were a UAS of 68.52 % for <span class="u-small-caps">Det</span> and a UAS of 74.80 % for <span class="u-small-caps">Det+F</span>.</p><p>In the first set of experiments with the spectral method, we evaluated the models trained with the <i>Σ</i>′ basis. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10994-013-5416-x#Fig8">8</a>(a) shows the resulting UAS scores in terms of the parameter <i>n</i> (the number of states). We plot curves for the basic spectral model with no state discarding (<i>m</i>=0) and with state discarding for different values of minimal mass (<i>m</i>&gt;0). The basic model improves over the baselines, reaching a peak UAS of 79.75 % with 9 states, but then the accuracy starts to drop and with 20 states it performs worse than <span class="u-small-caps">Det+F</span>. The curves for the models with the singular-value based state discarding strategy also have a peak at 9 states, but then they converge to a stable performance, always above the baselines. The best result is a UAS of 79.81 % for <i>m</i>=0.0001, but the best overall curve is for <i>m</i>=0.0005, with a peak of 79.79 % and converging then to 79.64 %. Although very simple, our state discarding strategy seems to be effective to obtain models with stable performance. </p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-8"><figure><figcaption><b id="Fig8" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 8</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-category="article body" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10994-013-5416-x/figures/8" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10994-013-5416-x/MediaObjects/10994_2013_5416_Fig8_HTML.gif?as=webp"></source><img aria-describedby="figure-8-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10994-013-5416-x/MediaObjects/10994_2013_5416_Fig8_HTML.gif" alt="figure8" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-8-desc"><p>Accuracy of the different spectral methods (UAS in function of the number of states). (<b>a</b>) Curves for the <i>Σ</i>′ basis: basic spectral method (<i>m</i>=0) and state discarding with minimum mass <i>m</i>&gt;0. (<b>b</b>) Curves for the extended basis with different cut factors <i>f</i>
                                    </p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-category="article body" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10994-013-5416-x/figures/8" data-track-dest="link:Figure8 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>In the second set of experiments with the spectral method, we evaluated models estimated with extended basis. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10994-013-5416-x#Fig8">8</a>(b) shows curves for different cut factors <i>f</i>, plotting UAS scores in terms of the number of states.<sup><a href="#Fn4"><span class="u-visually-hidden">Footnote </span>4</a></sup> Here, we clearly see that the performance largely improves and is more stable with bigger values for <i>f</i>.<sup><a href="#Fn5"><span class="u-visually-hidden">Footnote </span>5</a></sup> The best results are clearly better than the ones of the basic model (UAS 80.90 % vs. 79.81 %) and, more interestingly, the curves reach stability without the need of a state discarding strategy.</p><p>The results for the experiments with EM are shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10994-013-5416-x#Fig9">9</a>. The left figure plots accuracy with respect to the number of states, where we see that EM obtains improvements as the number of states increases (though for <i>n</i>&gt;100 the improvements are small). The right plot shows the convergence of EM in terms of accuracy relative to the number of iterations. As in the experiments with WA, EM needs about 50 iterations to obtain a stable performance. </p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-9"><figure><figcaption><b id="Fig9" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 9</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-category="article body" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10994-013-5416-x/figures/9" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10994-013-5416-x/MediaObjects/10994_2013_5416_Fig9_HTML.gif?as=webp"></source><img aria-describedby="figure-9-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10994-013-5416-x/MediaObjects/10994_2013_5416_Fig9_HTML.gif" alt="figure9" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-9-desc"><p>Accuracy for EM. (<b>a</b>) UAS with respect to number of states. (<b>b</b>) UAS with respect to number of training iterations. Curves for different numbers of states <i>n</i>
                                    </p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-category="article body" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10994-013-5416-x/figures/9" data-track-dest="link:Figure9 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>To summarize, in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10994-013-5416-x#Fig10">10</a>(a) we compare the best runs of each method. In terms of accuracy, the spectral method with extended basis obtains accuracies comparable to EM. We would like to note that, as in the experiments with WFA, in terms of training time the spectral algorithm is much faster than EM (each EM iteration takes at least twice the time of running the spectral method). </p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-10"><figure><figcaption><b id="Fig10" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 10</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-category="article body" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10994-013-5416-x/figures/10" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10994-013-5416-x/MediaObjects/10994_2013_5416_Fig10_HTML.gif?as=webp"></source><img aria-describedby="figure-10-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10994-013-5416-x/MediaObjects/10994_2013_5416_Fig10_HTML.gif" alt="figure10" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-10-desc"><p>Comparison of different methods in terms of UAS with respect to the number of states</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-category="article body" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10994-013-5416-x/figures/10" data-track-dest="link:Figure10 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <h3 class="c-article__sub-heading u-h3" id="Sec26">Result analysis</h3><p>Our purpose in this section is to see what information is encoded in the models learned by the spectral algorithm. However, hidden state spaces are hard to interpret, and this is even harder if they are projected into a non-probabilistic space through a basis change, as in our case. To do the analysis, we build DFA that approximate the behaviour of the non-deterministic models when they generate highly probable sequences. The DFA approximations allows us to observe in a simple way some linguistically relevant phenomena encoded in the states, and to compare them with manually encoded features of well-known models. In this section we describe the DFA approximation construction method, and then we use it to analyze the most relevant unlexicalized automaton in terms of number of dependencies, namely, the automaton for <i>h</i>=NN and <i>d</i>=<span class="u-small-caps">left</span>.</p><h4 class="c-article__sub-heading u-h3 c-article__sub-heading--light" id="Sec27">DFA approximation for stochastic WFA</h4><p>When generating a sequence, a DFA is in a single state at each step of the generation. However, in a PNFA, what we have at each step is a vector with the probabilities of being at each state. More generally, in a WFA, at each step we have an arbitrary vector in <span class="mathjax-tex">\(\mathbb{R}^{n}\)</span>, called the forward-state vector. For a WFA and a given sequence <i>x</i>=<i>x</i>
                              <sub>1</sub>…<i>x</i>
                              <sub>
                      <i>t</i>
                    </sub>, the forward-state vector after generating <i>x</i> is defined as </p><div id="Equr" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex"> $$\begin{aligned} \boldsymbol{\alpha}(x) = \bigl( \boldsymbol{\alpha}_1^\top \mathbf {A}_{x_1} \cdots \mathbf {A}_{x_t} \bigr)^\top. \end{aligned}$$ </span></div></div><p> While generating a sequence <i>x</i>, a WFA traverses the <span class="mathjax-tex">\(\mathbb{R}^{n} \)</span> space with a path <b><i>α</i></b>(<i>x</i>
                              <sub>1</sub>), <b><i>α</i></b>(<i>x</i>
                              <sub>1</sub>
                              <i>x</i>
                              <sub>2</sub>), …, <b><i>α</i></b>(<i>x</i>
                              <sub>1</sub>…<i>x</i>
                              <sub>
                      <i>t</i>
                    </sub>), resembling a deterministic process in an infinite-state space.</p><p>To build a DFA approximation, we first compute a set of forward vectors corresponding to the most frequent prefixes of training sequences. Then, we cluster these vectors using a Group Average Agglomerative algorithm using the cosine similarity measure (Manning et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="&#xA;Manning, C. D., Raghavan, P., &amp; Schütze, H. (2008). Introduction to information retrieval (1st ed.). Cambridge: Cambridge University Press.&#xA;" href="/article/10.1007/s10994-013-5416-x#ref-CR34" id="ref-link-section-d84034e7074">2008</a>). Each cluster <i>i</i> defines a state in the DFA, and we say that a sequence <i>m</i>
                              <sub>1:<i>t</i>
                              </sub> is in state <i>i</i> if its corresponding forward vector at time <i>t</i> is in cluster <i>i</i>. The transitions in the DFA are defined using a procedure that looks at how sequences traverse the states. If a sequence <i>m</i>
                              <sub>1:<i>t</i>
                              </sub> is at state <i>i</i> at time <i>t</i>−1, and goes to state <i>j</i> at time <i>t</i>, then we define a transition from state <i>i</i> to state <i>j</i> with label <i>m</i>
                              <sub>
                      <i>t</i>
                    </sub>. This procedure may require merging states to give a consistent DFA, because different sequences may define different transitions for the same states and modifiers. After doing a merge, new merges may be required, so the procedure must be repeated until a DFA is obtained.</p><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10994-013-5416-x#Fig11">11</a> illustrates the DFA construction process showing fictitious forward vectors in a 3 dimensional space. The forward vectors correspond to the prefixes of the sequence “JJ JJ DT STOP”, a frequent left-modifier sequence for nouns. In this example, we construct a 3 state automaton by clustering the vectors into three different sets and then defining the transitions as described in the previous paragraphs. </p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-11"><figure><figcaption><b id="Fig11" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 11</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-category="article body" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10994-013-5416-x/figures/11" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10994-013-5416-x/MediaObjects/10994_2013_5416_Fig11_HTML.gif?as=webp"></source><img aria-describedby="figure-11-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10994-013-5416-x/MediaObjects/10994_2013_5416_Fig11_HTML.gif" alt="figure11" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-11-desc"><p>Example of construction of a 3 state DFA approximation. (<b>a</b>) Concrete example for the modifier sequence “JJ JJ DT STOP”. (<b>b</b>) Forward vectors <b><i>α</i></b> for the prefixes of the sequence. (<b>c</b>) Cosine similarity clustering. (<b>d</b>) Resulting DFA after adding the transitions</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-category="article body" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10994-013-5416-x/figures/11" data-track-dest="link:Figure11 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <h4 class="c-article__sub-heading u-h3 c-article__sub-heading--light" id="Sec28">Experiments on unlexicalized WFA</h4><p>A DFA approximation for the automaton (NN,<span class="u-small-caps">left</span>) is shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10994-013-5416-x#Fig12">12</a>. The vectors were originally divided into ten clusters, but the DFA construction required two state mergings, leading to a eight state automaton. The state named I is the initial state. Clearly, we can see that there are special states for punctuation (state 9) and coordination (states 1 and 5). States 0 and 2 are harder to interpret. To understand them better, we computed an estimation of the probabilities of the transitions, by counting the number of times each of them is used. We found that our estimation of generating STOP from state 0 is 0.67, and from state 2 it is 0.15. Interestingly, state 2 can transition to state 0 generating PRP$, POS or DT, that are usual endings of modifier sequences for nouns (recall that modifiers are generated head-outwards, so for a left automaton the final modifier is the left-most modifier in the sentence). </p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-12"><figure><figcaption><b id="Fig12" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 12</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-category="article body" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10994-013-5416-x/figures/12" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10994-013-5416-x/MediaObjects/10994_2013_5416_Fig12_HTML.gif?as=webp"></source><img aria-describedby="figure-12-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10994-013-5416-x/MediaObjects/10994_2013_5416_Fig12_HTML.gif" alt="figure12" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-12-desc"><p>DFA approximation for the generation of NN left modifier sequences</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-category="article body" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10994-013-5416-x/figures/12" data-track-dest="link:Figure12 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           </div></div></section><section aria-labelledby="Sec29"><div class="c-article-section" id="Sec29-section"><h2 class="c-article-section__title u-h2 js-section-title js-c-reading-companion-sections-item" id="Sec29">Conclusion</h2><div class="c-article-section__content" id="Sec29-content"><p>The central objective of this paper was to offer a broad view of the main results in spectral learning in the context of grammatical inference, and more precisely in the context of learning weighted automata. With this goal in mind, we presented the recent advances in the field in a way that makes the main underlying principles of spectral learning accessible to a wide audience.</p><p>We believe this to be useful since spectral methods are becoming an interesting alternative to the classical EM algorithms widely used for grammatical inference. One of the attractiveness of the spectral approach resides in its computational efficiency (at least in the context of automata learning). This efficiency might open the door to large-scale applications of automata learning, where models can be inferred from big data collections.</p><p>Apart from scalability, some important questions about the different properties of EM versus spectral learning remain unanswered. That been said, in broad terms we can make two main distinctions between spectral learning and EM: </p><ul class="u-list-style-bullet">
                  <li>
                    <p>EM attempts to minimize the KL divergence between the model distribution and the observed distribution. In contrast, the spectral method attempts to minimize an <i>ℓ</i>
                                 <sub>
                        <i>p</i>
                      </sub> distance between model and observed distribution.</p>
                  </li>
                  <li>
                    <p>EM searches for stable points of the likelihood function. Instead, the spectral method finds an approximate minimizer of a global loss function.</p>
                  </li>
                </ul>
                     <p>Most empirical studies, including ours, suggest that the statistical performance of spectral methods is similar to that of EM (e.g. see Cohen et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="&#xA;Cohen, S. B., Stratos, K., Collins, M., Foster, D. P., &amp; Ungar, L. (2013). Experiments with spectral learning of latent-variable pcfgs. In Proceedings of the 2013 conference of the North American chapter of the Association for Computational Linguistics: human language technologies (pp. 148–157). Atlanta: Association for Computational Linguistics.&#xA;" href="/article/10.1007/s10994-013-5416-x#ref-CR21" id="ref-link-section-d84034e7248">2013</a> for experiments learning latent-variable probabilistic context free grammars). However, our empirical understanding is still quite limited and more research needs to be done to understand the relative performance of each algorithm with respect to the complexity of the target model (i.e., size of the alphabet and number of states). Nonetheless, spectral methods offer a very competitive computational performance when compared to iterative methods like EM.</p><p>A key difference between the spectral method and other approaches to induce weighted automata is at the conceptual level, particularly in the way in which the learning problem is framed. This conceptual difference is precisely what we tried to emphasize in our presentation of the subject. In a snapshot, the central idea of the spectral approach to learning functions over <i>Σ</i>
                        <sup>⋆</sup> is to directly exploit recurrence relations satisfied by families of functions. This is done by providing algebraic formulations of these recurrence relations.</p><p>Because spectral learning for grammatical inference is still a young field, many problems remain open. At a technical level, we have already mentioned the two most important: how to choose a sample-dependent basis for the Hankel matrices fed to the method, and how to guarantee that the output WFA is stochastic or probabilistic. The former problem has been discussed at large in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10994-013-5416-x#Sec12">4.2.2</a>, where we gave heuristics for choosing the input parameters given to the algorithm. The latter problem has received less attention in the present paper, mainly because our experimental framework is not affected by it. However, ensuring the output of the spectral method is a proper probability distribution is important in many applications. Different solutions have been proposed to address this issue: Bailly (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="&#xA;Bailly, R. (2011). Quadratic weighted automata: spectral algorithm and likelihood maximization. Journal of Machine Learning Research—Proceedings Track, 20, 147–163.&#xA;" href="/article/10.1007/s10994-013-5416-x#ref-CR4" id="ref-link-section-d84034e7266">2011</a>) gave a spectral method for Quadratic WFA which by definition always define a non-negative function; heuristics to modify the output of a spectral algorithm in order to enforce non-negativity were discussed in Cohen et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="&#xA;Cohen, S. B., Stratos, K., Collins, M., Foster, D. P., &amp; Ungar, L. (2013). Experiments with spectral learning of latent-variable pcfgs. In Proceedings of the 2013 conference of the North American chapter of the Association for Computational Linguistics: human language technologies (pp. 148–157). Atlanta: Association for Computational Linguistics.&#xA;" href="/article/10.1007/s10994-013-5416-x#ref-CR21" id="ref-link-section-d84034e7269">2013</a>) in the context of PCFG, though they also apply to WFA; for HMM one can use methods based on spectral decompositions of tensors to overcome this problem (Anandkumar et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012b" title="&#xA;Anandkumar, A., Ge, R., Hsu, D., Kakade, S. M., &amp; Telgarsky, M. (2012b). Tensor decompositions for learning latent variable models.&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;                      &#xA;                    arXiv:1210.7559&#xA;                    &#xA;                  .&#xA;" href="/article/10.1007/s10994-013-5416-x#ref-CR2" id="ref-link-section-d84034e7272">2012b</a>); one can obtain probabilistic WFA by imposing some convex constraints on the search space of the optimization-based spectral method presented in Balle et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="&#xA;Balle, B., Quattoni, A., &amp; Carreras, X. (2012). Local loss optimization in operator models: a new insight into spectral learning. In J. Langford &amp; J. Pineau (Eds.), Proceedings of the 29th international conference on machine learning (ICML-2012), ICML’12 (pp. 1879–1886). New York: Omnipress.&#xA;" href="/article/10.1007/s10994-013-5416-x#ref-CR11" id="ref-link-section-d84034e7275">2012</a>). All these methods rely on variations of the SVD-based method presented in this paper. An interesting exercise would be to compare their behavior in practical applications.</p><p>Besides these technical questions, several conceptual questions regarding spectral learning and its relations to EM remain open. In particular, we would like to have a deeper understanding of the relations between EM, spectral learning and split-merge algorithms, both from a theoretical perspective and from a practical point of view. On the other hand, the principles that underlie spectral learning can be applied to any computational or probabilistic model with some notion of locality, in the sense that the model admits some strong Markov-like conditional independence assumptions. Several extensions along these lines can already be found in the literature, but the limits of these techniques remain largely unknown. From the perspective of grammatical inference, learning beyond stochastic rational languages is the most promising line of work.</p></div></div></section>
                        
                    

                    <section aria-labelledby="notes"><div class="c-article-section" id="notes-section"><h2 class="c-article-section__title u-h2 js-section-title js-c-reading-companion-sections-item" id="notes">Notes</h2><div class="c-article-section__content" id="notes-content"><ol class="c-article-footnote c-article-footnote--listed"><li class="c-article-footnote--listed__item" id="Fn1"><span class="c-article-footnote--listed__index">1.</span><div class="c-article-footnote--listed__content"><p>A similar notion can be defined for suffixes as well.</p></div></li><li class="c-article-footnote--listed__item" id="Fn2"><span class="c-article-footnote--listed__index">2.</span><div class="c-article-footnote--listed__content"><p>This is not always the case, see Denis and Esposito (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="&#xA;Denis, F., &amp; Esposito, Y. (2008). On rational stochastic languages. Fundamenta Informaticae, 86(1–2), 41–77.&#xA;" href="/article/10.1007/s10994-013-5416-x#ref-CR23" id="ref-link-section-d84034e5077">2008</a>) for details.</p></div></li><li class="c-article-footnote--listed__item" id="Fn3"><span class="c-article-footnote--listed__index">3.</span><div class="c-article-footnote--listed__content"><p>Throughout the paper we assume we can distinguish the words in a derivation, irrespective of whether two words at different positions correspond to the same symbol.</p></div></li><li class="c-article-footnote--listed__item" id="Fn4"><span class="c-article-footnote--listed__index">4.</span><div class="c-article-footnote--listed__content"><p>It must be clear that <i>f</i>=1 is not equivalent to a <i>Σ</i>′ basis. While both have the same basis size, the <i>Σ</i>′ basis only has sequences of length ≤1, while the extended model may include longer sequences and discard unfrequent symbols.</p></div></li><li class="c-article-footnote--listed__item" id="Fn5"><span class="c-article-footnote--listed__index">5.</span><div class="c-article-footnote--listed__content"><p>For <i>f</i>&gt;10 we did not see significant improvements in the performance.</p></div></li><li class="c-article-footnote--listed__item" id="Fn6"><span class="c-article-footnote--listed__index">6.</span><div class="c-article-footnote--listed__content"><p>Technically, when working with the projected operators the state-distribution vectors will not be distributions in the formal sense. However, they correspond to a projection of a state distribution, for some projection that we do not recover from data (namely a change of basis as discussed in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10994-013-5416-x#Sec5">2.2</a>). This projection has no effect on the computations because it cancels out.</p></div></li></ol></div></div></section><section aria-labelledby="Bib1"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title u-h2 js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references"><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="A.. Anandkumar, D. P.. Foster, D.. Hsu, S.. Kakade, Y. K.. Liu, " /><meta itemprop="datePublished" content="2012" /><meta itemprop="headline" content="&#xA;Anandkumar, A., Foster, D. P., Hsu, D., Kakade, S., &amp; Liu, Y. K. (2012a). A spectral algorithm for latent Dir" /><p class="c-article-references__text" id="ref-CR1">
Anandkumar, A., Foster, D. P., Hsu, D., Kakade, S., &amp; Liu, Y. K. (2012a). A spectral algorithm for latent Dirichlet allocation. In P. L. Bartlett, F. C. N. Pereira, C. J. C. Burges, L. Bottou, &amp; K. Q. Weinberger (Eds.), <i>NIPS</i> (pp. 926–934).
</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 1 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=NIPS&amp;pages=926-934&amp;publication_year=2012&amp;author=Anandkumar%2CA.&amp;author=Foster%2CD.%20P.&amp;author=Hsu%2CD.&amp;author=Kakade%2CS.&amp;author=Liu%2CY.%20K.">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="&#xA;Anandkumar, A., Ge, R., Hsu, D., Kakade, S. M., &amp; Telgarsky, M. (2012b). Tensor decompositions for learning l" /><p class="c-article-references__text" id="ref-CR2">
Anandkumar, A., Ge, R., Hsu, D., Kakade, S. M., &amp; Telgarsky, M. (2012b). <i>Tensor decompositions for learning latent variable models.</i>
					                      <a href="http://arxiv.org/abs/arXiv:1210.7559">arXiv:1210.7559</a>.
</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="A.. Anandkumar, D.. Hsu, S. M.. Kakade, " /><meta itemprop="datePublished" content="2012" /><meta itemprop="headline" content="&#xA;Anandkumar, A., Hsu, D., &amp; Kakade, S. M. (2012c). A method of moments for mixture models and hidden Markov mo" /><p class="c-article-references__text" id="ref-CR3">
Anandkumar, A., Hsu, D., &amp; Kakade, S. M. (2012c). A method of moments for mixture models and hidden Markov models. <i>Journal of Machine Learning Research—Proceedings Track</i>, <i>23</i>, 33.1–33.34.
</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 3 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20method%20of%20moments%20for%20mixture%20models%20and%20hidden%20Markov%20models&amp;journal=Journal%20of%20Machine%20Learning%20Research%E2%80%94Proceedings%20Track&amp;volume=23&amp;pages=33.1-33.34&amp;publication_year=2012&amp;author=Anandkumar%2CA.&amp;author=Hsu%2CD.&amp;author=Kakade%2CS.%20M.">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="R.. Bailly, " /><meta itemprop="datePublished" content="2011" /><meta itemprop="headline" content="&#xA;Bailly, R. (2011). Quadratic weighted automata: spectral algorithm and likelihood maximization. Journal of Ma" /><p class="c-article-references__text" id="ref-CR4">
Bailly, R. (2011). Quadratic weighted automata: spectral algorithm and likelihood maximization. <i>Journal of Machine Learning Research—Proceedings Track</i>, <i>20</i>, 147–163.
</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 4 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Quadratic%20weighted%20automata%3A%20spectral%20algorithm%20and%20likelihood%20maximization&amp;journal=Journal%20of%20Machine%20Learning%20Research%E2%80%94Proceedings%20Track&amp;volume=20&amp;pages=147-163&amp;publication_year=2011&amp;author=Bailly%2CR.">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="R.. Bailly, F.. Denis, L.. Ralaivola, " /><meta itemprop="datePublished" content="2009" /><meta itemprop="headline" content="&#xA;Bailly, R., Denis, F., &amp; Ralaivola, L. (2009). Grammatical inference as a principal component analysis proble" /><p class="c-article-references__text" id="ref-CR5">
Bailly, R., Denis, F., &amp; Ralaivola, L. (2009). Grammatical inference as a principal component analysis problem. In L. Bottou &amp; M. Littman (Eds.), <i>Proceedings of the 26th international conference on machine learning</i> (pp. 33–40). Montreal: Omnipress.
</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 5 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Proceedings%20of%20the%2026th%20international%20conference%20on%20machine%20learning&amp;pages=33-40&amp;publication_year=2009&amp;author=Bailly%2CR.&amp;author=Denis%2CF.&amp;author=Ralaivola%2CL.">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="R.. Bailly, A.. Habrard, F.. Denis, " /><meta itemprop="datePublished" content="2010" /><meta itemprop="headline" content="&#xA;Bailly, R., Habrard, A., &amp; Denis, F. (2010). A spectral approach for probabilistic grammatical inference on t" /><p class="c-article-references__text" id="ref-CR6">
Bailly, R., Habrard, A., &amp; Denis, F. (2010). A spectral approach for probabilistic grammatical inference on trees. In M. Hutter, F. Stephan, V. Vovk, &amp; T. Zeugmann (Eds.), <i>Lecture notes in computer science</i> (Vol. 6331, pp. 74–88). Berlin: Springer.
</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 6 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Lecture%20notes%20in%20computer%20science&amp;pages=74-88&amp;publication_year=2010&amp;author=Bailly%2CR.&amp;author=Habrard%2CA.&amp;author=Denis%2CF.">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="J. K.. Baker, " /><meta itemprop="datePublished" content="1979" /><meta itemprop="headline" content="&#xA;Baker, J. K. (1979). Trainable grammars for speech recognition. In D. H. Klatt &amp; J. J. Wolf (Eds.), Speech co" /><p class="c-article-references__text" id="ref-CR7">
Baker, J. K. (1979). Trainable grammars for speech recognition. In D. H. Klatt &amp; J. J. Wolf (Eds.), <i>Speech communication papers for the 97th meeting of the Acoustical Society of America</i> (pp. 547–550).
</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 7 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Speech%20communication%20papers%20for%20the%2097th%20meeting%20of%20the%20Acoustical%20Society%20of%20America&amp;pages=547-550&amp;publication_year=1979&amp;author=Baker%2CJ.%20K.">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="&#xA;Balle, B. (2013). Learning finite-state machines: algorithmic and statistical aspects. PhD thesis, Universita" /><p class="c-article-references__text" id="ref-CR8">
Balle, B. (2013). <i>Learning finite-state machines: algorithmic and statistical aspects</i>. PhD thesis, Universitat Politècnica de Catalunya.
</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="B.. Balle, M.. Mohri, " /><meta itemprop="datePublished" content="2012" /><meta itemprop="headline" content="&#xA;Balle, B., &amp; Mohri, M. (2012). Spectral learning of general weighted automata via constrained matrix completi" /><p class="c-article-references__text" id="ref-CR9">
Balle, B., &amp; Mohri, M. (2012). Spectral learning of general weighted automata via constrained matrix completion. In <i>Advances in neural information processing systems</i> (Vol. 25, pp. 2168–2176).
</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 9 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Advances%20in%20neural%20information%20processing%20systems&amp;pages=2168-2176&amp;publication_year=2012&amp;author=Balle%2CB.&amp;author=Mohri%2CM.">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="B.. Balle, A.. Quattoni, X.. Carreras, " /><meta itemprop="datePublished" content="2011" /><meta itemprop="headline" content="&#xA;Balle, B., Quattoni, A., &amp; Carreras, X. (2011). A spectral learning algorithm for finite state transducers. I" /><p class="c-article-references__text" id="ref-CR10">
Balle, B., Quattoni, A., &amp; Carreras, X. (2011). A spectral learning algorithm for finite state transducers. In D. Gunopulos, T. Hofmann, D. Malerba, &amp; M. Vazirgiannis (Eds.), <i>Lecture notes in computer science: Vol.</i> <i>6911</i>. <i>ECML/PKDD (1)</i> (pp. 156–171). Berlin: Springer.
</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 10 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=ECML%2FPKDD%20%281%29&amp;pages=156-171&amp;publication_year=2011&amp;author=Balle%2CB.&amp;author=Quattoni%2CA.&amp;author=Carreras%2CX.">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="B.. Balle, A.. Quattoni, X.. Carreras, " /><meta itemprop="datePublished" content="2012" /><meta itemprop="headline" content="&#xA;Balle, B., Quattoni, A., &amp; Carreras, X. (2012). Local loss optimization in operator models: a new insight int" /><p class="c-article-references__text" id="ref-CR11">
Balle, B., Quattoni, A., &amp; Carreras, X. (2012). Local loss optimization in operator models: a new insight into spectral learning. In J. Langford &amp; J. Pineau (Eds.), <i>Proceedings of the 29th international conference on machine learning (ICML-2012), ICML’12</i> (pp. 1879–1886). New York: Omnipress.
</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 11 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Proceedings%20of%20the%2029th%20international%20conference%20on%20machine%20learning%20%28ICML-2012%29%2C%20ICML%E2%80%9912&amp;pages=1879-1886&amp;publication_year=2012&amp;author=Balle%2CB.&amp;author=Quattoni%2CA.&amp;author=Carreras%2CX.">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="B.. Balle, J.. Castro, R.. Gavaldà, " /><meta itemprop="datePublished" content="2013" /><meta itemprop="headline" content="&#xA;Balle, B., Castro, J., &amp; Gavaldà, R. (2013). Learning probabilistic automata: a study in state distinguishabi" /><p class="c-article-references__text" id="ref-CR12">
Balle, B., Castro, J., &amp; Gavaldà, R. (2013). Learning probabilistic automata: a study in state distinguishability. <i>Theoretical Computer Science</i>, <i>473</i>, 46–60.
</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="http://www.emis.de/MATH-item?1257.68085" aria-label="View reference 12 on MATH">MATH</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="http://www.ams.org/mathscinet-getitem?mr=3015338" aria-label="View reference 12 on MathSciNet">MathSciNet</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="https://doi.org/10.1016%2Fj.tcs.2012.10.009" aria-label="View reference 12">Article</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 12 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Learning%20probabilistic%20automata%3A%20a%20study%20in%20state%20distinguishability&amp;journal=Theoretical%20Computer%20Science&amp;volume=473&amp;pages=46-60&amp;publication_year=2013&amp;author=Balle%2CB.&amp;author=Castro%2CJ.&amp;author=Gavald%C3%A0%2CR.">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="A.. Beimel, F.. Bergadano, N.. Bshouty, E.. Kushilevitz, S.. Varricchio, " /><meta itemprop="datePublished" content="2000" /><meta itemprop="headline" content="&#xA;Beimel, A., Bergadano, F., Bshouty, N., Kushilevitz, E., &amp; Varricchio, S. (2000). Learning functions represen" /><p class="c-article-references__text" id="ref-CR13">
Beimel, A., Bergadano, F., Bshouty, N., Kushilevitz, E., &amp; Varricchio, S. (2000). Learning functions represented as multiplicity automata. <i>Journal of the ACM</i>, <i>47</i>, 506–530.
</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="http://www.emis.de/MATH-item?1094.68575" aria-label="View reference 13 on MATH">MATH</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="http://www.ams.org/mathscinet-getitem?mr=1768145" aria-label="View reference 13 on MathSciNet">MathSciNet</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="https://doi.org/10.1145%2F337244.337257" aria-label="View reference 13">Article</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 13 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Learning%20functions%20represented%20as%20multiplicity%20automata&amp;journal=Journal%20of%20the%20ACM&amp;volume=47&amp;pages=506-530&amp;publication_year=2000&amp;author=Beimel%2CA.&amp;author=Bergadano%2CF.&amp;author=Bshouty%2CN.&amp;author=Kushilevitz%2CE.&amp;author=Varricchio%2CS.">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="J.. Berstel, C.. Reutenauer, " /><meta itemprop="datePublished" content="1988" /><meta itemprop="headline" content="&#xA;Berstel, J., &amp; Reutenauer, C. (1988). Rational series and their languages. Berlin: Springer.&#xA;" /><p class="c-article-references__text" id="ref-CR14">
Berstel, J., &amp; Reutenauer, C. (1988). <i>Rational series and their languages</i>. Berlin: Springer.
</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 14 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Rational%20series%20and%20their%20languages&amp;publication_year=1988&amp;author=Berstel%2CJ.&amp;author=Reutenauer%2CC.">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="B.. Boots, S.. Siddiqi, G.. Gordon, " /><meta itemprop="datePublished" content="2011" /><meta itemprop="headline" content="&#xA;Boots, B., Siddiqi, S., &amp; Gordon, G. (2011). Closing the learning planning loop with predictive state represe" /><p class="c-article-references__text" id="ref-CR15">
Boots, B., Siddiqi, S., &amp; Gordon, G. (2011). Closing the learning planning loop with predictive state representations. <i>The International Journal of Robotics Research</i>, <i>30</i>(7), 954–966.
</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="https://doi.org/10.1177%2F0278364911404092" aria-label="View reference 15">Article</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 15 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Closing%20the%20learning%20planning%20loop%20with%20predictive%20state%20representations&amp;journal=The%20International%20Journal%20of%20Robotics%20Research&amp;volume=30&amp;issue=7&amp;pages=954-966&amp;publication_year=2011&amp;author=Boots%2CB.&amp;author=Siddiqi%2CS.&amp;author=Gordon%2CG.">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="J. W.. Carlyle, A.. Paz, " /><meta itemprop="datePublished" content="1971" /><meta itemprop="headline" content="&#xA;Carlyle, J. W., &amp; Paz, A. (1971). Realizations by stochastic finite automata. Journal of Computer and System " /><p class="c-article-references__text" id="ref-CR16">
Carlyle, J. W., &amp; Paz, A. (1971). Realizations by stochastic finite automata. <i>Journal of Computer and System Sciences</i>, <i>5</i>(1), 26–40.
</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="http://www.emis.de/MATH-item?0236.94042" aria-label="View reference 16 on MATH">MATH</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="http://www.ams.org/mathscinet-getitem?mr=284295" aria-label="View reference 16 on MathSciNet">MathSciNet</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="https://doi.org/10.1016%2FS0022-0000%2871%2980005-3" aria-label="View reference 16">Article</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 16 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Realizations%20by%20stochastic%20finite%20automata&amp;journal=Journal%20of%20Computer%20and%20System%20Sciences&amp;volume=5&amp;issue=1&amp;pages=26-40&amp;publication_year=1971&amp;author=Carlyle%2CJ.%20W.&amp;author=Paz%2CA.">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="J.. Castro, R.. Gavaldà, " /><meta itemprop="datePublished" content="2008" /><meta itemprop="headline" content="&#xA;Castro, J., &amp; Gavaldà, R. (2008). Towards feasible PAC-learning of probabilistic deterministic finite automat" /><p class="c-article-references__text" id="ref-CR17">
Castro, J., &amp; Gavaldà, R. (2008). Towards feasible PAC-learning of probabilistic deterministic finite automata. In <i>Proceedings of the 9th international colloquium on grammatical inference (ICGI)</i> (pp. 163–174).
</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 17 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Proceedings%20of%20the%209th%20international%20colloquium%20on%20grammatical%20inference%20%28ICGI%29&amp;pages=163-174&amp;publication_year=2008&amp;author=Castro%2CJ.&amp;author=Gavald%C3%A0%2CR.">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="A.. Clark, F.. Thollard, " /><meta itemprop="datePublished" content="2004" /><meta itemprop="headline" content="&#xA;Clark, A., &amp; Thollard, F. (2004). PAC-learnability of probabilistic deterministic finite state automata. Jour" /><p class="c-article-references__text" id="ref-CR18">
Clark, A., &amp; Thollard, F. (2004). PAC-learnability of probabilistic deterministic finite state automata. <i>Journal of Machine Learning Research</i>, <i>5</i>, 473–497.
</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="http://www.emis.de/MATH-item?1222.68094" aria-label="View reference 18 on MATH">MATH</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="http://www.ams.org/mathscinet-getitem?mr=2247988" aria-label="View reference 18 on MathSciNet">MathSciNet</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 18 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=PAC-learnability%20of%20probabilistic%20deterministic%20finite%20state%20automata&amp;journal=Journal%20of%20Machine%20Learning%20Research&amp;volume=5&amp;pages=473-497&amp;publication_year=2004&amp;author=Clark%2CA.&amp;author=Thollard%2CF.">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="S.. Clark, J. R.. Curran, " /><meta itemprop="datePublished" content="2004" /><meta itemprop="headline" content="&#xA;Clark, S., &amp; Curran, J. R. (2004). Parsing the WSJ using CCG and log-linear models. In Proceedings of the 42n" /><p class="c-article-references__text" id="ref-CR19">
Clark, S., &amp; Curran, J. R. (2004). Parsing the WSJ using CCG and log-linear models. In <i>Proceedings of the 42nd meeting of the association for computational linguistics (ACL’04), main volume</i>, Barcelona, Spain (pp. 103–110).
</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 19 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Proceedings%20of%20the%2042nd%20meeting%20of%20the%20association%20for%20computational%20linguistics%20%28ACL%E2%80%9904%29%2C%20main%20volume&amp;pages=103-110&amp;publication_year=2004&amp;author=Clark%2CS.&amp;author=Curran%2CJ.%20R.">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="S. B.. Cohen, K.. Stratos, M.. Collins, D. P.. Foster, L.. Ungar, " /><meta itemprop="datePublished" content="2012" /><meta itemprop="headline" content="&#xA;Cohen, S. B., Stratos, K., Collins, M., Foster, D. P., &amp; Ungar, L. (2012). Spectral learning of latent-variab" /><p class="c-article-references__text" id="ref-CR20">
Cohen, S. B., Stratos, K., Collins, M., Foster, D. P., &amp; Ungar, L. (2012). Spectral learning of latent-variable PCFGS. In <i>Proceedings of the 50th annual meeting of the association for computational linguistics (Volume 1: Long papers)</i> (pp. 223–231). Jeju Island: Association for Computational Linguistics.
</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 20 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Proceedings%20of%20the%2050th%20annual%20meeting%20of%20the%20association%20for%20computational%20linguistics%20%28Volume%201%3A%20Long%20papers%29&amp;pages=223-231&amp;publication_year=2012&amp;author=Cohen%2CS.%20B.&amp;author=Stratos%2CK.&amp;author=Collins%2CM.&amp;author=Foster%2CD.%20P.&amp;author=Ungar%2CL.">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="S. B.. Cohen, K.. Stratos, M.. Collins, D. P.. Foster, L.. Ungar, " /><meta itemprop="datePublished" content="2013" /><meta itemprop="headline" content="&#xA;Cohen, S. B., Stratos, K., Collins, M., Foster, D. P., &amp; Ungar, L. (2013). Experiments with spectral learning" /><p class="c-article-references__text" id="ref-CR21">
Cohen, S. B., Stratos, K., Collins, M., Foster, D. P., &amp; Ungar, L. (2013). Experiments with spectral learning of latent-variable pcfgs. In <i>Proceedings of the 2013 conference of the North American chapter of the Association for Computational Linguistics: human language technologies</i> (pp. 148–157). Atlanta: Association for Computational Linguistics.
</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 21 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Proceedings%20of%20the%202013%20conference%20of%20the%20North%20American%20chapter%20of%20the%20Association%20for%20Computational%20Linguistics%3A%20human%20language%20technologies&amp;pages=148-157&amp;publication_year=2013&amp;author=Cohen%2CS.%20B.&amp;author=Stratos%2CK.&amp;author=Collins%2CM.&amp;author=Foster%2CD.%20P.&amp;author=Ungar%2CL.">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="A. P.. Dempster, N. M.. Laird, D. B.. Rubin, " /><meta itemprop="datePublished" content="1977" /><meta itemprop="headline" content="&#xA;Dempster, A. P., Laird, N. M., &amp; Rubin, D. B. (1977). Maximum likelihood from incomplete data via the EM algo" /><p class="c-article-references__text" id="ref-CR22">
Dempster, A. P., Laird, N. M., &amp; Rubin, D. B. (1977). Maximum likelihood from incomplete data via the EM algorithm. <i>Journal of the Royal Statistical Society</i>, <i>39</i>(1), 1–38.
</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="http://www.emis.de/MATH-item?0364.62022" aria-label="View reference 22 on MATH">MATH</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="http://www.ams.org/mathscinet-getitem?mr=501537" aria-label="View reference 22 on MathSciNet">MathSciNet</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 22 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Maximum%20likelihood%20from%20incomplete%20data%20via%20the%20EM%20algorithm&amp;journal=Journal%20of%20the%20Royal%20Statistical%20Society&amp;volume=39&amp;issue=1&amp;pages=1-38&amp;publication_year=1977&amp;author=Dempster%2CA.%20P.&amp;author=Laird%2CN.%20M.&amp;author=Rubin%2CD.%20B.">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="F.. Denis, Y.. Esposito, " /><meta itemprop="datePublished" content="2008" /><meta itemprop="headline" content="&#xA;Denis, F., &amp; Esposito, Y. (2008). On rational stochastic languages. Fundamenta Informaticae, 86(1–2), 41–77.&#xA;" /><p class="c-article-references__text" id="ref-CR23">
Denis, F., &amp; Esposito, Y. (2008). On rational stochastic languages. <i>Fundamenta Informaticae</i>, <i>86</i>(1–2), 41–77.
</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="http://www.emis.de/MATH-item?1159.68015" aria-label="View reference 23 on MATH">MATH</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="http://www.ams.org/mathscinet-getitem?mr=2462489" aria-label="View reference 23 on MathSciNet">MathSciNet</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 23 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=On%20rational%20stochastic%20languages&amp;journal=Fundamenta%20Informaticae&amp;volume=86&amp;issue=1%E2%80%932&amp;pages=41-77&amp;publication_year=2008&amp;author=Denis%2CF.&amp;author=Esposito%2CY.">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="P.. Dhillon, J.. Rodu, M.. Collins, D.. Foster, L.. Ungar, " /><meta itemprop="datePublished" content="2012" /><meta itemprop="headline" content="&#xA;Dhillon, P., Rodu, J., Collins, M., Foster, D., &amp; Ungar, L. (2012). Spectral dependency parsing with latent v" /><p class="c-article-references__text" id="ref-CR24">
Dhillon, P., Rodu, J., Collins, M., Foster, D., &amp; Ungar, L. (2012). Spectral dependency parsing with latent variables. In <i>Proceedings of the 2012 joint conference on empirical methods in natural language processing and computational natural language learning</i> (pp. 205–213). Jeju Island: Association for Computational Linguistics.
</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 24 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Proceedings%20of%20the%202012%20joint%20conference%20on%20empirical%20methods%20in%20natural%20language%20processing%20and%20computational%20natural%20language%20learning&amp;pages=205-213&amp;publication_year=2012&amp;author=Dhillon%2CP.&amp;author=Rodu%2CJ.&amp;author=Collins%2CM.&amp;author=Foster%2CD.&amp;author=Ungar%2CL.">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="J.. Eisner, " /><meta itemprop="datePublished" content="2000" /><meta itemprop="headline" content="&#xA;Eisner, J. (2000). Bilexical grammars and their cubic-time parsing algorithms. In H. Bunt &amp; A. Nijholt (Eds.)" /><p class="c-article-references__text" id="ref-CR25">
Eisner, J. (2000). Bilexical grammars and their cubic-time parsing algorithms. In H. Bunt &amp; A. Nijholt (Eds.), <i>Advances in probabilistic and other parsing technologies</i> (pp. 29–62). Norwell: Kluwer Academic.
</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 25 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Advances%20in%20probabilistic%20and%20other%20parsing%20technologies&amp;pages=29-62&amp;publication_year=2000&amp;author=Eisner%2CJ.">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="J.. Eisner, G.. Satta, " /><meta itemprop="datePublished" content="1999" /><meta itemprop="headline" content="&#xA;Eisner, J., &amp; Satta, G. (1999). Efficient parsing for bilexical context-free grammars and head-automaton gram" /><p class="c-article-references__text" id="ref-CR26">
Eisner, J., &amp; Satta, G. (1999). Efficient parsing for bilexical context-free grammars and head-automaton grammars. In <i>Proceedings of the 37th annual meeting of the Association for Computational Linguistics (ACL), University of Maryland</i> (pp. 457–464).
</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 26 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Proceedings%20of%20the%2037th%20annual%20meeting%20of%20the%20Association%20for%20Computational%20Linguistics%20%28ACL%29%2C%20University%20of%20Maryland&amp;pages=457-464&amp;publication_year=1999&amp;author=Eisner%2CJ.&amp;author=Satta%2CG.">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="J.. Eisner, N. A.. Smith, " /><meta itemprop="datePublished" content="2010" /><meta itemprop="headline" content="&#xA;Eisner, J., &amp; Smith, N. A. (2010). Favor short dependencies: parsing with soft and hard constraints on depend" /><p class="c-article-references__text" id="ref-CR27">
Eisner, J., &amp; Smith, N. A. (2010). Favor short dependencies: parsing with soft and hard constraints on dependency length. In H. Bunt, P. Merlo, &amp; J. Nivre (Eds.), <i>Trends in parsing technology: dependency parsing, domain adaptation, and deep parsing</i> (Vol. 8, pp. 121–150). Berlin: Springer.
</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 27 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Trends%20in%20parsing%20technology%3A%20dependency%20parsing%2C%20domain%20adaptation%2C%20and%20deep%20parsing&amp;pages=121-150&amp;publication_year=2010&amp;author=Eisner%2CJ.&amp;author=Smith%2CN.%20A.">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="M.. Fliess, " /><meta itemprop="datePublished" content="1974" /><meta itemprop="headline" content="&#xA;Fliess, M. (1974). Matrices de Hankel. Journal de Mathématiques Pures et Appliquées, 53, 197–222.&#xA;" /><p class="c-article-references__text" id="ref-CR28">
Fliess, M. (1974). Matrices de Hankel. <i>Journal de Mathématiques Pures et Appliquées</i>, <i>53</i>, 197–222.
</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="http://www.emis.de/MATH-item?0315.94051" aria-label="View reference 28 on MATH">MATH</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="http://www.ams.org/mathscinet-getitem?mr=364328" aria-label="View reference 28 on MathSciNet">MathSciNet</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 28 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Matrices%20de%20Hankel&amp;journal=Journal%20de%20Math%C3%A9matiques%20Pures%20et%20Appliqu%C3%A9es&amp;volume=53&amp;pages=197-222&amp;publication_year=1974&amp;author=Fliess%2CM.">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="J.. Goodman, " /><meta itemprop="datePublished" content="1996" /><meta itemprop="headline" content="&#xA;Goodman, J. (1996). Parsing algorithms and metrics. In Proceedings of the 34th annual meeting of the Associat" /><p class="c-article-references__text" id="ref-CR29">
Goodman, J. (1996). Parsing algorithms and metrics. In <i>Proceedings of the 34th annual meeting of the Association for Computational Linguistics</i> (pp. 177–183). Santa Cruz: Association for Computational Linguistics.
</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 29 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Proceedings%20of%20the%2034th%20annual%20meeting%20of%20the%20Association%20for%20Computational%20Linguistics&amp;pages=177-183&amp;publication_year=1996&amp;author=Goodman%2CJ.">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="D.. Hsu, S. M.. Kakade, T.. Zhang, " /><meta itemprop="datePublished" content="2009" /><meta itemprop="headline" content="&#xA;Hsu, D., Kakade, S. M., &amp; Zhang, T. (2009). A spectral algorithm for learning hidden Markov models. In Procee" /><p class="c-article-references__text" id="ref-CR30">
Hsu, D., Kakade, S. M., &amp; Zhang, T. (2009). A spectral algorithm for learning hidden Markov models. In <i>Proceedings of the annual conference on computational learning theory (COLT)</i>.
</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 30 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Proceedings%20of%20the%20annual%20conference%20on%20computational%20learning%20theory%20%28COLT%29&amp;publication_year=2009&amp;author=Hsu%2CD.&amp;author=Kakade%2CS.%20M.&amp;author=Zhang%2CT.">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="D.. Hsu, S. M.. Kakade, P.. Liang, " /><meta itemprop="datePublished" content="2012" /><meta itemprop="headline" content="&#xA;Hsu, D., Kakade, S. M., &amp; Liang, P. (2012). Identifiability and unmixing of latent parse trees. Advances in n" /><p class="c-article-references__text" id="ref-CR31">
Hsu, D., Kakade, S. M., &amp; Liang, P. (2012). <i>Identifiability and unmixing of latent parse trees</i>. <i>Advances in neural information processing systems (NIPS)</i>.
</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 31 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Identifiability%20and%20unmixing%20of%20latent%20parse%20trees&amp;publication_year=2012&amp;author=Hsu%2CD.&amp;author=Kakade%2CS.%20M.&amp;author=Liang%2CP.">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="M.. Kearns, Y.. Mansour, D.. Ron, R.. Rubinfeld, R. E.. Schapire, L.. Sellie, " /><meta itemprop="datePublished" content="1994" /><meta itemprop="headline" content="&#xA;Kearns, M., Mansour, Y., Ron, D., Rubinfeld, R., Schapire, R. E., &amp; Sellie, L. (1994). On the learnability of" /><p class="c-article-references__text" id="ref-CR32">
Kearns, M., Mansour, Y., Ron, D., Rubinfeld, R., Schapire, R. E., &amp; Sellie, L. (1994). On the learnability of discrete distributions. <i>STOC ’94</i>. In <i>Proceedings of the twenty-sixth annual ACM symposium on theory of computing</i> (pp. 273–282). New York: ACM.
</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 32 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Proceedings%20of%20the%20twenty-sixth%20annual%20ACM%20symposium%20on%20theory%20of%20computing&amp;pages=273-282&amp;publication_year=1994&amp;author=Kearns%2CM.&amp;author=Mansour%2CY.&amp;author=Ron%2CD.&amp;author=Rubinfeld%2CR.&amp;author=Schapire%2CR.%20E.&amp;author=Sellie%2CL.">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="F. M.. Luque, A.. Quattoni, B.. Balle, X.. Carreras, " /><meta itemprop="datePublished" content="2012" /><meta itemprop="headline" content="&#xA;Luque, F. M., Quattoni, A., Balle, B., &amp; Carreras, X. (2012). Spectral learning for non-deterministic depende" /><p class="c-article-references__text" id="ref-CR33">
Luque, F. M., Quattoni, A., Balle, B., &amp; Carreras, X. (2012). Spectral learning for non-deterministic dependency parsing. In <i>Proceedings of the 13th conference of the European chapter of the Association for Computational Linguistics</i> (pp. 409–419). Avignon: Association for Computational Linguistics.
</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 33 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Proceedings%20of%20the%2013th%20conference%20of%20the%20European%20chapter%20of%20the%20Association%20for%20Computational%20Linguistics&amp;pages=409-419&amp;publication_year=2012&amp;author=Luque%2CF.%20M.&amp;author=Quattoni%2CA.&amp;author=Balle%2CB.&amp;author=Carreras%2CX.">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="C. D.. Manning, P.. Raghavan, H.. Schütze, " /><meta itemprop="datePublished" content="2008" /><meta itemprop="headline" content="&#xA;Manning, C. D., Raghavan, P., &amp; Schütze, H. (2008). Introduction to information retrieval (1st ed.). Cambridg" /><p class="c-article-references__text" id="ref-CR34">
Manning, C. D., Raghavan, P., &amp; Schütze, H. (2008). <i>Introduction to information retrieval</i> (1st ed.). Cambridge: Cambridge University Press.
</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 34 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Introduction%20to%20information%20retrieval&amp;publication_year=2008&amp;author=Manning%2CC.%20D.&amp;author=Raghavan%2CP.&amp;author=Sch%C3%BCtze%2CH.">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="M. P.. Marcus, B.. Santorini, M. A.. Marcinkiewicz, " /><meta itemprop="datePublished" content="1993" /><meta itemprop="headline" content="&#xA;Marcus, M. P., Santorini, B., &amp; Marcinkiewicz, M. A. (1993). Building a large annotated corpus of English: th" /><p class="c-article-references__text" id="ref-CR35">
Marcus, M. P., Santorini, B., &amp; Marcinkiewicz, M. A. (1993). Building a large annotated corpus of English: the Penn Treebank. <i>Computational Linguistics</i>, <i>19</i>, 313–330.
</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 35 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Building%20a%20large%20annotated%20corpus%20of%20English%3A%20the%20Penn%20Treebank&amp;journal=Computational%20Linguistics&amp;volume=19&amp;pages=313-330&amp;publication_year=1993&amp;author=Marcus%2CM.%20P.&amp;author=Santorini%2CB.&amp;author=Marcinkiewicz%2CM.%20A.">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="R.. McDonald, F.. Pereira, " /><meta itemprop="datePublished" content="2006" /><meta itemprop="headline" content="&#xA;McDonald, R., &amp; Pereira, F. (2006). Online learning of approximate dependency parsing algorithms. In Proceedi" /><p class="c-article-references__text" id="ref-CR36">
McDonald, R., &amp; Pereira, F. (2006). Online learning of approximate dependency parsing algorithms. In <i>Proceedings of the 11th conference of the European chapter of the Association for Computational Linguistics</i> (pp. 81–88).
</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 36 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Proceedings%20of%20the%2011th%20conference%20of%20the%20European%20chapter%20of%20the%20Association%20for%20Computational%20Linguistics&amp;pages=81-88&amp;publication_year=2006&amp;author=McDonald%2CR.&amp;author=Pereira%2CF.">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="R.. McDonald, F.. Pereira, K.. Ribarov, J.. Hajic, " /><meta itemprop="datePublished" content="2005" /><meta itemprop="headline" content="&#xA;McDonald, R., Pereira, F., Ribarov, K., &amp; Hajic, J. (2005). Non-projective dependency parsing using spanning " /><p class="c-article-references__text" id="ref-CR37">
McDonald, R., Pereira, F., Ribarov, K., &amp; Hajic, J. (2005). Non-projective dependency parsing using spanning tree algorithms. In <i>Proceedings of human language technology conference and conference on empirical methods in natural language processing</i> (pp. 523–530). Vancouver: Association for Computational Linguistics.
</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 37 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Proceedings%20of%20human%20language%20technology%20conference%20and%20conference%20on%20empirical%20methods%20in%20natural%20language%20processing&amp;pages=523-530&amp;publication_year=2005&amp;author=McDonald%2CR.&amp;author=Pereira%2CF.&amp;author=Ribarov%2CK.&amp;author=Hajic%2CJ.">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="M.. Mohri, " /><meta itemprop="datePublished" content="2009" /><meta itemprop="headline" content="&#xA;Mohri, M. (2009). Weighted automata algorithms. In M. Droste, W. Kuich, &amp; H. Vogler (Eds.), Monographs in the" /><p class="c-article-references__text" id="ref-CR38">
Mohri, M. (2009). Weighted automata algorithms. In M. Droste, W. Kuich, &amp; H. Vogler (Eds.), <i>Monographs in theoretical computer science. An EATCS series</i>. <i>Handbook of weighted automata</i> (pp. 213–254). Berlin: Springer.
</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 38 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Handbook%20of%20weighted%20automata&amp;pages=213-254&amp;publication_year=2009&amp;author=Mohri%2CM.">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="E.. Mossel, S.. Roch, " /><meta itemprop="datePublished" content="2005" /><meta itemprop="headline" content="&#xA;Mossel, E., &amp; Roch, S. (2005). Learning nonsingular phylogenies and hidden Markov models. In Proceedings of t" /><p class="c-article-references__text" id="ref-CR39">
Mossel, E., &amp; Roch, S. (2005). Learning nonsingular phylogenies and hidden Markov models. In <i>Proceedings of the 37th annual ACM symposium on theory of computing (STOC)</i> (pp. 366–375).
</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 39 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Proceedings%20of%20the%2037th%20annual%20ACM%20symposium%20on%20theory%20of%20computing%20%28STOC%29&amp;pages=366-375&amp;publication_year=2005&amp;author=Mossel%2CE.&amp;author=Roch%2CS.">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="N.. Palmer, P. W.. Goldberg, " /><meta itemprop="datePublished" content="2007" /><meta itemprop="headline" content="&#xA;Palmer, N., &amp; Goldberg, P. W. (2007). PAC-learnability of probabilistic deterministic finite state automata i" /><p class="c-article-references__text" id="ref-CR40">
Palmer, N., &amp; Goldberg, P. W. (2007). PAC-learnability of probabilistic deterministic finite state automata in terms of variation distance. <i>Theoretical Computer Science</i>, <i>387</i>(1), 18–31.
</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="http://www.emis.de/MATH-item?1143.68024" aria-label="View reference 40 on MATH">MATH</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="http://www.ams.org/mathscinet-getitem?mr=2359011" aria-label="View reference 40 on MathSciNet">MathSciNet</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="https://doi.org/10.1016%2Fj.tcs.2007.07.023" aria-label="View reference 40">Article</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 40 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=PAC-learnability%20of%20probabilistic%20deterministic%20finite%20state%20automata%20in%20terms%20of%20variation%20distance&amp;journal=Theoretical%20Computer%20Science&amp;volume=387&amp;issue=1&amp;pages=18-31&amp;publication_year=2007&amp;author=Palmer%2CN.&amp;author=Goldberg%2CP.%20W.">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="A.. Parikh, L.. Song, E.. Xing, " /><meta itemprop="datePublished" content="2011" /><meta itemprop="headline" content="&#xA;Parikh, A., Song, L., &amp; Xing, E. (2011). A spectral algorithm for latent tree graphical models. In Proceeding" /><p class="c-article-references__text" id="ref-CR41">
Parikh, A., Song, L., &amp; Xing, E. (2011). A spectral algorithm for latent tree graphical models. In <i>Proceedings of the 28th international conference on machine learning, ICML 2011 (ICML)</i> (pp. 1065–1072).
</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 41 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Proceedings%20of%20the%2028th%20international%20conference%20on%20machine%20learning%2C%20ICML%202011%20%28ICML%29&amp;pages=1065-1072&amp;publication_year=2011&amp;author=Parikh%2CA.&amp;author=Song%2CL.&amp;author=Xing%2CE.">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="J. D.. Park, A.. Darwiche, " /><meta itemprop="datePublished" content="2004" /><meta itemprop="headline" content="&#xA;Park, J. D., &amp; Darwiche, A. (2004). Complexity results and approximation strategies for map explanations. Jou" /><p class="c-article-references__text" id="ref-CR42">
Park, J. D., &amp; Darwiche, A. (2004). Complexity results and approximation strategies for map explanations. <i>Journal of Artificial Intelligence Research</i>, <i>21</i>, 101–133.
</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="http://www.emis.de/MATH-item?1080.68689" aria-label="View reference 42 on MATH">MATH</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="http://www.ams.org/mathscinet-getitem?mr=2130725" aria-label="View reference 42 on MathSciNet">MathSciNet</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 42 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Complexity%20results%20and%20approximation%20strategies%20for%20map%20explanations&amp;journal=Journal%20of%20Artificial%20Intelligence%20Research&amp;volume=21&amp;pages=101-133&amp;publication_year=2004&amp;author=Park%2CJ.%20D.&amp;author=Darwiche%2CA.">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="S.. Petrov, D.. Klein, " /><meta itemprop="datePublished" content="2007" /><meta itemprop="headline" content="&#xA;Petrov, S., &amp; Klein, D. (2007). Improved inference for unlexicalized parsing. Proceedings of the main confere" /><p class="c-article-references__text" id="ref-CR43">
Petrov, S., &amp; Klein, D. (2007). Improved inference for unlexicalized parsing. <i>Proceedings of the main conference, Association for Computational Linguistics</i>. In <i>Human language technologies 2007: the conference of the North American chapter of the Association for Computational Linguistics</i> (pp. 404–411). Rochester: Association for Computational Linguistics.
</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 43 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Human%20language%20technologies%202007%3A%20the%20conference%20of%20the%20North%20American%20chapter%20of%20the%20Association%20for%20Computational%20Linguistics&amp;pages=404-411&amp;publication_year=2007&amp;author=Petrov%2CS.&amp;author=Klein%2CD.">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="S.. Petrov, D.. Das, R.. McDonald, " /><meta itemprop="datePublished" content="2012" /><meta itemprop="headline" content="&#xA;Petrov, S., Das, D., &amp; McDonald, R. (2012). A universal part-of-speech tagset. In Proceedings of LREC.&#xA;" /><p class="c-article-references__text" id="ref-CR44">
Petrov, S., Das, D., &amp; McDonald, R. (2012). A universal part-of-speech tagset. In <i>Proceedings of LREC</i>.
</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 44 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Proceedings%20of%20LREC&amp;publication_year=2012&amp;author=Petrov%2CS.&amp;author=Das%2CD.&amp;author=McDonald%2CR.">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="D.. Ron, Y.. Singer, N.. Tishby, " /><meta itemprop="datePublished" content="1998" /><meta itemprop="headline" content="&#xA;Ron, D., Singer, Y., &amp; Tishby, N. (1998). On the learnability and usage of acyclic probabilistic finite autom" /><p class="c-article-references__text" id="ref-CR45">
Ron, D., Singer, Y., &amp; Tishby, N. (1998). On the learnability and usage of acyclic probabilistic finite automata. <i>Journal of Computing Systems Science</i>, <i>56</i>(2), 133–152.
</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="http://www.emis.de/MATH-item?0915.68124" aria-label="View reference 45 on MATH">MATH</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="http://www.ams.org/mathscinet-getitem?mr=1629686" aria-label="View reference 45 on MathSciNet">MathSciNet</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="https://doi.org/10.1006%2Fjcss.1997.1555" aria-label="View reference 45">Article</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 45 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=On%20the%20learnability%20and%20usage%20of%20acyclic%20probabilistic%20finite%20automata&amp;journal=Journal%20of%20Computing%20Systems%20Science&amp;volume=56&amp;issue=2&amp;pages=133-152&amp;publication_year=1998&amp;author=Ron%2CD.&amp;author=Singer%2CY.&amp;author=Tishby%2CN.">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="A.. Salomaa, M.. Soittola, " /><meta itemprop="datePublished" content="1978" /><meta itemprop="headline" content="&#xA;Salomaa, A., &amp; Soittola, M. (1978). Automata-theoretic aspects of formal power series. New York: Springer.&#xA;" /><p class="c-article-references__text" id="ref-CR46">
Salomaa, A., &amp; Soittola, M. (1978). <i>Automata-theoretic aspects of formal power series</i>. New York: Springer.
</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 46 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Automata-theoretic%20aspects%20of%20formal%20power%20series&amp;publication_year=1978&amp;author=Salomaa%2CA.&amp;author=Soittola%2CM.">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="M.. Schützenberger, " /><meta itemprop="datePublished" content="1961" /><meta itemprop="headline" content="&#xA;Schützenberger, M. (1961). On the definition of a family of automata. Information and Control, 4, 245–270.&#xA;" /><p class="c-article-references__text" id="ref-CR47">
Schützenberger, M. (1961). On the definition of a family of automata. <i>Information and Control</i>, <i>4</i>, 245–270.
</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="http://www.emis.de/MATH-item?0104.00702" aria-label="View reference 47 on MATH">MATH</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="http://www.ams.org/mathscinet-getitem?mr=135680" aria-label="View reference 47 on MathSciNet">MathSciNet</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="https://doi.org/10.1016%2FS0019-9958%2861%2980020-X" aria-label="View reference 47">Article</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 47 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=On%20the%20definition%20of%20a%20family%20of%20automata&amp;journal=Information%20and%20Control&amp;volume=4&amp;pages=245-270&amp;publication_year=1961&amp;author=Sch%C3%BCtzenberger%2CM.">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="S. M.. Siddiqi, B.. Boots, G. J.. Gordon, " /><meta itemprop="datePublished" content="2010" /><meta itemprop="headline" content="&#xA;Siddiqi, S. M., Boots, B., &amp; Gordon, G. J. (2010). Reduced-rank hidden Markov models. In Proceedings of the t" /><p class="c-article-references__text" id="ref-CR48">
Siddiqi, S. M., Boots, B., &amp; Gordon, G. J. (2010). Reduced-rank hidden Markov models. In <i>Proceedings of the thirteenth international conference on artificial intelligence and statistics (AISTATS)</i> (pp. 741–748).
</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 48 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Proceedings%20of%20the%20thirteenth%20international%20conference%20on%20artificial%20intelligence%20and%20statistics%20%28AISTATS%29&amp;pages=741-748&amp;publication_year=2010&amp;author=Siddiqi%2CS.%20M.&amp;author=Boots%2CB.&amp;author=Gordon%2CG.%20J.">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="L.. Song, S. M.. Siddiqi, G.. Gordon, A.. Smola, " /><meta itemprop="datePublished" content="2010" /><meta itemprop="headline" content="&#xA;Song, L., Siddiqi, S. M., Gordon, G., &amp; Smola, A. (2010). Hilbert space embeddings of hidden Markov models. I" /><p class="c-article-references__text" id="ref-CR49">
Song, L., Siddiqi, S. M., Gordon, G., &amp; Smola, A. (2010). Hilbert space embeddings of hidden Markov models. In J. Fürnkranz &amp; T. Joachims (Eds.), <i>Proceedings of the 27th international conference on machine learning (ICML-10)</i> (pp. 991–998). Haifa: Omnipress.
</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 49 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Proceedings%20of%20the%2027th%20international%20conference%20on%20machine%20learning%20%28ICML-10%29&amp;pages=991-998&amp;publication_year=2010&amp;author=Song%2CL.&amp;author=Siddiqi%2CS.%20M.&amp;author=Gordon%2CG.&amp;author=Smola%2CA.">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="I.. Titov, J.. Henderson, " /><meta itemprop="datePublished" content="2006" /><meta itemprop="headline" content="&#xA;Titov, I., &amp; Henderson, J. (2006). Loss minimization in parse reranking. In Proceedings of the 2006 conferenc" /><p class="c-article-references__text" id="ref-CR50">
Titov, I., &amp; Henderson, J. (2006). Loss minimization in parse reranking. In <i>Proceedings of the 2006 conference on empirical methods in natural language processing</i> (pp. 560–567). Sydney: Association for Computational Linguistics.
</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 50 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Proceedings%20of%20the%202006%20conference%20on%20empirical%20methods%20in%20natural%20language%20processing&amp;pages=560-567&amp;publication_year=2006&amp;author=Titov%2CI.&amp;author=Henderson%2CJ.">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="L. G.. Valiant, " /><meta itemprop="datePublished" content="1984" /><meta itemprop="headline" content="&#xA;Valiant, L. G. (1984). A theory of the learnable. Communications of the ACM, 27(11), 1134–1142.&#xA;" /><p class="c-article-references__text" id="ref-CR51">
Valiant, L. G. (1984). A theory of the learnable. <i>Communications of the ACM</i>, <i>27</i>(11), 1134–1142.
</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="http://www.emis.de/MATH-item?0587.68077" aria-label="View reference 51 on MATH">MATH</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="https://doi.org/10.1145%2F1968.1972" aria-label="View reference 51">Article</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 51 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20theory%20of%20the%20learnable&amp;journal=Communications%20of%20the%20ACM&amp;volume=27&amp;issue=11&amp;pages=1134-1142&amp;publication_year=1984&amp;author=Valiant%2CL.%20G.">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="E.. Wiewiora, " /><meta itemprop="datePublished" content="2005" /><meta itemprop="headline" content="&#xA;Wiewiora, E. (2005). Learning predictive representations from a history. In Proceedings of the 22nd internati" /><p class="c-article-references__text" id="ref-CR52">
Wiewiora, E. (2005). Learning predictive representations from a history. In <i>Proceedings of the 22nd international conference on machine learning</i> (pp. 964–971). New York: ACM.
</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 52 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Proceedings%20of%20the%2022nd%20international%20conference%20on%20machine%20learning&amp;pages=964-971&amp;publication_year=2005&amp;author=Wiewiora%2CE.">
                        Google Scholar</a></li></ul></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-category="article body" data-track-label="link" href="/article/10.1007/s10994-013-5416-x-references.ris">Download references<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p></div></div></div></section><section aria-labelledby="Ack1"><div class="c-article-section" id="Ack1-section"><h2 class="c-article-section__title u-h2 js-section-title js-c-reading-companion-sections-item" id="Ack1">Acknowledgements</h2><div class="c-article-section__content" id="Ack1-content"><p>We are grateful to the anonymous reviewers for providing us with helpful comments. This work was supported by a Google Research Award, and by projects XLike (FP7-288342), BASMATI (TIN2011-27479-C04-03), SGR-LARCA (SGR2009-1428), and by the EU PASCAL2 Network of Excellence (FP7-ICT-216886). Borja Balle was supported by an FPU fellowship (AP2008-02064) from the Spanish Ministry of Education. Xavier Carreras was supported by the Ramón y Cajal program of the Spanish Government (RYC-2008-02223). Franco M. Luque was supported by the National University of Córdoba and by a Postdoctoral fellowship of CONICET, Argentinian Ministry of Science, Technology and Productive Innovation. Ariadna Quattoni was supported by a Juan de la Cierva contract from the Spanish Government (JCI-2009-04240).</p></div></div></section><section aria-labelledby="author-information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title u-h2 js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><h3 class="c-article-author-information__subtitle u-h3" id="affiliations">Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><span class="c-article-author-affiliation__address u-h3">Universitat Politècnica de Catalunya, Barcelona, 08034, Spain</span><ul class="c-article-author-affiliation__authors-list"><li class="c-article-author-affiliation__authors-item">Borja Balle</li><li class="c-article-author-affiliation__authors-item">, Xavier Carreras</li><li class="c-article-author-affiliation__authors-item"> &amp; Ariadna Quattoni</li></ul></li><li id="Aff2"><span class="c-article-author-affiliation__address u-h3">Universidad Nacional de Córdoba and CONICET, X5000HUA, Córdoba, Argentina</span><ul class="c-article-author-affiliation__authors-list"><li class="c-article-author-affiliation__authors-item">Franco M. Luque</li></ul></li></ol><div class="js-hide u-hide-print" data-test="author-info"><span class="c-article-author-information__subtitle u-h3">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-1"><span class="c-article-authors-search__title u-h3 js-search-name">Borja Balle</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Borja+Balle&#34;" class="c-article-button" data-track="click" data-track-category="Article body" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><span class="search-in-title-js">You can also search for this author in </span><ul class="c-article-identifiers"><li class="c-article-identifiers__item"><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Borja+Balle" data-track="click" data-track-category="Article body" data-track-action="author link - pubmed" data-track-label="link">PubMed</a></li><li class="c-article-identifiers__item"><a href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Borja+Balle%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-category="Article body" data-track-action="author link - scholar" data-track-label="link">
                                Google Scholar
                            </a></li></ul></div></div></li><li id="auth-2"><span class="c-article-authors-search__title u-h3 js-search-name">Xavier Carreras</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Xavier+Carreras&#34;" class="c-article-button" data-track="click" data-track-category="Article body" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><span class="search-in-title-js">You can also search for this author in </span><ul class="c-article-identifiers"><li class="c-article-identifiers__item"><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Xavier+Carreras" data-track="click" data-track-category="Article body" data-track-action="author link - pubmed" data-track-label="link">PubMed</a></li><li class="c-article-identifiers__item"><a href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Xavier+Carreras%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-category="Article body" data-track-action="author link - scholar" data-track-label="link">
                                Google Scholar
                            </a></li></ul></div></div></li><li id="auth-3"><span class="c-article-authors-search__title u-h3 js-search-name">Franco M. Luque</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Franco M.+Luque&#34;" class="c-article-button" data-track="click" data-track-category="Article body" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><span class="search-in-title-js">You can also search for this author in </span><ul class="c-article-identifiers"><li class="c-article-identifiers__item"><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Franco M.+Luque" data-track="click" data-track-category="Article body" data-track-action="author link - pubmed" data-track-label="link">PubMed</a></li><li class="c-article-identifiers__item"><a href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Franco M.+Luque%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-category="Article body" data-track-action="author link - scholar" data-track-label="link">
                                Google Scholar
                            </a></li></ul></div></div></li><li id="auth-4"><span class="c-article-authors-search__title u-h3 js-search-name">Ariadna Quattoni</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Ariadna+Quattoni&#34;" class="c-article-button" data-track="click" data-track-category="Article body" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><span class="search-in-title-js">You can also search for this author in </span><ul class="c-article-identifiers"><li class="c-article-identifiers__item"><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Ariadna+Quattoni" data-track="click" data-track-category="Article body" data-track-action="author link - pubmed" data-track-label="link">PubMed</a></li><li class="c-article-identifiers__item"><a href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Ariadna+Quattoni%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-category="Article body" data-track-action="author link - scholar" data-track-label="link">
                                Google Scholar
                            </a></li></ul></div></div></li></ol></div><h3 class="c-article-author-information__subtitle u-h3" id="corresponding-author">Corresponding author</h3><p>Correspondence to
                <a id="corresp-c1" rel="nofollow" href="/article/10.1007/s10994-013-5416-x/email/correspondent/c1/new">Xavier Carreras</a>.</p></div></div></section><section aria-labelledby="additional-information"><div class="c-article-section" id="additional-information-section"><h2 class="c-article-section__title u-h2 js-section-title js-c-reading-companion-sections-item" id="additional-information">Additional information</h2><div class="c-article-section__content" id="additional-information-content"><p>Editors: Jeffrey Heinz, Colin de la Higuera, and Tim Oates.</p></div></div></section><section aria-labelledby="appendices"><div class="c-article-section" id="appendices-section"><h2 class="c-article-section__title u-h2 js-section-title js-c-reading-companion-sections-item" id="appendices">Appendix A: An inside-outside algorithm for non-deterministic SHAG</h2><div class="c-article-section__content" id="appendices-content"><h3 class="c-article__sub-heading u-h3 u-visually-hidden" id="App1">Appendix A: An inside-outside algorithm for non-deterministic SHAG</h3><p>In this section we sketch an algorithm to compute marginal probabilities of dependencies. Our algorithm is an adaptation of the parsing algorithm for SHAG by Eisner and Satta (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="&#xA;Eisner, J., &amp; Satta, G. (1999). Efficient parsing for bilexical context-free grammars and head-automaton grammars. In Proceedings of the 37th annual meeting of the Association for Computational Linguistics (ACL), University of Maryland (pp. 457–464).&#xA;" href="/article/10.1007/s10994-013-5416-x#ref-CR26" id="ref-link-section-d84034e7305">1999</a>) to the case of non-deterministic head-automata, and has a runtime cost of <i>O</i>(<i>n</i>
                           <sup>2</sup>
                           <i>N</i>
                           <sup>3</sup>), where <i>n</i> is the number of states of the model, and <i>N</i> is the length of the input sentence. Hence the algorithm maintains the standard cubic cost on the sentence length, while the quadratic cost on <i>n</i> is inherent to the computations defined by our model in Eq. (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10994-013-5416-x#Equ2">2</a>). The main insight behind our extension is that, because the computations of our model involve state-distribution vectors, we need to extend the standard inside/outside quantities to be in the form of such state-distribution quantities.<sup><a href="#Fn6"><span class="u-visually-hidden">Footnote </span>6</a></sup>
                        </p><p>Throughout this section we assume a fixed sentence <i>s</i>
                           <sub>0:<i>N</i>
                           </sub>. Let <span class="mathjax-tex">\(\mathcal{Y}(x_{i:j})\)</span> be the set of derivations that yield a subsequence <i>x</i>
                           <sub>
                              <i>i</i>:<i>j</i>
                           </sub>. For a derivation <i>y</i>, we use root(<i>y</i>) to indicate the root word of it, and use (<i>x</i>
                           <sub>
                    <i>i</i>
                  </sub>,<i>x</i>
                           <sub>
                    <i>j</i>
                  </sub>)∈<i>y</i> to refer a dependency in <i>y</i> from head <i>x</i>
                           <sub>
                    <i>i</i>
                  </sub> to modifier <i>x</i>
                           <sub>
                    <i>j</i>
                  </sub>. Following Eisner and Satta (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="&#xA;Eisner, J., &amp; Satta, G. (1999). Efficient parsing for bilexical context-free grammars and head-automaton grammars. In Proceedings of the 37th annual meeting of the Association for Computational Linguistics (ACL), University of Maryland (pp. 457–464).&#xA;" href="/article/10.1007/s10994-013-5416-x#ref-CR26" id="ref-link-section-d84034e7429">1999</a>), we use decoding structures related to complete half-constituents (or “triangles”, denoted <span class="u-small-caps">c</span>) and incomplete half-constituents (or “trapezoids”, denoted <span class="u-small-caps">i</span>), each decorated with a direction (denoted <span class="u-small-caps">l</span> and <span class="u-small-caps">r</span>). We assume familiarity with their algorithm.</p><p>We define <img src="//media.springernature.com/full/springer-static/image/art%3A10.1007%2Fs10994-013-5416-x/MediaObjects/10994_2013_5416_IEq179_HTML.gif" /> as the inside score-vector of a right trapezoid dominated by dependency (<i>s</i>
                           <sub>
                    <i>i</i>
                  </sub>,<i>s</i>
                           <sub>
                    <i>j</i>
                  </sub>), </p><div id="Equs" class="c-article-equation"><div class="c-article-equation__content"><img src="//media.springernature.com/full/springer-static/image/art%3A10.1007%2Fs10994-013-5416-x/MediaObjects/10994_2013_5416_Equs_HTML.gif" class="u-display-block" alt="" /></div></div><p> The term <span class="mathjax-tex">\(\mathbb{P}(y')\)</span> is the probability of head-modifier sequences in the range <i>s</i>
                           <sub>
                              <i>i</i>:<i>j</i>
                           </sub> that do not involve <i>s</i>
                           <sub>
                    <i>i</i>
                  </sub>. The term <img src="//media.springernature.com/full/springer-static/image/art%3A10.1007%2Fs10994-013-5416-x/MediaObjects/10994_2013_5416_IEq181_HTML.gif" /> is a <i>forward</i> state-distribution vector —the <i>q</i>th coordinate of the vector is the probability that <i>s</i>
                           <sub>
                    <i>i</i>
                  </sub> generates right modifiers <i>x</i>
                           <sub>1:<i>t</i>
                           </sub> and remains at state <i>q</i>. Similarly, we define <img src="//media.springernature.com/full/springer-static/image/art%3A10.1007%2Fs10994-013-5416-x/MediaObjects/10994_2013_5416_IEq182_HTML.gif" /> as the outside score-vector of a right trapezoid, as </p><div id="Equt" class="c-article-equation"><div class="c-article-equation__content"><img src="//media.springernature.com/full/springer-static/image/art%3A10.1007%2Fs10994-013-5416-x/MediaObjects/10994_2013_5416_Equt_HTML.gif" class="u-display-block" alt="" /></div></div><p> where <img src="//media.springernature.com/full/springer-static/image/art%3A10.1007%2Fs10994-013-5416-x/MediaObjects/10994_2013_5416_IEq183_HTML.gif" /> is a <i>backward</i> state-distribution vector—the <i>q</i>th coordinate is the probability of being at state <i>q</i> of the right automaton of <i>s</i>
                           <sub>
                    <i>i</i>
                  </sub> and generating <i>x</i>
                           <sub>
                              <i>t</i>+1:<i>T</i>
                           </sub>. Analogous inside-outside expressions can be defined for the rest of structures (left/right triangles and trapezoids). With these quantities, we can compute marginals as </p><div id="Equu" class="c-article-equation"><div class="c-article-equation__content"><img src="//media.springernature.com/full/springer-static/image/art%3A10.1007%2Fs10994-013-5416-x/MediaObjects/10994_2013_5416_Equu_HTML.gif" class="u-display-block" alt="" /></div></div><p> where <img src="//media.springernature.com/full/springer-static/image/art%3A10.1007%2Fs10994-013-5416-x/MediaObjects/10994_2013_5416_IEq184_HTML.gif" />.</p><p>Finally, we sketch the equations for computing inside and outside scores in <i>O</i>(<i>N</i>
                           <sup>3</sup>) time. The inside computations are, for 0≤<i>i</i>&lt;<i>j</i>≤<i>N</i>: </p><div id="Equ3" class="c-article-equation"><div class="c-article-equation__content"><img src="//media.springernature.com/full/springer-static/image/art%3A10.1007%2Fs10994-013-5416-x/MediaObjects/10994_2013_5416_Equ3_HTML.gif" class="u-display-block" alt="" /></div><div class="c-article-equation__number">
                    (3)
                </div></div>
                           <div id="Equ4" class="c-article-equation"><div class="c-article-equation__content"><img src="//media.springernature.com/full/springer-static/image/art%3A10.1007%2Fs10994-013-5416-x/MediaObjects/10994_2013_5416_Equ4_HTML.gif" class="u-display-block" alt="" /></div><div class="c-article-equation__number">
                    (4)
                </div></div>
                           <div id="Equ5" class="c-article-equation"><div class="c-article-equation__content"><img src="//media.springernature.com/full/springer-static/image/art%3A10.1007%2Fs10994-013-5416-x/MediaObjects/10994_2013_5416_Equ5_HTML.gif" class="u-display-block" alt="" /></div><div class="c-article-equation__number">
                    (5)
                </div></div>
                        <p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10994-013-5416-x#Fig13">13</a> illustrates these equations. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10994-013-5416-x#Fig13">13</a>(a) corresponds to the basic case of Eq. <a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10994-013-5416-x#Equ3">3</a>, and Figs. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10994-013-5416-x#Fig13">13</a>(b) and <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10994-013-5416-x#Fig13">13</a>(c) correspond respectively to Eqs. (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10994-013-5416-x#Equ4">4</a>) and (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10994-013-5416-x#Equ5">5</a>) for a fixed <i>k</i>. </p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-13"><figure><figcaption><b id="Fig13" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 13</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-category="article body" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10994-013-5416-x/figures/13" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10994-013-5416-x/MediaObjects/10994_2013_5416_Fig13_HTML.gif?as=webp"></source><img aria-describedby="figure-13-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10994-013-5416-x/MediaObjects/10994_2013_5416_Fig13_HTML.gif" alt="figure13" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-13-desc"><p>Graphical depiction of the inside scores computations for the right half-constituents. Computations for left half-constituents are symmetrical. (<b>a</b>) Empty right half-constituent (“triangle”). (<b>b</b>) Non-empty complete right half-constituent (“triangle”). (<b>c</b>) Incomplete right half-constituent (“trapezoid”)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-category="article body" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10994-013-5416-x/figures/13" data-track-dest="link:Figure13 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>The outside computations are: </p><div id="Equ6" class="c-article-equation"><div class="c-article-equation__content"><img src="//media.springernature.com/full/springer-static/image/art%3A10.1007%2Fs10994-013-5416-x/MediaObjects/10994_2013_5416_Equ6_HTML.gif" class="u-display-block" alt="" /></div><div class="c-article-equation__number">
                    (6)
                </div></div>
                           <div id="Equ7" class="c-article-equation"><div class="c-article-equation__content"><img src="//media.springernature.com/full/springer-static/image/art%3A10.1007%2Fs10994-013-5416-x/MediaObjects/10994_2013_5416_Equ7_HTML.gif" class="u-display-block" alt="" /></div><div class="c-article-equation__number">
                    (7)
                </div></div>
                           <div id="Equ8" class="c-article-equation"><div class="c-article-equation__content"><img src="//media.springernature.com/full/springer-static/image/art%3A10.1007%2Fs10994-013-5416-x/MediaObjects/10994_2013_5416_Equ8_HTML.gif" class="u-display-block" alt="" /></div><div class="c-article-equation__number">
                    (8)
                </div></div>
                        <p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10994-013-5416-x#Fig14">14</a> illustrates these equations. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10994-013-5416-x#Fig14">14</a>(a) corresponds to the basic case of Eq. <a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10994-013-5416-x#Equ6">6</a>. Figures <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10994-013-5416-x#Fig14">14</a>(b), (c) and (d) correspond to the three members of Eq. <a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10994-013-5416-x#Equ7">7</a>. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10994-013-5416-x#Fig14">14</a>(e) corresponds to Eq. <a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10994-013-5416-x#Equ8">8</a>. </p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-14"><figure><figcaption><b id="Fig14" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 14</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-category="article body" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10994-013-5416-x/figures/14" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10994-013-5416-x/MediaObjects/10994_2013_5416_Fig14_HTML.gif?as=webp"></source><img aria-describedby="figure-14-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10994-013-5416-x/MediaObjects/10994_2013_5416_Fig14_HTML.gif" alt="figure14" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-14-desc"><p>Graphical depiction of the outside scores computations for the right half-constituents. Computations for left half-constituents are symmetrical. The dotted shapes correspond to outsides and the solid shapes correspond to insides</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-category="article body" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10994-013-5416-x/figures/14" data-track-dest="link:Figure14 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        </div></div></section><section aria-labelledby="rightslink"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title u-h2 js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content"><p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-category="article body" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=Spectral%20learning%20of%20weighted%20automata&amp;author=Borja%20Balle%20et%20al&amp;contentID=10.1007%2Fs10994-013-5416-x&amp;publication=0885-6125&amp;publicationDate=2013-10-03&amp;publisherName=SpringerNature&amp;orderBeanReset=true">Reprints and Permissions</a></p></div></div></section><section aria-labelledby="article-info"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title u-h2 js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Balle, B., Carreras, X., Luque, F.M. <i>et al.</i> Spectral learning of weighted automata.
                    <i>Mach Learn</i> <b>96, </b>33–63 (2014). https://doi.org/10.1007/s10994-013-5416-x</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-category="article body" data-track-label="link" href="/article/10.1007/s10994-013-5416-x.ris">Download citation<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Received<span class="u-hide">: </span><span class="u-clearfix c-bibliographic-information__value"><time datetime="2012-12-08">08 December 2012</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Accepted<span class="u-hide">: </span><span class="u-clearfix c-bibliographic-information__value"><time datetime="2013-09-03">03 September 2013</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="u-clearfix c-bibliographic-information__value"><time datetime="2013-10-03">03 October 2013</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Issue Date<span class="u-hide">: </span><span class="u-clearfix c-bibliographic-information__value"><time datetime="2014-07">July 2014</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--doi"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="u-clearfix c-bibliographic-information__value"><a href="https://doi.org/10.1007/s10994-013-5416-x" data-track="click" data-track-action="view doi" data-track-category="article body" data-track-label="link" itemprop="sameAs">https://doi.org/10.1007/s10994-013-5416-x</a></span></p></li></ul><div data-component="share-box"></div><h3 class="c-article__sub-heading u-h3">Keywords</h3><ul class="c-article-subject-list"><li class="c-article-subject-list__subject"><span itemprop="about">Spectral learning</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Weighted finite automata</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Dependency parsing</span></li></ul><div data-component="article-info-list"></div></div></div></div></div></section>
                </div>
            </article>
        </main>

        <div class="c-article-extras u-text-sm u-hide-print" id="sidebar" data-container-type="reading-companion" data-track-component="reading companion">
            <aside>
                <div data-test="download-article-link-wrapper">
                    <div id="pdflink-container" class="download-article test-pdf-link">
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10994-013-5416-x.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-category="article body" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    
</div>

                </div>

                <div data-test="collections">
                    
                </div>

                <div data-test="editorial-summary">
                    
                </div>

                <div class="c-reading-companion">
                    <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky">
                        

                        <div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections">
                            <div class="js-ad">
    <div class="c-ad c-ad--MPU1">
        <div class="c-ad c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-MPU1" data-gpt-unitpath="/270604982/springerlink/10994/article" data-gpt-sizes="300x250" data-gpt-targeting="pos=MPU1;articleid=5416;"></div>
        </div>
    </div>
</div>

                        </div>
                        <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures"></div>
                        <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references"></div>
                    </div>
                </div>
            </aside>
        </div>
    </div>


        
    <footer class="app-footer" role="contentinfo">
        <div class="app-footer__aside-wrapper u-hide-print">
            <div class="app-footer__container">
                <p class="app-footer__strapline">Over 10 million scientific documents at your fingertips</p>
                
                    <div class="app-footer__edition" data-component="SV.EditionSwitcher">
                        <span class="u-visually-hidden" data-role="button-dropdown__title" data-btn-text="Switch between Academic & Corporate Edition">Switch Edition</span>
                        <ul class="app-footer-edition-list" data-role="button-dropdown__content" data-test="footer-edition-switcher-list">
                            <li class="selected">
                                <a data-test="footer-academic-link"
                                   href="/siteEdition/link"
                                   id="siteedition-academic-link">Academic Edition</a>
                            </li>
                            <li>
                                <a data-test="footer-corporate-link"
                                   href="/siteEdition/rd"
                                   id="siteedition-corporate-link">Corporate Edition</a>
                            </li>
                        </ul>
                    </div>
                
            </div>
        </div>
        <div class="app-footer__container">
            <ul class="app-footer__nav u-hide-print">
                <li><a href="/">Home</a></li>
                <li><a href="/impressum">Impressum</a></li>
                <li><a href="/termsandconditions">Legal information</a></li>
                <li><a href="/privacystatement">Privacy statement</a></li>
                <li><a href="/cookiepolicy">How we use cookies</a></li>
                <li><a href="/accessibility">Accessibility</a></li>
                <li><a id="contactus-footer-link" href="/contactus">Contact us</a></li>
            </ul>
            <div class="c-user-metadata">
    
        <p class="c-user-metadata__item">
            <span data-test="footer-user-login-status">Not logged in</span>
            <span data-test="footer-user-ip"> - 185.128.27.147</span>
        </p>
        <p class="c-user-metadata__item" data-test="footer-business-partners">
            Not affiliated
        </p>

        
    
</div>

            <a class="app-footer__parent-logo" target="_blank" rel="noopener" href="//www.springernature.com"  title="Go to Springer Nature">
                <span class="u-visually-hidden">Springer Nature</span>
                <svg width="125" height="12" focusable="false" aria-hidden="true">
                    <image width="125" height="12" alt="Springer Nature logo"
                           src=/oscar-static/images/springerlink/png/springernature-60a72a849b.png
                           xmlns:xlink="http://www.w3.org/1999/xlink"
                           xlink:href=/oscar-static/images/springerlink/svg/springernature-ecf01c77dd.svg>
                    </image>
                </svg>
            </a>
            <p class="app-footer__copyright">&copy; 2020 Springer Nature Switzerland AG. Part of <a target="_blank" rel="noopener" href="//www.springernature.com">Springer Nature</a>.</p>
            
        </div>
        
    <svg class="u-hide hide">
        <symbol id="global-icon-chevron-right" viewBox="0 0 16 16">
            <path d="M7.782 7L5.3 4.518c-.393-.392-.4-1.022-.02-1.403a1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L7.782 9l1.013-.998z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-download" viewBox="0 0 16 16">
            <path d="M2 14c0-.556.449-1 1.002-1h9.996a.999.999 0 110 2H3.002A1.006 1.006 0 012 14zM9 2v6.8l2.482-2.482c.392-.392 1.022-.4 1.403-.02a1.001 1.001 0 010 1.417l-4.177 4.177a1.001 1.001 0 01-1.416 0L3.115 7.715a.991.991 0 01-.016-1.4 1 1 0 011.42.003L7 8.8V2c0-.55.444-.996 1-.996.552 0 1 .445 1 .996z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-email" viewBox="0 0 18 18">
            <path d="M1.995 2h14.01A2 2 0 0118 4.006v9.988A2 2 0 0116.005 16H1.995A2 2 0 010 13.994V4.006A2 2 0 011.995 2zM1 13.994A1 1 0 001.995 15h14.01A1 1 0 0017 13.994V4.006A1 1 0 0016.005 3H1.995A1 1 0 001 4.006zM9 11L2 7V5.557l7 4 7-4V7z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-institution" viewBox="0 0 18 18">
            <path d="M14 8a1 1 0 011 1v6h1.5a.5.5 0 01.5.5v.5h.5a.5.5 0 01.5.5V18H0v-1.5a.5.5 0 01.5-.5H1v-.5a.5.5 0 01.5-.5H3V9a1 1 0 112 0v6h8V9a1 1 0 011-1zM6 8l2 1v4l-2 1zm6 0v6l-2-1V9zM9.573.401l7.036 4.925A.92.92 0 0116.081 7H1.92a.92.92 0 01-.528-1.674L8.427.401a1 1 0 011.146 0zM9 2.441L5.345 5h7.31z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-search" viewBox="0 0 14 14">
            <path d="M13.545 12.648a.641.641 0 01.006.903.646.646 0 01-.903-.006l-2.664-2.663a6.125 6.125 0 11.897-.898l2.664 2.664zm-7.42-1.273a5.25 5.25 0 100-10.5 5.25 5.25 0 000 10.5z"></path>
        </symbol>
    </svg>

    </footer>



    </div>
    
    
</body>
</html>

