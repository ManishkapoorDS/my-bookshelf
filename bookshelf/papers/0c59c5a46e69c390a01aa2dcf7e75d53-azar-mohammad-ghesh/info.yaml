abstract: We consider the problem of learning the optimal action-value function in
  the discounted-reward Markov decision processes (MDPs). We prove a new PAC bound
  on the sample-complexity of model-based value iteration algorithm in the presence
  of the generative model, which indicates that for an MDP with N state-action pairs
  and the discount factor \gamma\in[0,1) only O(N\log(N/\delta)/((1-\gamma)^3\epsilon^2))
  samples are required to find an \epsilon-optimal estimation of the action-value
  function with the probability 1-\delta. We also prove a matching lower bound of
  \Theta (N\log(N/\delta)/((1-\gamma)^3\epsilon^2)) on the sample complexity of estimating
  the optimal action-value function by every RL algorithm. To the best of our knowledge,
  this is the first matching result on the sample complexity of estimating the optimal
  (action-) value function in which the upper bound matches the lower bound of RL
  in terms of N, \epsilon, \delta and 1/(1-\gamma). Also, both our lower bound and
  our upper bound significantly improve on the state-of-the-art in terms of 1/(1-\gamma).
archiveprefix: arXiv
author: Azar, Mohammad Gheshlaghi and Munos, Remi and Kappen, Bert
author_list:
- family: Azar
  given: Mohammad Gheshlaghi
- family: Munos
  given: Remi
- family: Kappen
  given: Bert
eprint: 1206.6461v1
file: 1206.6461v1.pdf
files:
- azar-mohammad-gheshlaghi-and-munos-remi-and-kappen-berton-the-sample-complexity-of-reinforcement-learning-with-a-generative-model2012.pdf
month: Jun
primaryclass: cs.LG
ref: 1206.6461v1
time-added: 2020-10-23-15:28:21
title: On the Sample Complexity of Reinforcement Learning with a Generative   Model
type: article
url: http://arxiv.org/abs/1206.6461v1
year: '2012'
