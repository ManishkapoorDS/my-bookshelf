abstract: We propose a plan online and learn offline (POLO) framework for the setting
  where an agent, with an internal model, needs to continually act and learn in the
  world. Our work builds on the synergistic relationship between local model-based
  control, global value function learning, and exploration. We study how local trajectory
  optimization can cope with approximation errors in the value function, and can stabilize
  and accelerate value function learning. Conversely, we also study how approximate
  value functions can help reduce the planning horizon and allow for better policies
  beyond local solutions. Finally, we also demonstrate how trajectory optimization
  can be used to perform temporally coordinated exploration in conjunction with estimating
  uncertainty in value function approximation. This exploration is critical for fast
  and stable learning of the value function. Combining these components enable solutions
  to complex simulated control tasks, like humanoid locomotion and dexterous in-hand
  manipulation, in the equivalent of a few minutes of experience in the real world.
archiveprefix: arXiv
author: Lowrey, Kendall and Rajeswaran, Aravind and Kakade, Sham and Todorov, Emanuel
  and Mordatch, Igor
author_list:
- family: Lowrey
  given: Kendall
- family: Rajeswaran
  given: Aravind
- family: Kakade
  given: Sham
- family: Todorov
  given: Emanuel
- family: Mordatch
  given: Igor
eprint: 1811.01848v3
file: 1811.01848v3.pdf
files:
- lowrey-kendall-and-rajeswaran-aravind-and-kakade-sham-and-todorov-emanuel-and-mordatch-igorplan-online-learn-offline-efficient-learning-and-exp.pdf
month: Nov
primaryclass: cs.LG
ref: 1811.01848v3
time-added: 2020-05-26-22:08:27
title: 'Plan Online, Learn Offline: Efficient Learning and Exploration via   Model-Based
  Control'
type: article
url: http://arxiv.org/abs/1811.01848v3
year: '2018'
