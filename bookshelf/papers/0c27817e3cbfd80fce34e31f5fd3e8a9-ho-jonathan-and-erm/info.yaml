abstract: Consider learning a policy from example expert behavior, without interaction
  with the expert or access to reinforcement signal. One approach is to recover the
  expert's cost function with inverse reinforcement learning, then extract a policy
  from that cost function with reinforcement learning. This approach is indirect and
  can be slow. We propose a new general framework for directly extracting a policy
  from data, as if it were obtained by reinforcement learning following inverse reinforcement
  learning. We show that a certain instantiation of our framework draws an analogy
  between imitation learning and generative adversarial networks, from which we derive
  a model-free imitation learning algorithm that obtains significant performance gains
  over existing model-free methods in imitating complex behaviors in large, high-dimensional
  environments.
archiveprefix: arXiv
author: Ho, Jonathan and Ermon, Stefano
author_list:
- family: Ho
  given: Jonathan
- family: Ermon
  given: Stefano
eprint: 1606.03476v1
file: 1606.03476v1.pdf
files:
- ho-jonathan-and-ermon-stefanogenerative-adversarial-imitation-learning2016.pdf
month: Jun
primaryclass: cs.LG
ref: 1606.03476v1
time-added: 2020-05-17-12:55:46
title: Generative Adversarial Imitation Learning
type: article
url: http://arxiv.org/abs/1606.03476v1
year: '2016'
