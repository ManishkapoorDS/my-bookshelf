abstract: Neural network based methods have obtained great progress on a variety of
  natural language processing tasks. However, in most previous works, the models are
  learned based on single-task supervised objectives, which often suffer from insufficient
  training data. In this paper, we use the multi-task learning framework to jointly
  learn across multiple related tasks. Based on recurrent neural network, we propose
  three different mechanisms of sharing information to model text with task-specific
  and shared layers. The entire network is trained jointly on all these tasks. Experiments
  on four benchmark text classification tasks show that our proposed models can improve
  the performance of a task with the help of other related tasks.
archiveprefix: arXiv
author: Liu, Pengfei and Qiu, Xipeng and Huang, Xuanjing
author_list:
- family: Liu
  given: Pengfei
- family: Qiu
  given: Xipeng
- family: Huang
  given: Xuanjing
eprint: 1605.05101v1
file: 1605.05101v1.pdf
files:
- liu-pengfei-and-qiu-xipeng-and-huang-xuanjingrecurrent-neural-network-for-text-classification-with-multi-task-learning2016.pdf
month: May
primaryclass: cs.CL
ref: 1605.05101v1
title: Recurrent Neural Network for Text Classification with Multi-Task   Learning
type: article
url: http://arxiv.org/abs/1605.05101v1
year: '2016'
