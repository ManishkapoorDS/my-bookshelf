abstract: We study the complexity of functions computable by deep feedforward neural
  networks with piecewise linear activations in terms of the symmetries and the number
  of linear regions that they have. Deep networks are able to sequentially map portions
  of each layer's input-space to the same output. In this way, deep models compute
  functions that react equally to complicated patterns of different inputs. The compositional
  structure of these functions enables them to re-use pieces of computation exponentially
  often in terms of the network's depth. This paper investigates the complexity of
  such compositional maps and contributes new theoretical results regarding the advantage
  of depth for neural networks with piecewise linear activation functions. In particular,
  our analysis is not specific to a single family of models, and as an example, we
  employ it for rectifier and maxout networks. We improve complexity bounds from pre-existing
  work and investigate the behavior of units in higher layers.
archiveprefix: arXiv
author: Montúfar, Guido and Pascanu, Razvan and Cho, Kyunghyun and Bengio, Yoshua
author_list:
- family: Montúfar
  given: Guido
- family: Pascanu
  given: Razvan
- family: Cho
  given: Kyunghyun
- family: Bengio
  given: Yoshua
eprint: 1402.1869v2
file: 1402.1869v2.pdf
files:
- montufar-guido-and-pascanu-razvan-and-cho-kyunghyun-and-bengio-yoshuaon-the-number-of-linear-regions-of-deep-neural-networks2014.pdf
month: Feb
primaryclass: stat.ML
ref: 1402.1869v2
title: On the Number of Linear Regions of Deep Neural Networks
type: article
url: http://arxiv.org/abs/1402.1869v2
year: '2014'
