abstract: Neural Tangents is a library designed to enable research into infinite-width
  neural networks. It provides a high-level API for specifying complex and hierarchical
  neural network architectures. These networks can then be trained and evaluated either
  at finite-width as usual or in their infinite-width limit. Infinite-width networks
  can be trained analytically using exact Bayesian inference or using gradient descent
  via the Neural Tangent Kernel. Additionally, Neural Tangents provides tools to study
  gradient descent training dynamics of wide but finite networks in either function
  space or weight space.   The entire library runs out-of-the-box on CPU, GPU, or
  TPU. All computations can be automatically distributed over multiple accelerators
  with near-linear scaling in the number of devices. Neural Tangents is available
  at www.github.com/google/neural-tangents. We also provide an accompanying interactive
  Colab notebook.
archiveprefix: arXiv
author: Novak, Roman and Xiao, Lechao and Hron, Jiri and Lee, Jaehoon and Alemi, Alexander
  A. and Sohl-Dickstein, Jascha and Schoenholz, Samuel S.
author_list:
- family: Novak
  given: Roman
- family: Xiao
  given: Lechao
- family: Hron
  given: Jiri
- family: Lee
  given: Jaehoon
- family: Alemi
  given: Alexander A.
- family: Sohl-Dickstein
  given: Jascha
- family: Schoenholz
  given: Samuel S.
eprint: 1912.02803v1
file: 1912.02803v1.pdf
files:
- novak-roman-and-xiao-lechao-and-hron-jiri-and-lee-jaehoon-and-alemi-alexander-a.-and-sohl-dickstein-jascha-and-schoenholz-samuel-s.neural-tange.pdf
month: Dec
primaryclass: stat.ML
ref: 1912.02803v1
time-added: 2020-06-21-00:14:13
title: 'Neural Tangents: Fast and Easy Infinite Neural Networks in Python'
type: article
url: http://arxiv.org/abs/1912.02803v1
year: '2019'
