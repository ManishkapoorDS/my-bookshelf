abstract: 'Though deep reinforcement learning has led to breakthroughs in many difficult
  domains, these successes have required an ever-increasing number of samples. As
  state-of-the-art reinforcement learning (RL) systems require an exponentially increasing
  number of samples, their development is restricted to a continually shrinking segment
  of the AI community. Likewise, many of these systems cannot be applied to real-world
  problems, where environment samples are expensive. Resolution of these limitations
  requires new, sample-efficient methods. To facilitate research in this direction,
  we introduce the MineRL Competition on Sample Efficient Reinforcement Learning using
  Human Priors.   The primary goal of the competition is to foster the development
  of algorithms which can efficiently leverage human demonstrations to drastically
  reduce the number of samples needed to solve complex, hierarchical, and sparse environments.
  To that end, we introduce: (1) the Minecraft ObtainDiamond task, a sequential decision
  making environment requiring long-term planning, hierarchical control, and efficient
  exploration methods; and (2) the MineRL-v0 dataset, a large-scale collection of
  over 60 million state-action pairs of human demonstrations that can be resimulated
  into embodied trajectories with arbitrary modifications to game state and visuals.   Participants
  will compete to develop systems which solve the ObtainDiamond task with a limited
  number of samples from the environment simulator, Malmo. The competition is structured
  into two rounds in which competitors are provided several paired versions of the
  dataset and environment with different game textures. At the end of each round,
  competitors will submit containerized versions of their learning algorithms and
  they will then be trained/evaluated from scratch on a hold-out dataset-environment
  pair for a total of 4-days on a prespecified hardware platform.'
archiveprefix: arXiv
author: Guss, William H. and Codel, Cayden and Hofmann, Katja and Houghton, Brandon
  and Kuno, Noboru and Milani, Stephanie and Mohanty, Sharada and Liebana, Diego Perez
  and Salakhutdinov, Ruslan and Topin, Nicholay and Veloso, Manuela and Wang, Phillip
author_list:
- family: Guss
  given: William H.
- family: Codel
  given: Cayden
- family: Hofmann
  given: Katja
- family: Houghton
  given: Brandon
- family: Kuno
  given: Noboru
- family: Milani
  given: Stephanie
- family: Mohanty
  given: Sharada
- family: Liebana
  given: Diego Perez
- family: Salakhutdinov
  given: Ruslan
- family: Topin
  given: Nicholay
- family: Veloso
  given: Manuela
- family: Wang
  given: Phillip
eprint: 1904.10079v2
file: 1904.10079v2.pdf
files:
- guss-william-h.-and-codel-cayden-and-hofmann-katja-and-houghton-brandon-and-kuno-noboru-and-milani-stephanie-and-mohanty-sharada-and-liebana-d.pdf
month: Apr
primaryclass: cs.LG
ref: 1904.10079v2
time-added: 2020-09-28-19:34:08
title: The MineRL Competition on Sample Efficient Reinforcement Learning using   Human
  Priors
type: article
url: http://arxiv.org/abs/1904.10079v2
year: '2019'
