abstract: This paper addresses the general problem of reinforcement learning (RL)
  in partially observable environments. In 2013, our large RL recurrent neural networks
  (RNNs) learned from scratch to drive simulated cars from high-dimensional video
  input. However, real brains are more powerful in many ways. In particular, they
  learn a predictive model of their initially unknown environment, and somehow use
  it for abstract (e.g., hierarchical) planning and reasoning. Guided by algorithmic
  information theory, we describe RNN-based AIs (RNNAIs) designed to do the same.
  Such an RNNAI can be trained on never-ending sequences of tasks, some of them provided
  by the user, others invented by the RNNAI itself in a curious, playful fashion,
  to improve its RNN-based world model. Unlike our previous model-building RNN-based
  RL machines dating back to 1990, the RNNAI learns to actively query its model for
  abstract reasoning and planning and decision making, essentially "learning to think."
  The basic ideas of this report can be applied to many other cases where one RNN-like
  system exploits the algorithmic information content of another. They are taken from
  a grant proposal submitted in Fall 2014, and also explain concepts such as "mirror
  neurons." Experimental results will be described in separate papers.
archiveprefix: arXiv
author: Schmidhuber, Juergen
author_list:
- family: Schmidhuber
  given: Juergen
eprint: 1511.09249v1
file: 1511.09249v1.pdf
files:
- schmidhuber-juergenon-learning-to-think-algorithmic-information-theory-for-novel-combinations-of-reinforcement-learning-controllers-and-recurrent.pdf
month: Nov
primaryclass: cs.AI
ref: 1511.09249v1
title: 'On Learning to Think: Algorithmic Information Theory for Novel   Combinations
  of Reinforcement Learning Controllers and Recurrent Neural World   Models'
type: article
url: http://arxiv.org/abs/1511.09249v1
year: '2015'
