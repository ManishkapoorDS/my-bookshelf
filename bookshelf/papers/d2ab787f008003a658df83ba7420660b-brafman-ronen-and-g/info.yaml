abstract: In Markov Decision Processes (MDPs), the reward obtained in a state is Markovian,
  i.e., depends on the last state and action. This dependency makes it difficult to
  reward more interesting long-term behaviors, such as always closing a door after
  it has been opened, or providing coffee only following a request. Extending MDPs
  to handle non-Markovian reward functions was the subject of two previous lines of
  work. Both use LTL variants to specify the reward function and then compile the
  new model back into a Markovian model. Building on recent progress in temporal logics
  over finite traces, we adopt LDLf for specifying non-Markovian rewards and provide
  an elegant automata construction for building a Markovian model, which extends that
  of previous work and offers strong minimality and compositionality guarantees.
author: Brafman, Ronen and Giacomo, Giuseppe De and Patrizi, Fabio
author_list:
- family: Brafman
  given: Ronen
- family: Giacomo
  given: Giuseppe De
- family: Patrizi
  given: Fabio
conference: AAAI Conference on Artificial Intelligence
files:
- ldlf-non-markovian-rewards2018.pdf
keywords: MDPs; non-Markovian Rewards; LTLf/LDLf
ref: AAAI1817342
title: LTLf/LDLf Non-Markovian Rewards
type: paper
url: https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17342
year: '2018'
