abstract: We present a unified framework for learning continuous control policies
  using backpropagation. It supports stochastic control by treating stochasticity
  in the Bellman equation as a deterministic function of exogenous noise. The product
  is a spectrum of general policy gradient algorithms that range from model-free methods
  with value functions to model-based methods without value functions. We use learned
  models but only require observations from the environment in- stead of observations
  from model-predicted trajectories, minimizing the impact of compounded model errors.
  We apply these algorithms first to a toy stochastic control problem and then to
  several physics-based control problems in simulation. One of these variants, SVG(1),
  shows the effectiveness of learning models, value functions, and policies simultaneously
  in continuous domains.
archiveprefix: arXiv
author: Heess, Nicolas and Wayne, Greg and Silver, David and Lillicrap, Timothy and
  Tassa, Yuval and Erez, Tom
author_list:
- family: Heess
  given: Nicolas
- family: Wayne
  given: Greg
- family: Silver
  given: David
- family: Lillicrap
  given: Timothy
- family: Tassa
  given: Yuval
- family: Erez
  given: Tom
eprint: 1510.09142v1
file: 1510.09142v1.pdf
files:
- heess-nicolas-and-wayne-greg-and-silver-david-and-lillicrap-timothy-and-tassa-yuval-and-erez-tomlearning-continuous-control-policies-by-stochast.pdf
month: Oct
primaryclass: cs.LG
ref: 1510.09142v1
title: Learning Continuous Control Policies by Stochastic Value Gradients
type: article
url: http://arxiv.org/abs/1510.09142v1
year: '2015'
