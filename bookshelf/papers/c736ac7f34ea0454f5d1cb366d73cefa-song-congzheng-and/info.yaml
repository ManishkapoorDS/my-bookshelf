abstract: Machine learning (ML) is becoming a commodity. Numerous ML frameworks and
  services are available to data holders who are not ML experts but want to train
  predictive models on their data. It is important that ML models trained on sensitive
  inputs (e.g., personal images or documents) not leak too much information about
  the training data.   We consider a malicious ML provider who supplies model-training
  code to the data holder, does not observe the training, but then obtains white-
  or black-box access to the resulting model. In this setting, we design and implement
  practical algorithms, some of them very similar to standard ML techniques such as
  regularization and data augmentation, that "memorize" information about the training
  dataset in the model yet the model is as accurate and predictive as a conventionally
  trained model. We then explain how the adversary can extract memorized information
  from the model.   We evaluate our techniques on standard ML tasks for image classification
  (CIFAR10), face recognition (LFW and FaceScrub), and text analysis (20 Newsgroups
  and IMDB). In all cases, we show how our algorithms create models that have high
  predictive power yet allow accurate extraction of subsets of their training data.
archiveprefix: arXiv
author: Song, Congzheng and Ristenpart, Thomas and Shmatikov, Vitaly
author_list:
- family: Song
  given: Congzheng
- family: Ristenpart
  given: Thomas
- family: Shmatikov
  given: Vitaly
eprint: 1709.07886v1
file: 1709.07886v1.pdf
files:
- song-congzheng-and-ristenpart-thomas-and-shmatikov-vitalymachine-learning-models-that-remember-too-much2017.pdf
month: Sep
primaryclass: cs.CR
ref: 1709.07886v1
time-added: 2020-05-22-16:14:30
title: Machine Learning Models that Remember Too Much
type: article
url: http://arxiv.org/abs/1709.07886v1
year: '2017'
