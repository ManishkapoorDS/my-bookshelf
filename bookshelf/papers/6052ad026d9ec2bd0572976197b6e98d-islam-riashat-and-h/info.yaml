abstract: 'Policy gradient methods in reinforcement learning have become increasingly
  prevalent for state-of-the-art performance in continuous control tasks. Novel methods
  typically benchmark against a few key algorithms such as deep deterministic policy
  gradients and trust region policy optimization. As such, it is important to present
  and use consistent baselines experiments. However, this can be difficult due to
  general variance in the algorithms, hyper-parameter tuning, and environment stochasticity.
  We investigate and discuss: the significance of hyper-parameters in policy gradients
  for continuous control, general variance in the algorithms, and reproducibility
  of reported results. We provide guidelines on reporting novel results as comparisons
  against baseline methods such that future researchers can make informed decisions
  when investigating novel methods.'
archiveprefix: arXiv
author: Islam, Riashat and Henderson, Peter and Gomrokchi, Maziar and Precup, Doina
author_list:
- family: Islam
  given: Riashat
- family: Henderson
  given: Peter
- family: Gomrokchi
  given: Maziar
- family: Precup
  given: Doina
eprint: 1708.04133v1
file: 1708.04133v1.pdf
files:
- islam-riashat-and-henderson-peter-and-gomrokchi-maziar-and-precup-doinareproducibility-of-benchmarked-deep-reinforcement-learning-tasks-for-cont.pdf
month: Aug
primaryclass: cs.LG
ref: 1708.04133v1
title: Reproducibility of Benchmarked Deep Reinforcement Learning Tasks for   Continuous
  Control
type: article
url: http://arxiv.org/abs/1708.04133v1
year: '2017'
