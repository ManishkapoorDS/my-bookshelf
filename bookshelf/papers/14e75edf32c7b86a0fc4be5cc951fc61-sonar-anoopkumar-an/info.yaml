abstract: 'A fundamental challenge in reinforcement learning is to learn policies
  that generalize beyond the operating domain experienced during training. In this
  paper, we approach this challenge through the following invariance principle: an
  agent must find a representation such that there exists an action-predictor built
  on top of this representation that is simultaneously optimal across all training
  domains. Intuitively, the resulting invariant policy enhances generalization by
  finding causes of successful actions. We propose a novel learning algorithm, Invariant
  Policy Optimization (IPO), that explicitly enforces this principle and learns an
  invariant policy during training. We compare our approach with standard policy gradient
  methods and demonstrate significant improvements in generalization performance on
  unseen domains for Linear Quadratic Regulator (LQR) problems and our own benchmark
  in the MiniGrid Gym environment.'
archiveprefix: arXiv
author: Sonar, Anoopkumar and Pacelli, Vincent and Majumdar, Anirudha
author_list:
- family: Sonar
  given: Anoopkumar
- family: Pacelli
  given: Vincent
- family: Majumdar
  given: Anirudha
eprint: 2006.01096v1
file: 2006.01096v1.pdf
files:
- sonar-anoopkumar-and-pacelli-vincent-and-majumdar-anirudhainvariant-policy-optimization-towards-stronger-generalization-in-reinforcement-learnin.pdf
month: Jun
primaryclass: cs.LG
ref: 2006.01096v1
time-added: 2020-06-22-12:07:55
title: 'Invariant Policy Optimization: Towards Stronger Generalization in   Reinforcement
  Learning'
type: article
url: http://arxiv.org/abs/2006.01096v1
year: '2020'
