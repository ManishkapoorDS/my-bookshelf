abstract: We consider learning two layer neural networks using stochastic gradient
  descent. The mean-field description of this learning dynamics approximates the evolution
  of the network weights by an evolution in the space of probability distributions
  in $R^D$ (where $D$ is the number of parameters associated to each neuron). This
  evolution can be defined through a partial differential equation or, equivalently,
  as the gradient flow in the Wasserstein space of probability distributions. Earlier
  work shows that (under some regularity assumptions), the mean field description
  is accurate as soon as the number of hidden units is much larger than the dimension
  $D$. In this paper we establish stronger and more general approximation guarantees.
  First of all, we show that the number of hidden units only needs to be larger than
  a quantity dependent on the regularity properties of the data, and independent of
  the dimensions. Next, we generalize this analysis to the case of unbounded activation
  functions, which was not covered by earlier bounds. We extend our results to noisy
  stochastic gradient descent.   Finally, we show that kernel ridge regression can
  be recovered as a special limit of the mean field analysis.
archiveprefix: arXiv
author: Mei, Song and Misiakiewicz, Theodor and Montanari, Andrea
author_list:
- family: Mei
  given: Song
- family: Misiakiewicz
  given: Theodor
- family: Montanari
  given: Andrea
eprint: 1902.06015v1
file: 1902.06015v1.pdf
files:
- mei-song-and-misiakiewicz-theodor-and-montanari-andreamean-field-theory-of-two-layers-neural-networks-dimension-free-bounds-and-kernel-limit2019.pdf
month: Feb
primaryclass: stat.ML
ref: 1902.06015v1
time-added: 2020-06-21-12:52:48
title: 'Mean-field theory of two-layers neural networks: dimension-free bounds   and
  kernel limit'
type: article
url: http://arxiv.org/abs/1902.06015v1
year: '2019'
