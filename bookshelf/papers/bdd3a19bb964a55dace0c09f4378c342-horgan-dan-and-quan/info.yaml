abstract: 'We propose a distributed architecture for deep reinforcement learning at
  scale, that enables agents to learn effectively from orders of magnitude more data
  than previously possible. The algorithm decouples acting from learning: the actors
  interact with their own instances of the environment by selecting actions according
  to a shared neural network, and accumulate the resulting experience in a shared
  experience replay memory; the learner replays samples of experience and updates
  the neural network. The architecture relies on prioritized experience replay to
  focus only on the most significant data generated by the actors. Our architecture
  substantially improves the state of the art on the Arcade Learning Environment,
  achieving better final performance in a fraction of the wall-clock training time.'
archiveprefix: arXiv
author: Horgan, Dan and Quan, John and Budden, David and Barth-Maron, Gabriel and
  Hessel, Matteo and van Hasselt, Hado and Silver, David
author_list:
- family: Horgan
  given: Dan
- family: Quan
  given: John
- family: Budden
  given: David
- family: Barth-Maron
  given: Gabriel
- family: Hessel
  given: Matteo
- family: van Hasselt
  given: Hado
- family: Silver
  given: David
eprint: 1803.00933v1
file: 1803.00933v1.pdf
files:
- horgan-dan-and-quan-john-and-budden-david-and-barth-maron-gabriel-and-hessel-matteo-and-van-hasselt-hado-and-silver-daviddistributed-prioritize.pdf
month: Mar
primaryclass: cs.LG
ref: 1803.00933v1
title: Distributed Prioritized Experience Replay
type: article
url: http://arxiv.org/abs/1803.00933v1
year: '2018'
