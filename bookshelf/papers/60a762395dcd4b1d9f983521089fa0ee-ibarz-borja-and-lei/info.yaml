abstract: 'To solve complex real-world problems with reinforcement learning, we cannot
  rely on manually specified reward functions. Instead, we can have humans communicate
  an objective to the agent directly. In this work, we combine two approaches to learning
  from human feedback: expert demonstrations and trajectory preferences. We train
  a deep neural network to model the reward function and use its predicted reward
  to train an DQN-based deep reinforcement learning agent on 9 Atari games. Our approach
  beats the imitation learning baseline in 7 games and achieves strictly superhuman
  performance on 2 games without using game rewards. Additionally, we investigate
  the goodness of fit of the reward model, present some reward hacking problems, and
  study the effects of noise in the human labels.'
archiveprefix: arXiv
author: Ibarz, Borja and Leike, Jan and Pohlen, Tobias and Irving, Geoffrey and Legg,
  Shane and Amodei, Dario
author_list:
- family: Ibarz
  given: Borja
- family: Leike
  given: Jan
- family: Pohlen
  given: Tobias
- family: Irving
  given: Geoffrey
- family: Legg
  given: Shane
- family: Amodei
  given: Dario
eprint: 1811.06521v1
file: 1811.06521v1.pdf
files:
- ibarz-borja-and-leike-jan-and-pohlen-tobias-and-irving-geoffrey-and-legg-shane-and-amodei-darioreward-learning-from-human-preferences-and-demons.pdf
month: Nov
primaryclass: cs.LG
ref: 1811.06521v1
time-added: 2020-06-10-11:36:20
title: Reward learning from human preferences and demonstrations in Atari
type: article
url: http://arxiv.org/abs/1811.06521v1
year: '2018'
