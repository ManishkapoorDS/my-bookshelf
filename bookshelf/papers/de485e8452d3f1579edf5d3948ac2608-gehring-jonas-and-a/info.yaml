abstract: The prevalent approach to sequence to sequence learning maps an input sequence
  to a variable length output sequence via recurrent neural networks. We introduce
  an architecture based entirely on convolutional neural networks. Compared to recurrent
  models, computations over all elements can be fully parallelized during training
  and optimization is easier since the number of non-linearities is fixed and independent
  of the input length. Our use of gated linear units eases gradient propagation and
  we equip each decoder layer with a separate attention module. We outperform the
  accuracy of the deep LSTM setup of Wu et al. (2016) on both WMT'14 English-German
  and WMT'14 English-French translation at an order of magnitude faster speed, both
  on GPU and CPU.
archiveprefix: arXiv
author: Gehring, Jonas and Auli, Michael and Grangier, David and Yarats, Denis and
  Dauphin, Yann N.
author_list:
- family: Gehring
  given: Jonas
- family: Auli
  given: Michael
- family: Grangier
  given: David
- family: Yarats
  given: Denis
- family: Dauphin
  given: Yann N.
eprint: 1705.03122v3
file: 1705.03122v3.pdf
files:
- gehring-jonas-and-auli-michael-and-grangier-david-and-yarats-denis-and-dauphin-yann-n.convolutional-sequence-to-sequence-learning2017.pdf
month: May
primaryclass: cs.CL
ref: 1705.03122v3
title: Convolutional Sequence to Sequence Learning
type: article
url: http://arxiv.org/abs/1705.03122v3
year: '2017'
