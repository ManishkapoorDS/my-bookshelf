abstract: Training state-of-the-art, deep neural networks is computationally expensive.
  One way to reduce the training time is to normalize the activities of the neurons.
  A recently introduced technique called batch normalization uses the distribution
  of the summed input to a neuron over a mini-batch of training cases to compute a
  mean and variance which are then used to normalize the summed input to that neuron
  on each training case. This significantly reduces the training time in feed-forward
  neural networks. However, the effect of batch normalization is dependent on the
  mini-batch size and it is not obvious how to apply it to recurrent neural networks.
  In this paper, we transpose batch normalization into layer normalization by computing
  the mean and variance used for normalization from all of the summed inputs to the
  neurons in a layer on a single training case. Like batch normalization, we also
  give each neuron its own adaptive bias and gain which are applied after the normalization
  but before the non-linearity. Unlike batch normalization, layer normalization performs
  exactly the same computation at training and test times. It is also straightforward
  to apply to recurrent neural networks by computing the normalization statistics
  separately at each time step. Layer normalization is very effective at stabilizing
  the hidden state dynamics in recurrent networks. Empirically, we show that layer
  normalization can substantially reduce the training time compared with previously
  published techniques.
archiveprefix: arXiv
author: Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E.
author_list:
- family: Ba
  given: Jimmy Lei
- family: Kiros
  given: Jamie Ryan
- family: Hinton
  given: Geoffrey E.
eprint: 1607.06450v1
file: 1607.06450v1.pdf
files:
- ba-jimmy-lei-and-kiros-jamie-ryan-and-hinton-geoffrey-e.layer-normalization2016.pdf
month: Jul
primaryclass: stat.ML
ref: 1607.06450v1
time-added: 2020-06-05-21:33:41
title: Layer Normalization
type: article
url: http://arxiv.org/abs/1607.06450v1
year: '2016'
