<!DOCTYPE html>
<html lang="en" class="no-js">
<head>
    <meta charset="UTF-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <title>A survey of machine learning for big data processing | SpringerLink</title>

    <link rel="shortcut icon" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
    <link rel="icon" sizes="16x16 32x32 48x48" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
    <link rel="icon" sizes="16x16" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-16x16-8bd8c1c945.png />
    <link rel="icon" sizes="32x32" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-32x32-61a52d80ab.png />
    <link rel="icon" sizes="48x48" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-48x48-0ec46b6b10.png />
    <link rel="apple-touch-icon" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />
    <link rel="apple-touch-icon" sizes="72x72" href=/oscar-static/images/favicons/springerlink/ic_launcher_hdpi-f77cda7f65.png />
    <link rel="apple-touch-icon" sizes="76x76" href=/oscar-static/images/favicons/springerlink/app-icon-ipad-c3fd26520d.png />
    <link rel="apple-touch-icon" sizes="114x114" href=/oscar-static/images/favicons/springerlink/app-icon-114x114-3d7d4cf9f3.png />
    <link rel="apple-touch-icon" sizes="120x120" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@2x-67b35150b3.png />
    <link rel="apple-touch-icon" sizes="144x144" href=/oscar-static/images/favicons/springerlink/ic_launcher_xxhdpi-986442de7b.png />
    <link rel="apple-touch-icon" sizes="152x152" href=/oscar-static/images/favicons/springerlink/app-icon-ipad@2x-677ba24d04.png />
    <link rel="apple-touch-icon" sizes="180x180" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />

    <link rel="stylesheet" href=/oscar-static/app-springerlink/core-article-f837ea88d2.css media="screen">
    <link rel="stylesheet" id="js-mustard" href=/oscar-static/app-springerlink/enhanced-article-9120b115e5.css media="only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)">

    

    <meta name="citation_abstract" content="There is no doubt that big data are now rapidly expanding in all science and engineering domains. While the potential of these massive data is undoubtedly significant, fully making sense of them requires new ways of thinking and novel learning techniques to address the various challenges. In this paper, we present a literature survey of the latest advances in researches on machine learning for big data processing. First, we review the machine learning techniques and highlight some promising learning methods in recent studies, such as representation learning, deep learning, distributed and parallel learning, transfer learning, active learning, and kernel-based learning. Next, we focus on the analysis and discussions about the challenges and possible solutions of machine learning for big data. Following that, we investigate the close connections of machine learning with signal processing techniques for big data processing. Finally, we outline several open issues and research trends."/>

    <meta name="journal_id" content="13634"/>

    <meta name="dc.title" content="A survey of machine learning for big data processing"/>

    <meta name="dc.source" content="EURASIP Journal on Advances in Signal Processing 2016 2016:1"/>

    <meta name="dc.format" content="text/html"/>

    <meta name="dc.publisher" content="SpringerOpen"/>

    <meta name="dc.date" content="2016-05-28"/>

    <meta name="dc.type" content="ReviewPaper"/>

    <meta name="dc.language" content="En"/>

    <meta name="dc.copyright" content="2016 Qiu et al."/>

    <meta name="dc.rightsAgent" content="reprints@biomedcentral.com"/>

    <meta name="dc.description" content="There is no doubt that big data are now rapidly expanding in all science and engineering domains. While the potential of these massive data is undoubtedly significant, fully making sense of them requires new ways of thinking and novel learning techniques to address the various challenges. In this paper, we present a literature survey of the latest advances in researches on machine learning for big data processing. First, we review the machine learning techniques and highlight some promising learning methods in recent studies, such as representation learning, deep learning, distributed and parallel learning, transfer learning, active learning, and kernel-based learning. Next, we focus on the analysis and discussions about the challenges and possible solutions of machine learning for big data. Following that, we investigate the close connections of machine learning with signal processing techniques for big data processing. Finally, we outline several open issues and research trends."/>

    <meta name="prism.issn" content="1687-6180"/>

    <meta name="prism.publicationName" content="EURASIP Journal on Advances in Signal Processing"/>

    <meta name="prism.publicationDate" content="2016-05-28"/>

    <meta name="prism.volume" content="2016"/>

    <meta name="prism.number" content="1"/>

    <meta name="prism.section" content="ReviewPaper"/>

    <meta name="prism.startingPage" content="1"/>

    <meta name="prism.endingPage" content="16"/>

    <meta name="prism.copyright" content="2016 Qiu et al."/>

    <meta name="prism.rightsAgent" content="reprints@biomedcentral.com"/>

    <meta name="prism.url" content="https://link.springer.com/articles/10.1186/s13634-016-0355-x"/>

    <meta name="prism.doi" content="doi:10.1186/s13634-016-0355-x"/>

    <meta name="citation_pdf_url" content="https://link.springer.com/track/pdf/10.1186/s13634-016-0355-x"/>

    <meta name="citation_fulltext_html_url" content="https://link.springer.com/articles/10.1186/s13634-016-0355-x"/>

    <meta name="citation_journal_title" content="EURASIP Journal on Advances in Signal Processing"/>

    <meta name="citation_journal_abbrev" content="EURASIP J. Adv. Signal Process. "/>

    <meta name="citation_publisher" content="SpringerOpen"/>

    <meta name="citation_issn" content="1687-6180"/>

    <meta name="citation_title" content="A survey of machine learning for big data processing"/>

    <meta name="citation_volume" content="2016"/>

    <meta name="citation_issue" content="1"/>

    <meta name="citation_publication_date" content="2016/12"/>

    <meta name="citation_online_date" content="2016/05/28"/>

    <meta name="citation_firstpage" content="1"/>

    <meta name="citation_lastpage" content="16"/>

    <meta name="citation_article_type" content="Review"/>

    <meta name="citation_fulltext_world_readable" content=""/>

    <meta name="citation_language" content="en"/>

    <meta name="dc.identifier" content="doi:10.1186/s13634-016-0355-x"/>

    <meta name="DOI" content="10.1186/s13634-016-0355-x"/>

    <meta name="citation_doi" content="10.1186/s13634-016-0355-x"/>

    <meta name="description" content="There is no doubt that big data are now rapidly expanding in all science and engineering domains. While the potential of these massive data is undoubtedly significant, fully making sense of them requires new ways of thinking and novel learning techniques to address the various challenges. In this paper, we present a literature survey of the latest advances in researches on machine learning for big data processing. First, we review the machine learning techniques and highlight some promising learning methods in recent studies, such as representation learning, deep learning, distributed and parallel learning, transfer learning, active learning, and kernel-based learning. Next, we focus on the analysis and discussions about the challenges and possible solutions of machine learning for big data. Following that, we investigate the close connections of machine learning with signal processing techniques for big data processing. Finally, we outline several open issues and research trends."/>

    <meta name="dc.creator" content="Junfei Qiu"/>

    <meta name="dc.creator" content="Qihui Wu"/>

    <meta name="dc.creator" content="Guoru Ding"/>

    <meta name="dc.creator" content="Yuhua Xu"/>

    <meta name="dc.creator" content="Shuo Feng"/>

    <meta name="dc.subject" content="Signal,Image and Speech Processing"/>

    <meta name="dc.subject" content="Quantum Information Technology, Spintronics"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Signal Proc Mag; citation_title=Big data analysis with signal processing on graphs: representation and processing of massive data sets with irregular structure; citation_author=A Sandryhaila, JMF Moura; citation_volume=31; citation_issue=5; citation_publication_date=2014; citation_pages=80-90; citation_doi=10.1109/MSP.2014.2329213; citation_id=CR1"/>

    <meta name="citation_reference" content="citation_title=Extracting value from chaos; citation_publication_date=2011; citation_id=CR2; citation_author=J Gantz; citation_author=D Reinsel; citation_publisher=EMC"/>

    <meta name="citation_reference" content="citation_title=The digital universe decade&#8212;are you ready; citation_publication_date=2010; citation_id=CR3; citation_author=J Gantz; citation_author=D Reinsel; citation_publisher=EMC"/>

    <meta name="citation_reference" content="D Che, M Safran, Z Peng, From big data to big data mining: challenges, issues, and opportunities, in Proceedings of the 18th International Conference on DASFAA (Wuhan, 2013), pp. 1&#8211;15"/>

    <meta name="citation_reference" content="citation_journal_title=Mobile Netw Appl; citation_title=Big data: a survey; citation_author=M Chen, S Mao, Y Liu; citation_volume=19; citation_issue=2; citation_publication_date=2014; citation_pages=171-209; citation_doi=10.1007/s11036-013-0489-0; citation_id=CR5"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Access; citation_title=Toward scalable systems for big data analytics: a technology tutorial; citation_author=H Hu, Y Wen, T Chua, X Li; citation_volume=2; citation_publication_date=2014; citation_pages=652-687; citation_doi=10.1109/ACCESS.2014.2332453; citation_id=CR6"/>

    <meta name="citation_reference" content="citation_title=Big data: the next frontier for innovation, competition, and productivity; citation_publication_date=2011; citation_id=CR7; citation_author=J Manyika; citation_author=M Chui; citation_author=B Brown; citation_author=J Bughin; citation_author=R Dobbs; citation_author=C Roxburgh; citation_author=AH Byers; citation_publisher=McKinsey Global Institute"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Internet Things J; citation_title=Cognitive internet of things: a new paradigm beyond connection; citation_author=Q Wu, G Ding, Y Xu, S Feng, Z Du, J Wang, K Long; citation_volume=1; citation_issue=2; citation_publication_date=2014; citation_pages=129-143; citation_doi=10.1109/JIOT.2014.2311513; citation_id=CR8"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Commun Surv Tut; citation_title=Data mining for internet of things: a survey; citation_author=CW Tsai, CF Lai, MC Chiang, LT Yang; citation_volume=16; citation_issue=1; citation_publication_date=2014; citation_pages=77-97; citation_doi=10.1109/SURV.2013.103013.00206; citation_id=CR9"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Netw; citation_title=Challenges in 5G: how to empower SON with big data for enabling 5G; citation_author=A Imran, A Zoha; citation_volume=28; citation_issue=6; citation_publication_date=2014; citation_pages=27-33; citation_doi=10.1109/MNET.2014.6963801; citation_id=CR10"/>

    <meta name="citation_reference" content="X Wu, X Zhu, G Wu, W Ding, Data mining with big data. IEEE Trans Knowl Data Eng 26(1), 97&#8211;107 (2014)"/>

    <meta name="citation_reference" content="citation_title=Mining of massive data sets; citation_publication_date=2011; citation_id=CR12; citation_author=A Rajaraman; citation_author=JD Ullman; citation_publisher=Cambridge University Press"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Access; citation_title=Big data deep learning: challenges and perspectives; citation_author=XW Chen, X Lin; citation_volume=2; citation_publication_date=2014; citation_pages=514-525; citation_doi=10.1109/ACCESS.2014.2325029; citation_id=CR13"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Signal Proc Mag; citation_title=Modeling and optimization for big data analytics: (statistical) learning tools for our era of data deluge; citation_author=K Slavakis, GB Giannakis, G Mateos; citation_volume=31; citation_issue=5; citation_publication_date=2014; citation_pages=18-31; citation_doi=10.1109/MSP.2014.2327238; citation_id=CR14"/>

    <meta name="citation_reference" content="citation_title=Machine learning; citation_publication_date=1997; citation_id=CR15; citation_author=TM Mitchell; citation_publisher=McGraw-Hill"/>

    <meta name="citation_reference" content="citation_title=Artificial intelligence: a modern approach; citation_publication_date=1995; citation_id=CR16; citation_author=S Russell; citation_author=P Norvig; citation_publisher=Prentice-Hall"/>

    <meta name="citation_reference" content="citation_title=Learning from data: concepts, theory, and methods; citation_publication_date=2007; citation_id=CR17; citation_author=V Cherkassky; citation_author=FM Mulier; citation_publisher=John Wiley &amp; Sons"/>

    <meta name="citation_reference" content="TM Mitchell, The discipline of machine learning (Carnegie Mellon University, School of Computer Science, Machine Learning Department, 2006)"/>

    <meta name="citation_reference" content="citation_journal_title=Mach Learn; citation_title=Machine learning for science and society; citation_author=C Rudin, KL Wagstaff; citation_volume=95; citation_issue=1; citation_publication_date=2014; citation_pages=1-9; citation_doi=10.1007/s10994-013-5425-9; citation_id=CR19"/>

    <meta name="citation_reference" content="citation_title=Pattern recognition and machine learning; citation_publication_date=2006; citation_id=CR20; citation_author=CM Bishop; citation_publisher=Springer"/>

    <meta name="citation_reference" content="citation_journal_title=J Comput Civil Eng; citation_title=Reinforcement learning for structural control; citation_author=B Adam, IFC Smith, F Asce; citation_volume=22; citation_issue=2; citation_publication_date=2008; citation_pages=133-139; citation_doi=10.1061/(ASCE)0887-3801(2008)22:2(133); citation_id=CR21"/>

    <meta name="citation_reference" content="citation_journal_title=Nature; citation_title=Computer science: the learning machines; citation_author=N Jones; citation_volume=505; citation_issue=7482; citation_publication_date=2014; citation_pages=146-148; citation_doi=10.1038/505146a; citation_id=CR22"/>

    <meta name="citation_reference" content="citation_journal_title=J Mach Learn Res; citation_title=Tutorial on practical prediction theory for classification; citation_author=J Langford; citation_volume=6; citation_issue=3; citation_publication_date=2005; citation_pages=273-306; citation_id=CR23"/>

    <meta name="citation_reference" content="citation_journal_title=J Mach Learn Res; citation_title=Distributional word clusters vs. words for text categorization; citation_author=R Bekkerman, EY Ran, N Tishby, Y Winter; citation_volume=3; citation_publication_date=2003; citation_pages=1183-1208; citation_id=CR24"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Pattern Anal; citation_title=Representation learning: a review and new perspectives; citation_author=Y Bengio, A Courville, P Vincent; citation_volume=35; citation_issue=8; citation_publication_date=2012; citation_pages=1798-1828; citation_doi=10.1109/TPAMI.2013.50; citation_id=CR25"/>

    <meta name="citation_reference" content="F Huang, E Yates, Biased representation learning for domain adaptation, in Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (Jeju Island, 2012), pp. 1313&#8211;1323"/>

    <meta name="citation_reference" content="W Tu, S Sun, Cross-domain representation-learning framework with combination of class-separate and domain-merge objectives, in Proceedings of the 1st International Workshop on Cross Domain Knowledge Discovery in Web and Social Network Mining (Beijing, 2012), pp. 18&#8211;25"/>

    <meta name="citation_reference" content="citation_journal_title=J Comput Sci Technol; citation_title=Multi-domain sentiment classification with classifier combination; citation_author=S Li, C Huang, C Zong; citation_volume=26; citation_issue=1; citation_publication_date=2011; citation_pages=25-33; citation_doi=10.1007/s11390-011-9412-y; citation_id=CR28"/>

    <meta name="citation_reference" content="F Huang, E Yates, Exploring representation-learning approaches to domain adaptation, in Proceedings of the 2010 Workshop on Domain Adaptation for Natural Language Processing (Uppsala, 2010), pp. 23&#8211;30"/>

    <meta name="citation_reference" content="A Bordes, X Glorot, JWAY Bengio, Joint learning of words and meaning representations for open-text semantic parsing, in Proceedings of 15th International Conference on Artificial Intelligence and Statistics (La Palma, 2012), pp. 127&#8211;135"/>

    <meta name="citation_reference" content="N. Boulanger-Lewandowski, Y. Bengio, P. Vincent, Modeling temporal dependencies in high-dimensional sequences: application to polyphonic music generation and transcription. arXiv preprint (2012). arXiv:1206.6392"/>

    <meta name="citation_reference" content="K Dwivedi, K Biswaranjan, A Sethi, Drowsy driver detection using representation learning, in Proceedings of the IEEE International Advance Computing Conference (Gurgaon, 2014), pp. 995&#8211;999"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Signal Proc Mag; citation_title=Deep learning and its applications to signal and information processing; citation_author=D Yu, L Deng; citation_volume=28; citation_issue=1; citation_publication_date=2011; citation_pages=145-154; citation_doi=10.1109/MSP.2010.939038; citation_id=CR33"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Comput Intell Mag; citation_title=Deep machine learning-a new frontier in artificial intelligence research; citation_author=I Arel, DC Rose, TP Karnowski; citation_volume=5; citation_issue=4; citation_publication_date=2010; citation_pages=13-18; citation_doi=10.1109/MCI.2010.938364; citation_id=CR34"/>

    <meta name="citation_reference" content="citation_journal_title=Foundations Trends Mach Learn; citation_title=Learning deep architectures for AI; citation_author=Y Bengio; citation_volume=2; citation_issue=1; citation_publication_date=2009; citation_pages=1-127; citation_doi=10.1561/2200000006; citation_id=CR35"/>

    <meta name="citation_reference" content="citation_journal_title=J Mach Learn Res; citation_title=Natural language processing (almost) from scratch; citation_author=R Collobert, J Weston, L Bottou, M Karlen, K Kavukcuoglu, P Kuksa; citation_volume=12; citation_publication_date=2011; citation_pages=2493-2537; citation_id=CR36"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Neural Networ; citation_title=A convolutional neural network approach for objective video quality assessment; citation_author=P Callet, C Viard-Gaudin, D Barba; citation_volume=17; citation_issue=5; citation_publication_date=2006; citation_pages=1316-1327; citation_doi=10.1109/TNN.2006.879766; citation_id=CR37"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Audio Speech Lang Proc; citation_title=Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition; citation_author=GE Dahl, D Yu, L Deng, A Acero; citation_volume=20; citation_issue=1; citation_publication_date=2012; citation_pages=30-42; citation_doi=10.1109/TASL.2011.2134090; citation_id=CR38"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Signal Proc Mag; citation_title=Deep neural networks for acoustic modeling in speech recognition: the shared views of four research groups; citation_author=G Hinton, L Deng, Y Dong, GE Dahl, A Mohamed, N Jaitly, A Senior, V Vanhoucke, P Nguyen, TN Sainath, B Kingsbury; citation_volume=29; citation_issue=6; citation_publication_date=2012; citation_pages=82-97; citation_doi=10.1109/MSP.2012.2205597; citation_id=CR39"/>

    <meta name="citation_reference" content="citation_journal_title=Neural Comput; citation_title=Deep, big, simple neural nets for handwritten digit recognition; citation_author=DC Ciresan, U Meier, LM Gambardella, J Schmidhuber; citation_volume=22; citation_issue=12; citation_publication_date=2010; citation_pages=3207-3220; citation_doi=10.1162/NECO_a_00052; citation_id=CR40"/>

    <meta name="citation_reference" content="citation_title=Voice search; citation_inbook_title=Language understanding: systems for extracting semantic information from speech; citation_publication_date=2011; citation_id=CR41; citation_author=Y Wang; citation_author=D Yu; citation_author=Y Ju; citation_author=A Acero; citation_publisher=Wiley"/>

    <meta name="citation_reference" content="citation_journal_title=Progress in Artificial Intelligence; citation_title=A survey of methods for distributed machine learning; citation_author=D Peteiro-Barral, B Guijarro-Berdi&#241;as; citation_volume=2; citation_issue=1; citation_publication_date=2012; citation_pages=1-11; citation_doi=10.1007/s13748-012-0035-5; citation_id=CR42"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Signal Process; citation_title=Attribute-distributed learning: models, limits, and algorithms; citation_author=H Zheng, SR Kulkarni, HV Poor; citation_volume=59; citation_issue=1; citation_publication_date=2011; citation_pages=386-398; citation_doi=10.1109/TSP.2010.2088393; citation_id=CR43"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Knowl Data Eng; citation_title=A rough set-based method for updating decision rules on attribute values&#8217; coarsening and refining; citation_author=H Chen, T Li, C Luo, SJ Horng, G Wang; citation_volume=26; citation_issue=12; citation_publication_date=2014; citation_pages=2886-2899; citation_doi=10.1109/TKDE.2014.2320740; citation_id=CR44"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Geosci Remote; citation_title=Using stacked generalization to combine SVMs in magnitude and shape feature spaces for classification of hyperspectral data; citation_author=J Chen, C Wang, R Wang; citation_volume=47; citation_issue=7; citation_publication_date=2009; citation_pages=2193-2205; citation_doi=10.1109/TGRS.2008.2010491; citation_id=CR45"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Knowl Data Eng; citation_title=A set of complexity measures designed for applying meta-learning to instance selection; citation_author=E Leyva, A Gonz&#225;lez, R P&#233;rez; citation_volume=27; citation_issue=2; citation_publication_date=2014; citation_pages=354-367; citation_doi=10.1109/TKDE.2014.2327034; citation_id=CR46"/>

    <meta name="citation_reference" content="M Sarnovsky, M Vronc, Distributed boosting algorithm for classification of text documents, in Proceedings of the 12th IEEE International Symposium on Applied Machine Intelligence and Informatics (SAMI) (Herl&#39;any, 2014), pp. 217&#8211;220"/>

    <meta name="citation_reference" content="citation_journal_title=J Parallel Distr Com; citation_title=Parallel approaches to machine learning&#8212;a comprehensive survey; citation_author=SR Upadhyaya; citation_volume=73; citation_issue=3; citation_publication_date=2013; citation_pages=284-292; citation_doi=10.1016/j.jpdc.2012.11.001; citation_id=CR48"/>

    <meta name="citation_reference" content="citation_title=Scaling up machine learning: parallel and distributed approaches; citation_publication_date=2011; citation_id=CR49; citation_author=R Bekkerman; citation_author=M Bilenko; citation_author=J Langford; citation_publisher=Cambridge University Press"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Knowl Data Eng; citation_title=Bridging domains using world wide knowledge for transfer learning; citation_author=EW Xiang, B Cao, DH Hu, Q Yang; citation_volume=22; citation_issue=6; citation_publication_date=2010; citation_pages=770-783; citation_doi=10.1109/TKDE.2010.31; citation_id=CR50"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Knowl Data Eng; citation_title=A survey on transfer learning; citation_author=SJ Pan, Q Yang; citation_volume=22; citation_issue=10; citation_publication_date=2010; citation_pages=1345-1359; citation_doi=10.1109/TKDE.2009.191; citation_id=CR51"/>

    <meta name="citation_reference" content="W Fan, I Davidson, B Zadrozny, PS Yu, An improved categorization of classifier&#8217;s sensitivity on sample selection bias, in Proceedings of the 5th IEEE International Conference on Data Mining (ICDM) (Brussels, 2012), pp. 605&#8211;608"/>

    <meta name="citation_reference" content="J Gao, W Fan, J Jiang, J Han, Knowledge transfer via multiple model local structure mapping, in Proceedings of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (Las Vegas, 2008), pp. 283-291"/>

    <meta name="citation_reference" content="C Wang, S Mahadevan, Manifold alignment using procrustes analysis, in Proceedings of the 25th International Conference on Machine Learning (ICML) (Helsinki, 2008), pp. 1120&#8211;1127"/>

    <meta name="citation_reference" content="X Ling, W Dai, GR Xue, Q Yang, Y Yu, Spectral domain-transfer learning, in Proceedings of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (Las Vegas, 2008), pp. 488&#8211;496"/>

    <meta name="citation_reference" content="R Raina, AY Ng, D Koller, 2006, Constructing informative priors using transfer learning, in Proceedings of the 23rd International Conference on Machine Learning (ICML) (Pittsburgh, 2006), pp. 713&#8211;720"/>

    <meta name="citation_reference" content="&#8201;J Zhang, Deep transfer learning via restricted Boltzmann machine for document classification, in Proceedings of the 10th International Conference on Machine Learning and Applications and Workshops (ICMLA) (Honolulu, 2011), pp. 323&#8211;326"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Knowl Data Eng; citation_title=Active learning without knowing individual instance labels: a pairwise label homogeneity query approach; citation_author=Y Fu, B Li, X Zhu, C Zhang; citation_volume=26; citation_issue=4; citation_publication_date=2014; citation_pages=808-822; citation_doi=10.1109/TKDE.2013.165; citation_id=CR58"/>

    <meta name="citation_reference" content="citation_title=Active learning literature survey; citation_publication_date=2010; citation_id=CR59; citation_author=B Settles; citation_publisher=University of Wisconsin"/>

    <meta name="citation_reference" content="citation_journal_title=P IEEE; citation_title=Active learning: any value for classification of remotely sensed data?; citation_author=MM Crawford, D Tuia, HL Yang; citation_volume=101; citation_issue=3; citation_publication_date=2013; citation_pages=593-608; citation_doi=10.1109/JPROC.2012.2231951; citation_id=CR60"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE ACM Trans Comput Bi; citation_title=Generalized query-based active learning to identify differentially methylated regions in DNA; citation_author=MM Haque, LB Holder, MK Skinner, DJ Cook; citation_volume=10; citation_issue=3; citation_publication_date=2013; citation_pages=632-644; citation_id=CR61"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE J Sel Top Sign Proces; citation_title=A survey of active learning algorithms for supervised remote sensing image classification; citation_author=D Tuia, M Volpi, L Copa, M Kanevski, J Munoz-Mari; citation_volume=5; citation_issue=3; citation_publication_date=2011; citation_pages=606-617; citation_doi=10.1109/JSTSP.2011.2139193; citation_id=CR62"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Signal Proc Mag; citation_title=Kernel-based learning for statistical signal processing in cognitive radio networks; citation_author=G Ding, Q Wu, YD Yao, J Wang, Y Chen; citation_volume=30; citation_issue=4; citation_publication_date=2013; citation_pages=126-136; citation_doi=10.1109/MSP.2013.2251071; citation_id=CR63"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Neur Net Lear Syst; citation_title=A unifying framework for typical multitask multiple kernel learning problems; citation_author=C Li, M Georgiopoulos, GC Anagnostopoulos; citation_volume=25; citation_issue=7; citation_publication_date=2014; citation_pages=1287-1297; citation_doi=10.1109/TNNLS.2013.2291772; citation_id=CR64"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Signal Proc Mag; citation_title=Analyzing local structure in kernel-based learning: explanation, complexity, and reliability assessment; citation_author=G Montavon, M Braun, T Krueger, KR Muller; citation_volume=30; citation_issue=4; citation_publication_date=2013; citation_pages=62-74; citation_doi=10.1109/MSP.2013.2249294; citation_id=CR65"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Signal Process; citation_title=Online kernel-based classification using adaptive projection algorithms; citation_author=K Slavakis, S Theodoridis, I Yamada; citation_volume=56; citation_issue=7; citation_publication_date=2008; citation_pages=2781-2796; citation_doi=10.1109/TSP.2008.917376; citation_id=CR66"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Signal Proc Mag; citation_title=Adaptive learning in a world of projections; citation_author=S Theodoridis, K Slavakis, I Yamada; citation_volume=28; citation_issue=1; citation_publication_date=2011; citation_pages=97-123; citation_doi=10.1109/MSP.2010.938752; citation_id=CR67"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Signal Process; citation_title=Adaptive constrained learning in reproducing kernel Hilbert spaces: the robust beamforming case; citation_author=K Slavakis, S Theodoridis, I Yamada; citation_volume=57; citation_issue=12; citation_publication_date=2009; citation_pages=4744-4764; citation_doi=10.1109/TSP.2009.2027771; citation_id=CR68"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Neural Netw Learn Syst; citation_title=Adaptive multiregression in reproducing kernel Hilbert spaces: the multiaccess MIMO channel case; citation_author=K Slavakis, P Bouboulis, S Theodoridis; citation_volume=23; citation_issue=2; citation_publication_date=2012; citation_pages=260-276; citation_doi=10.1109/TNNLS.2011.2178321; citation_id=CR69"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Neural Networ; citation_title=An introduction to kernel-based learning algorithms; citation_author=KR M&#252;ller, S Mika, G R&#228;tsch, K Tsuda, B Sch&#246;lkopf; citation_volume=12; citation_issue=2; citation_publication_date=2001; citation_pages=181-201; citation_doi=10.1109/72.914517; citation_id=CR70"/>

    <meta name="citation_reference" content="citation_journal_title=MIT Sloan Manage Rev; citation_title=How &#8220;big data&#8221; is different; citation_author=TH Davenport, P Barth, R Bean; citation_volume=54; citation_issue=1; citation_publication_date=2012; citation_pages=22-24; citation_id=CR71"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Signal Process; citation_title=A new frequency estimation method for equally and unequally spaced data; citation_author=F Andersson, M Carlsson, JY Tourneret, H Wendt; citation_volume=62; citation_issue=21; citation_publication_date=2014; citation_pages=5761-5774; citation_doi=10.1109/TSP.2014.2358961; citation_id=CR72"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Automat Contr; citation_title=Design of optimal sparse feedback gains via the alternating direction method of multipliers; citation_author=F Lin, M Fardad, MR Jovanovic; citation_volume=58; citation_issue=9; citation_publication_date=2013; citation_pages=2426-2431; citation_doi=10.1109/TAC.2013.2257618; citation_id=CR73"/>

    <meta name="citation_reference" content="citation_journal_title=Foundations Trends Mach Learn; citation_title=Distributed optimization and statistical learning via the alternating direction method of multipliers; citation_author=S Boyd, N Parikh, E Chu, B Peleato, J Eckstein; citation_volume=3; citation_issue=1; citation_publication_date=2011; citation_pages=1-122; citation_doi=10.1561/2200000016; citation_id=CR74"/>

    <meta name="citation_reference" content="citation_journal_title=Commun ACM; citation_title=MapReduce: simplified data processing on large clusters; citation_author=J Dean, S Ghemawat; citation_volume=51; citation_issue=1; citation_publication_date=2008; citation_pages=107-113; citation_doi=10.1145/1327452.1327492; citation_id=CR75"/>

    <meta name="citation_reference" content="citation_journal_title=Commun ACM; citation_title=MapReduce: a flexible data processing tool; citation_author=J Dean, S Ghemawat; citation_volume=53; citation_issue=1; citation_publication_date=2010; citation_pages=72-77; citation_doi=10.1145/1629175.1629198; citation_id=CR76"/>

    <meta name="citation_reference" content="C Chu, SK Kim, YA Lin, Y Yu, G Bradski, AY Ng, K Olukotun, Map-reduce for machine learning on multicore, in Proceedings of 20th Annual Conference on Neural Information Processing Systems (NIPS) (Vancouver, 2006), pp. 281&#8211;288"/>

    <meta name="citation_reference" content="citation_journal_title=Commun ACM; citation_title=A view of cloud computing; citation_author=M Armbrust, A Fox, R Griffith, AD Joseph, R Katz, A Konwinski, G Lee, D Patterson, A Rabkin, I Stoica, M Zaharia; citation_volume=53; citation_issue=4; citation_publication_date=2010; citation_pages=50-58; citation_doi=10.1145/1721654.1721672; citation_id=CR78"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Internet Comput; citation_title=Cloud computing: distributed internet computing for IT and scientific research; citation_author=MD Dikaiakos, D Katsaros, P Mehra, G Pallis, A Vakali; citation_volume=13; citation_issue=5; citation_publication_date=2009; citation_pages=10-13; citation_doi=10.1109/MIC.2009.103; citation_id=CR79"/>

    <meta name="citation_reference" content="citation_journal_title=Proc VLDB Endow; citation_title=Distributed GraphLab: a framework for machine learning and data mining in the cloud; citation_author=Y Low, D Bickson, J Gonzalez, C Guestrin, A Kyrola, JM Hellerstein; citation_volume=5; citation_issue=8; citation_publication_date=2012; citation_pages=716-727; citation_doi=10.14778/2212351.2212354; citation_id=CR80"/>

    <meta name="citation_reference" content="M Lenzerini, Data integration: a theoretical perspective, in Proceedings of the twenty-first ACM SIGMOD-SIGACT-SIGART Symposium on Principles of Database Systems (Madison, 2002), pp. 233&#8211;246"/>

    <meta name="citation_reference" content="A Halevy, A Rajaraman, J Ordille, Data integration: the teenage years, in Proceedings of the 32nd International Conference on Very Large Data Bases (VLDB) (Seoul, 2006), pp. 9&#8211;16"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Wirel Commun; citation_title=Spatial-temporal opportunity detection for spectrum-heterogeneous cognitive radio networks: two-dimensional sensing; citation_author=Q Wu, G Ding, J Wang, YD Yao; citation_volume=12; citation_issue=2; citation_publication_date=2013; citation_pages=516-526; citation_doi=10.1109/TWC.2012.122212.111638; citation_id=CR83"/>

    <meta name="citation_reference" content="N Srivastava, RR Salakhutdinov, Multimodal learning with deep boltzmann machines, in Proceedings of Neural Information Processing Systems Conference (NIPS) (Nevada, 2012), pp. 2222&#8211;2230"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Pattern Anal Mach Intell; citation_title=Local-learning-based feature selection for high-dimensional data analysis; citation_author=Y Sun, S Todorovic, S Goodison; citation_volume=32; citation_issue=9; citation_publication_date=2010; citation_pages=1610-1626; citation_doi=10.1109/TPAMI.2009.190; citation_id=CR85"/>

    <meta name="citation_reference" content="citation_journal_title=J Mach Learn Res; citation_title=Dimensionality reduction: a comparative review; citation_author=LJP Maaten, EO Postma, HJ Herik; citation_volume=10; citation_issue=1-41; citation_publication_date=2009; citation_pages=66-71; citation_id=CR86"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Signal Process; citation_title=Subspace learning and imputation for streaming big data matrices and tensors; citation_author=M Mardani, G Mateos, GB Giannakis; citation_volume=63; citation_issue=10; citation_publication_date=2015; citation_pages=2663-2677; citation_doi=10.1109/TSP.2015.2417491; citation_id=CR87"/>

    <meta name="citation_reference" content="&#8201;K Mohan, M Fazel, New restricted isometry results for noisy low-rank recovery, in Proceedings of IEEE International Symposium on Information Theory Proceedings (ISIT) (Texas, 2010), pp. 1573&#8211;1577"/>

    <meta name="citation_reference" content="citation_journal_title=J ACM; citation_title=Robust principal component analysis?; citation_author=EJ Cand&#232;s, X Li, Y Ma, J Wright; citation_volume=58; citation_issue=3; citation_publication_date=2011; citation_pages=1-37; citation_doi=10.1145/1970392.1970395; citation_id=CR89"/>

    <meta name="citation_reference" content="Z Lin, R Liu, Z Su, Linearized alternating direction method with adaptive penalty for low-rank representation, in Proceedings of Neural Information Processing Systems Conference (NIPS) (Granada, 2011), pp. 612&#8211;620"/>

    <meta name="citation_reference" content="citation_journal_title=Foundations Trends Mach Learn; citation_title=Online learning and online convex optimization; citation_author=S Shalev-Shwartz; citation_volume=4; citation_publication_date=2011; citation_pages=107-194; citation_doi=10.1561/2200000018; citation_id=CR91"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Knowl Data Eng; citation_title=Online feature selection and its applications; citation_author=J Wang, P Zhao, SC Hoi, R Jin; citation_volume=26; citation_issue=3; citation_publication_date=2014; citation_pages=698-710; citation_doi=10.1109/TKDE.2013.32; citation_id=CR92"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Signal Process; citation_title=Online learning with kernels; citation_author=J Kivinen, AJ Smola, RC Williamson; citation_volume=52; citation_issue=8; citation_publication_date=2004; citation_pages=2165-2176; citation_doi=10.1109/TSP.2004.830991; citation_id=CR93"/>

    <meta name="citation_reference" content="M Bilenko, S Basil, M Sahami, Adaptive product normalization: using online learning for record linkage in comparison shopping, in Proceedings of the 5th IEEE International Conference on Data Mining (ICDM) (Texas, 2005), p. 8"/>

    <meta name="citation_reference" content="citation_journal_title=Neurocomputing; citation_title=Extreme learning machine: theory and applications; citation_author=GB Huang, QY Zhu, CK Siew; citation_volume=70; citation_issue=1; citation_publication_date=2006; citation_pages=489-501; citation_doi=10.1016/j.neucom.2005.12.126; citation_id=CR95"/>

    <meta name="citation_reference" content="citation_journal_title=Neural Comput Appl; citation_title=Extreme learning machine and its applications; citation_author=S Ding, X Xu, R Nie; citation_volume=25; citation_issue=3-4; citation_publication_date=2014; citation_pages=549-556; citation_doi=10.1007/s00521-013-1522-8; citation_id=CR96"/>

    <meta name="citation_reference" content="N Tatbul, Streaming data integration: challenges and opportunities, in Proceedings of the 26th IEEE International Conference on Data Engineering Workshops (ICDEW) (Long Beach, 2010), pp. 155&#8211;158"/>

    <meta name="citation_reference" content="&#8201;DJ Abadi, Y Ahmad, M Balazinska, U Cetintemel, M Cherniack, JH Hwang, W Lindner, A Maskey, A Rasin, E Ryvkina, N Tatbul, Y Xing, SB Zdonik, The design of the borealis stream processing engine, in Proceedings of the Second Biennial Conference on Innovative Data Systems Research (CIDR) (Asilomar, 2005), pp. 277&#8211;289"/>

    <meta name="citation_reference" content="L Neumeyer, B Robbins, A Nair, A Kesari, S4: Distributed stream computing platform, in Proceedings of IEEE International Conference on Data Mining Workshops (ICDMW) (Sydney, 2010), pp. 170&#8211;177"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Data Eng Bull; citation_title=Building Linkedin&#8217;s real-time activity data pipeline; citation_author=K Goodhope, J Koshy, J Kreps, N Narkhede, R Park, J Rao, VY Ye; citation_volume=35; citation_issue=2; citation_publication_date=2012; citation_pages=33-45; citation_id=CR100"/>

    <meta name="citation_reference" content="W Yang, X Liu, L Zhang, LT Yang, Big data real-time processing based on storm, in Proceedings of the 12th IEEE International Conference on Trust, Security and Privacy in Computing and Communications (TrustCom) (Melbourne, 2013), pp. 1784&#8211;1787"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Cloud Comput; citation_title=Streaming big data processing in datacenter clouds; citation_author=B SkieS; citation_volume=1; citation_publication_date=2014; citation_pages=78-83; citation_id=CR102"/>

    <meta name="citation_reference" content="A Baldominos, E Albacete, Y Saez, P Isasi, A scalable machine learning online service for big data real-time analysis, in Proceedings of IEEE Symposium on Computational Intelligence in Big Data (CIBD) (Orlando, 2014), pp. 1&#8211;8"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Smart Grid; citation_title=Real-time load elasticity tracking and pricing for electric vehicle charging; citation_author=NY Soltani, SJ Kim, GB Giannakis; citation_volume=6; citation_issue=3; citation_publication_date=2014; citation_pages=1303-1313; citation_doi=10.1109/TSG.2014.2363837; citation_id=CR104"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Knowl Data Eng; citation_title=Decision trees for uncertain data; citation_author=S Tsang, B Kao, KY Yip, WS Ho, SD Lee; citation_volume=23; citation_issue=1; citation_publication_date=2011; citation_pages=64-78; citation_doi=10.1109/TKDE.2009.175; citation_id=CR105"/>

    <meta name="citation_reference" content="F Nie, H Wang, X Cai, H Huang, C Ding, Robust matrix completion via joint schatten p-norm and lp-norm minimization, in Proceedings of the 12th IEEE International Conference on Data Mining (ICDM) (Brussels, 2012), p. 566"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Commun; citation_title=Robust spectrum sensing with crowd sensors; citation_author=G Ding, J Wang, Q Wu, L Zhang, Y Zou, YD Yao, Y Chen; citation_volume=62; citation_issue=9; citation_publication_date=2014; citation_pages=3129-3143; citation_doi=10.1109/TCOMM.2014.2346775; citation_id=CR107"/>

    <meta name="citation_reference" content="citation_journal_title=AI Mag; citation_title=From data mining to knowledge discovery in databases; citation_author=U Fayyad, G Piatetsky-Shapiro, P Smyth; citation_volume=17; citation_issue=3; citation_publication_date=1996; citation_pages=37-54; citation_id=CR108"/>

    <meta name="citation_reference" content="citation_title=Smart machines: IBM&#8217;s Watson and the era of cognitive computing; citation_publication_date=2013; citation_id=CR109; citation_author=J Kelly; citation_author=S Hamm; citation_publisher=Columbia University Press"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Signal Proc Mag; citation_title=Stochastic approximation vis-a-vis online learning for big data analytics; citation_author=K Slavakis, SJ Kim, G Mateos, GB Giannakis; citation_volume=31; citation_issue=6; citation_publication_date=2014; citation_pages=124-129; citation_doi=10.1109/MSP.2014.2345536; citation_id=CR110"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Signal Proc Mag; citation_title=Convex optimization for big data: scalable, randomized, and parallel algorithms for big data analytics; citation_author=V Cevher, S Becker, M Schmidt; citation_volume=31; citation_issue=5; citation_publication_date=2014; citation_pages=32-43; citation_doi=10.1109/MSP.2014.2329397; citation_id=CR111"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Signal Proc Mag; citation_title=Outlying sequence detection in large data sets: a data-driven approach; citation_author=A Tajer, VV Veeravalli, HV Poor; citation_volume=31; citation_issue=5; citation_publication_date=2014; citation_pages=44-56; citation_doi=10.1109/MSP.2014.2329428; citation_id=CR112"/>

    <meta name="citation_reference" content="citation_journal_title=Inf Sci; citation_title=Distributed learning for random vector functional-link networks; citation_author=S Scardapane, D Wang, M Panella, A Uncini; citation_volume=301; citation_publication_date=2015; citation_pages=271-284; citation_doi=10.1016/j.ins.2015.01.007; citation_id=CR113"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Signal Process; citation_title=Hybrid random/deterministic parallel algorithms for nonconvex big data optimization; citation_author=A Daneshmand, F Facchinei, V Kungurtsev, G Scutari; citation_volume=63; citation_issue=15; citation_publication_date=2015; citation_pages=3914-3929; citation_doi=10.1109/TSP.2015.2436357; citation_id=CR114"/>

    <meta name="citation_reference" content="P. Bianchi, W. Hachem, F. Iutzeler, A stochastic coordinate descent primal-dual algorithm and applications to large-scale composite optimization. arXiv preprint (2014). arXiv:1407.0898"/>

    <meta name="citation_reference" content="HT Wai, TH Chang, A Scaglione, A consensus-based decentralized algorithm for non-convex optimization with application to dictionary learning, in Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) (South Brisbane, 2015), pp. 3546&#8211;3550"/>

    <meta name="citation_reference" content="D. Berberidis, V. Kekatos, G.B. Giannakis, Online censoring for large-scale regressions with application to streaming big data. arXiv preprint (2015). arXiv:1507.07536"/>

    <meta name="citation_reference" content="K. Slavakis, G.B. Giannakis, Per-block-convex data modeling by accelerated stochastic approximation. arXiv preprint (2015). arXiv:1501.07315"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE J Sel Areas Commun; citation_title=Communication theoretic data analytics; citation_author=KC Chen, SL Huang, L Zheng, HV Poor; citation_volume=33; citation_issue=4; citation_publication_date=2015; citation_pages=663-675; citation_doi=10.1109/JSAC.2015.2393471; citation_id=CR119"/>

    <meta name="citation_reference" content="citation_journal_title=Neural Comput Appl; citation_title=An online incremental learning support vector machine for large-scale data; citation_author=J Zheng, F Shen, H Fan, J Zhao; citation_volume=22; citation_issue=5; citation_publication_date=2013; citation_pages=1023-1035; citation_doi=10.1007/s00521-011-0793-1; citation_id=CR120"/>

    <meta name="citation_reference" content="C Ghosh, C Cordeiro, DP Agrawal, M Bhaskara Rao, Markov chain existence and hidden Markov models in spectrum sensing, in Proceedings of the IEEE International Conference on Pervasive Computing &amp; Communications (PERCOM) (Galveston, 2009), pp. 1&#8211;6"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Cybern; citation_title=A parallel and incremental approach for data-intensive learning of Bayesian networks; citation_author=K Yue, Q Fang, X Wang, J Li, W Weiy; citation_volume=99; citation_publication_date=2015; citation_pages=1-15; citation_doi=10.1109/TCYB.2015.2478154; citation_id=CR122"/>

    <meta name="citation_reference" content="X Dong, Y Li, C Wu, Y Cai, A learner based on neural network for cognitive radio, in Proceedings of the 12th IEEE International Conference on Communication Technology (ICCT) (Nanjing, 2010), pp. 893&#8211;896"/>

    <meta name="citation_reference" content="citation_journal_title=Int J Antenn Propag; citation_title=Cognitive radio transceivers: RF, spectrum sensing, and learning algorithms review; citation_author=A El-Hajj, L Safatly, M Bkassiny, M Husseini; citation_volume=11; citation_issue=5; citation_publication_date=2014; citation_pages=479-482; citation_id=CR124"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Wirel Commun; citation_title=Multidimensional dirichlet process-based non-parametric signal classification for autonomous self-learning cognitive radios; citation_author=M Bkassiny, SK Jayaweera, Y Li; citation_volume=12; citation_issue=11; citation_publication_date=2013; citation_pages=5413-5423; citation_doi=10.1109/TWC.2013.092013.120688; citation_id=CR125"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Veh Technol; citation_title=Distributed Q-learning for aggregated interference control in cognitive radio networks; citation_author=A Galindo-Serrano, L Giupponi; citation_volume=59; citation_issue=4; citation_publication_date=2010; citation_pages=1823-1834; citation_doi=10.1109/TVT.2010.2043124; citation_id=CR126"/>

    <meta name="citation_reference" content="citation_journal_title=Manage Sci; citation_title=Solving semi-markov decision problems using average reward reinforcement learning; citation_author=TK Das, A Gosavi, S Mahadevan, N Marchalleck; citation_volume=45; citation_issue=4; citation_publication_date=1999; citation_pages=560-574; citation_doi=10.1287/mnsc.45.4.560; citation_id=CR127"/>

    <meta name="citation_reference" content="citation_journal_title=Mach Learn; citation_title=Learning to predict by the methods of temporal differences; citation_author=RS Sutton; citation_volume=3; citation_issue=1; citation_publication_date=1988; citation_pages=9-44; citation_id=CR128"/>

    <meta name="citation_reference" content="citation_journal_title=Mach Learn; citation_title=Convergence results for single-step on-policy reinforcement-learning algorithms; citation_author=S Singh, T Jaakkola, ML Littman, C Szepesv&#225;ri; citation_volume=38; citation_publication_date=2000; citation_pages=287-308; citation_doi=10.1023/A:1007678930559; citation_id=CR129"/>

    <meta name="citation_author" content="Junfei Qiu"/>

    <meta name="citation_author_institution" content="College of Communications Engineering, PLA University of Science and Technology, Nanjing, China"/>

    <meta name="citation_author" content="Qihui Wu"/>

    <meta name="citation_author_institution" content="College of Communications Engineering, PLA University of Science and Technology, Nanjing, China"/>

    <meta name="citation_author" content="Guoru Ding"/>

    <meta name="citation_author_institution" content="College of Communications Engineering, PLA University of Science and Technology, Nanjing, China"/>

    <meta name="citation_author" content="Yuhua Xu"/>

    <meta name="citation_author_institution" content="College of Communications Engineering, PLA University of Science and Technology, Nanjing, China"/>

    <meta name="citation_author" content="Shuo Feng"/>

    <meta name="citation_author_institution" content="College of Communications Engineering, PLA University of Science and Technology, Nanjing, China"/>

 
    
        <meta property="og:url" content="https://link.springer.com/article/10.1186/s13634-016-0355-x"/>
        <meta property="og:type" content="article"/>
        <meta property="og:site_name" content="EURASIP Journal on Advances in Signal Processing"/>
        <meta property="og:title" content="A survey of machine learning for big data processing"/>
        <meta property="og:description" content="There is no doubt that big data are now rapidly expanding in all science and engineering domains. While the potential of these massive data is undoubtedly significant, fully making sense of them requires new ways of thinking and novel learning techniques to address the various challenges. In this paper, we present a literature survey of the latest advances in researches on machine learning for big data processing. First, we review the machine learning techniques and highlight some promising learning methods in recent studies, such as representation learning, deep learning, distributed and parallel learning, transfer learning, active learning, and kernel-based learning. Next, we focus on the analysis and discussions about the challenges and possible solutions of machine learning for big data. Following that, we investigate the close connections of machine learning with signal processing techniques for big data processing. Finally, we outline several open issues and research trends."/>
        <meta property="og:image" content="https://media.springernature.com/w110/springer-static/cover/journal/13634.jpg"/>
    
    <meta name="access" content="Yes">

    <html>
<head>
    <script type="text/javascript">
        window.dataLayer = [{"Country":"IT","doi":"10.1186-s13634-016-0355-x","Journal Title":"EURASIP Journal on Advances in Signal Processing","Journal Id":13634,"Keywords":"Machine learning, Big data, Data mining, Signal processing techniques","kwrd":["Machine_learning","Big_data","Data_mining","Signal_processing_techniques"],"Page":"article","Labs":"Y","ksg":"Krux.segments","kuid":"Krux.uid","Has Body":"Y","Features":[],"Open Access":"Y","hasAccess":"Y","user":{"license":{"businessPartnerID":[],"businessPartnerIDString":""}},"Access Type":"subscription","Bpids":"","Bpnames":"","BPID":["1"],"eventTrackerBaseUrl":"https://event-tracker.springernature.com","VG Wort Identifier":"vgzm.415900-10.1186-s13634-016-0355-x","Full HTML":"Y","Subject Codes":["SCT","SCT24051","SCP31070"],"pmc":["T","T24051","P31070"],"session":{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},"content":{"serial":{"eissn":"1687-6180"},"type":"Article","category":{"pmc":{"primarySubject":"Engineering","primarySubjectCode":"T","secondarySubjects":{"1":"Signal,Image and Speech Processing","2":"Quantum Information Technology, Spintronics"},"secondarySubjectCodes":{"1":"T24051","2":"P31070"}},"sucode":"SC8"},"attributes":{"deliveryPlatform":"oscar"}},"Event Category":"Article","GA Key":"UA-26408784-1","DOI":"10.1186/s13634-016-0355-x"}];
        var event = new CustomEvent('dataLayerCreated');
        document.dispatchEvent(event);
    </script>
</head>
</html>


    


<script src="/oscar-static/js/jquery-220afd743d.js" id="jquery"></script>

<script>
    function OptanonWrapper() {
        dataLayer.push({
            'event' : 'onetrustActive'
        });
    }
</script>


    <script data-test="onetrust-link" type="text/javascript" src="https://cdn.cookielaw.org/consent/6b2ec9cd-5ace-4387-96d2-963e596401c6.js" charset="UTF-8"></script>






</head>
<body class="shared-article-renderer">
    <div class="u-vh-full">
        <div class="c-ad c-ad--LB1">
    <div class="c-ad c-ad__inner">
        <p class="c-ad__label">Advertisement</p>
        <div id="div-gpt-ad-LB1" data-gpt-unitpath="/270604982/springerlink/13634/article" data-gpt-sizes="728x90" data-gpt-targeting="pos=LB1;"></div>
    </div>
</div>

<header class="c-header">
    <div class="c-header__container">
        <div class="c-header__brand">
            <a id="logo" class="site-logo" href="/" title="Go to homepage">
    
        <div data-test="header-academic" class="u-visually-hidden">SpringerLink</div>
        <svg class="site-logo__springer" width="148" height="30" role="img" focusable="false" aria-hidden="true">
            <image width="148" height="30"
                   src=/oscar-static/images/springerlink/png/springerlink-58406caa4d.png
                   xmlns:xlink="http://www.w3.org/1999/xlink"
                   xlink:href=/oscar-static/images/springerlink/svg/springerlink-253e23a83d.svg></image>
        </svg>
    
    
    
</a>

        </div>
        <div class="c-header__navigation">
            
    <button type="button" class="c-header__link u-button-reset u-margin-right-lg js-search" data-test="header-search-button">
        <span class="u-display-flex u-flex-align-center">
            Search
            <svg class="c-icon u-margin-left-xs" aria-hidden="true" focusable="false" xmlns="http://www.w3.org/2000/svg" width="14" height="14" viewBox="0 0 16 16"><path d="M16 13.751l-4.415-4.366c.61-.944.969-2.06.968-3.257C12.553 2.749 9.738 0 6.276 0 2.815 0 0 2.749 0 6.128c0 3.38 2.815 6.129 6.276 6.129a6.363 6.363 0 0 0 2.881-.689L13.641 16 16 13.751zM1.521 6.128c0-2.56 2.133-4.643 4.755-4.643 2.623 0 4.756 2.083 4.756 4.643 0 2.562-2.134 4.644-4.756 4.644-2.622-.001-4.755-2.083-4.755-4.644z"/></svg>
        </span>
    </button>
    <nav>
        <ul class="c-header__menu" data-header-menu>
            <li class="c-header__item">
                <a class="c-header__link" href="/">Home</a>
            </li>

            
                <li class="c-header__item">
                    <a data-test="login-link" class="c-header__link" href="//link.springer.com/signup-login?previousUrl=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1186%2Fs13634-016-0355-x">Log in</a>
                </li>
            

            
        </ul>
    </nav>




        </div>
    </div>
</header>
<!-- TODO: pending to use a component for this -->
<div class="c-popup-search js-header-search u-js-hide">
    <div class="c-popup-search__content">
        <div class="u-container">
            <div class="c-popup-search__container c-popup-search__content">
    <div class="c-search">
        <form role="search" method="GET" action="/search">
            <label for="publisherSearch" class="c-search__label">Search SpringerLink</label>
            <div class="c-search__content">
                <input id="publisherSearch" class="c-search__input js-publisher-search-input" autocomplete="off" role="textbox" name="query" type="text" value="">
                <button class="c-search__button" type="submit">
                    <span class="u-visually-hidden">Search</span>
                    <div class="c-search__icon">
                        <!-- TODO: standarize the use of including svg icons -->
                        <svg aria-hidden="true" focusable="false" xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 16 16"><path d="M16 13.751l-4.415-4.366c.61-.944.969-2.06.968-3.257C12.553 2.749 9.738 0 6.276 0 2.815 0 0 2.749 0 6.128c0 3.38 2.815 6.129 6.276 6.129a6.363 6.363 0 0 0 2.881-.689L13.641 16 16 13.751zM1.521 6.128c0-2.56 2.133-4.643 4.755-4.643 2.623 0 4.756 2.083 4.756 4.643 0 2.562-2.134 4.644-4.756 4.644-2.622-.001-4.755-2.083-4.755-4.644z"/></svg>
                    </div>
                </button>
                <input type="hidden" name="searchType" value="publisherSearch">
            </div>
        </form>
    </div>
</div>

        </div>
    </div>
</div>


        <div class="u-container u-margin-top-xl u-margin-bottom-xl" id="main-content" data-component="article-container">
            <div class="c-page-layout c-page-layout--article" data-component="sticky-container">

                <main class="c-page-layout__main">
                    <article itemscope itemtype="http://schema.org/ScholarlyArticle" lang="en">
                        <div class="c-article-header">
                            <header>
                                <ul class="c-article-identifiers" data-test="article-identifier">
                                    
    <li class="c-article-identifiers__item" data-test="article-category">Review</li>
    
        
            <li class="c-article-identifiers__item">
                <span class="c-article-identifiers__open" data-test="open-access">Open Access</span>
            </li>
        
        
    
    

                                    <li class="c-article-identifiers__item"><a href="#article-info" data-track="click" data-track-action="publication date" data-track-category="article body" data-track-label="link">Published: <time datetime="2016-05-28" itemprop="datePublished">28 May 2016</time></a></li>
                                </ul>

                                
                                <h1 class="c-article-title u-h1" data-test="article-title" data-article-title="" itemprop="name headline">A survey of machine learning for big data processing</h1>
                                <ul class="c-author-list js-list-authors js-etal-collapsed" data-etal="25" data-etal-small="3" data-test="authors-list"><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-category="article body" data-track-label="link" href="#auth-1">Junfei Qiu</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="PLA University of Science and Technology" /><meta itemprop="address" content="grid.440614.3, College of Communications Engineering, PLA University of Science and Technology, Nanjing, 210007, China" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-category="article body" data-track-label="link" href="#auth-2">Qihui Wu</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="PLA University of Science and Technology" /><meta itemprop="address" content="grid.440614.3, College of Communications Engineering, PLA University of Science and Technology, Nanjing, 210007, China" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-category="article body" data-track-label="link" href="#auth-3" data-corresp-id="c1">Guoru Ding<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-email"></use></svg></a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="PLA University of Science and Technology" /><meta itemprop="address" content="grid.440614.3, College of Communications Engineering, PLA University of Science and Technology, Nanjing, 210007, China" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-category="article body" data-track-label="link" href="#auth-4">Yuhua Xu</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="PLA University of Science and Technology" /><meta itemprop="address" content="grid.440614.3, College of Communications Engineering, PLA University of Science and Technology, Nanjing, 210007, China" /></span></sup> &amp; </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-category="article body" data-track-label="link" href="#auth-5">Shuo Feng</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="PLA University of Science and Technology" /><meta itemprop="address" content="grid.440614.3, College of Communications Engineering, PLA University of Science and Technology, Nanjing, 210007, China" /></span></sup> </li></ul>
                                <p class="c-article-info-details" data-container-section="info">
                                    
    <a data-test="journal-link" href="/journal/13634"><i data-test="journal-title">EURASIP Journal on Advances in Signal Processing</i></a>

                                    <b data-test="journal-volume"><span class="u-visually-hidden">volume</span> 2016</b>, Article number: <span data-test="article-number">67</span> (<span data-test="article-publication-year">2016</span>)
            <a href="#citeas" class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-category="article body" data-track-label="link">Cite this article</a>
                                </p>
                                <p class="c-article-info-details" data-test="article-history-link">
                                    
    

                                </p>
                                <div data-test="article-metrics">
                                    <div id="altmetric-container">
    
        <div class="c-article-metrics-bar__wrapper u-clear-both">
            <ul class="c-article-metrics-bar u-list-inline">
                
                    <li class=" c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">49k <span class="c-article-metrics-bar__label">Accesses</span></p>
                    </li>
                
                
                    <li class="c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">65 <span class="c-article-metrics-bar__label">Citations</span></p>
                    </li>
                
                
                    
                        <li class="c-article-metrics-bar__item">
                            <p class="c-article-metrics-bar__count">155 <span class="c-article-metrics-bar__label">Altmetric</span></p>
                        </li>
                    
                    
                        <li class="c-article-metrics-bar__item">
                            <p class="c-article-metrics-bar__details"><a href="/article/10.1186%2Fs13634-016-0355-x/metrics" data-track="click" data-track-action="view metrics" data-track-category="article body" data-track-label="link" rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
                        </li>
                    
                
            </ul>
        </div>
    
</div>

                                </div>
                                
                                
                            </header>
                        </div>

                        <div data-article-body="true" data-track-component="article body" class="c-article-body">
                            <section aria-labelledby="Abs1" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title u-h2 js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><p>There is no doubt that big data are now rapidly expanding in all science and engineering domains. While the potential of these massive data is undoubtedly significant, fully making sense of them requires new ways of thinking and novel learning techniques to address the various challenges. In this paper, we present a literature survey of the latest advances in researches on machine learning for big data processing. First, we review the machine learning techniques and highlight some promising learning methods in recent studies, such as representation learning, deep learning, distributed and parallel learning, transfer learning, active learning, and kernel-based learning. Next, we focus on the analysis and discussions about the challenges and possible solutions of machine learning for big data. Following that, we investigate the close connections of machine learning with signal processing techniques for big data processing. Finally, we outline several open issues and research trends.</p></div></div></section>

                            

                            

                            
                                
                                    <section aria-labelledby="Sec1"><div class="c-article-section" id="Sec1-section"><h2 class="c-article-section__title u-h2 js-section-title js-c-reading-companion-sections-item" id="Sec1">Review</h2><div class="c-article-section__content" id="Sec1-content"><h3 class="c-article__sub-heading u-h3" id="Sec2">Introduction</h3><p>It is obvious that we are living in a data deluge era, evidenced by the phenomenon that enormous amount of data have been being continually generated at unprecedented and ever increasing scales. Large-scale data sets are collected and studied in numerous domains, from engineering sciences to social networks, commerce, biomolecular research, and security [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1" title="A Sandryhaila, JMF Moura, Big data analysis with signal processing on graphs: representation and processing of massive data sets with irregular structure. IEEE Signal Proc Mag 31(5), 80–90 (2014)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR1" id="ref-link-section-d69318e357">1</a>]. Particularly, digital data, generated from a variety of digital devices, are growing at astonishing rates. According to [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2" title="J Gantz, D Reinsel, Extracting value from chaos (EMC, Hopkinton, 2011)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR2" id="ref-link-section-d69318e360">2</a>], in 2011, digital information has grown nine times in volume in just 5 years and its amount in the world will reach 35 trillion gigabytes by 2020 [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 3" title="J Gantz, D Reinsel, The digital universe decade—are you ready (EMC, Hopkinton, 2010)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR3" id="ref-link-section-d69318e363">3</a>]. Therefore, the term “Big Data” was coined to capture the profound meaning of this data explosion trend.</p><p>To clarify what the big data refers to, several good surveys have been presented recently and each of them views the big data from different perspectives, including challenges and opportunities [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 4" title="D Che, M Safran, Z Peng, From big data to big data mining: challenges, issues, and opportunities, in Proceedings of the 18th International Conference on DASFAA (Wuhan, 2013), pp. 1–15" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR4" id="ref-link-section-d69318e369">4</a>], background and research status [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 5" title="M Chen, S Mao, Y Liu, Big data: a survey. Mobile Netw Appl 19(2), 171–209 (2014)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR5" id="ref-link-section-d69318e372">5</a>], and analytics platforms [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 6" title="H Hu, Y Wen, T Chua, X Li, Toward scalable systems for big data analytics: a technology tutorial. IEEE Access 2, 652–687 (2014)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR6" id="ref-link-section-d69318e375">6</a>]. Among these surveys, a comprehensive overview of the big data from three different angles, i.e., innovation, competition, and productivity, was presented by the McKinsey Global Institute (MGI) [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 7" title="J Manyika, M Chui, B Brown, J Bughin, R Dobbs, C Roxburgh, AH Byers, Big data: the next frontier for innovation, competition, and productivity (McKinsey Global Institute, USA, 2011)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR7" id="ref-link-section-d69318e378">7</a>]. Besides describing the fundamental techniques and technologies of big data, a number of more recent studies have investigated big data under particular context. For example, [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 8" title="Q Wu, G Ding, Y Xu, S Feng, Z Du, J Wang, K Long, Cognitive internet of things: a new paradigm beyond connection. IEEE Internet Things J 1(2), 129–143 (2014)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR8" id="ref-link-section-d69318e381">8</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 9" title="CW Tsai, CF Lai, MC Chiang, LT Yang, Data mining for internet of things: a survey. IEEE Commun Surv Tut 16(1), 77–97 (2014)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR9" id="ref-link-section-d69318e385">9</a>] gave a brief review of the features of big data from Internet of Things (IoT). Some authors also analyzed the new characteristics of big data in wireless networks, e.g., in terms of 5G [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 10" title="A Imran, A Zoha, Challenges in 5G: how to empower SON with big data for enabling 5G. IEEE Netw 28(6), 27–33 (2014)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR10" id="ref-link-section-d69318e388">10</a>]. In [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 11" title="X Wu, X Zhu, G Wu, W Ding, Data mining with big data. IEEE Trans Knowl Data Eng 26(1), 97–107 (2014)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR11" id="ref-link-section-d69318e391">11</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 12" title="A Rajaraman, JD Ullman, Mining of massive data sets (Cambridge University Press, Oxford, 2011)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR12" id="ref-link-section-d69318e394">12</a>], the authors proposed various big data processing models and algorithms from the data mining perspective.</p><p>Over the past decade, machine learning techniques have been widely adopted in a number of massive and complex data-intensive fields such as medicine, astronomy, biology, and so on, for these techniques provide possible solutions to mine the information hidden in the data. Nevertheless, as the time for big data is coming, the collection of data sets is so large and complex that it is difficult to deal with using traditional learning methods since the established process of learning from conventional datasets was not designed to and will not work well with high volumes of data. For instance, most traditional machine learning algorithms are designed for data that would be completely loaded into memory [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 13" title="XW Chen, X Lin, Big data deep learning: challenges and perspectives. IEEE Access 2, 514–525 (2014)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR13" id="ref-link-section-d69318e400">13</a>], which does not hold any more in the context of big data. Therefore, although learning from these numerous data is expected to bring significant science and engineering advances along with improvements in quality of our life [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 14" title="K Slavakis, GB Giannakis, G Mateos, Modeling and optimization for big data analytics: (statistical) learning tools for our era of data deluge. IEEE Signal Proc Mag 31(5), 18–31 (2014)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR14" id="ref-link-section-d69318e403">14</a>], it brings tremendous challenges at the same time.</p><p>The goal of this paper is twofold. One is mainly to discuss several important issues related to learning from massive amounts of data and highlight current research efforts and the challenges to big data, as well as the future trends. The other is to analyze the connections of machine learning with modern signal processing (SP) techniques for big data processing from different perspectives. The main contributions of this paper are summarized as follows:</p><ul class="u-list-style-bullet">
                    <li>
                      <p>We first give a brief review of the traditional machine learning techniques, followed by several advanced learning methods in recent researches that are either promising or much needed for solving the big data problems.</p>
                    </li>
                    <li>
                      <p>We then present a systematic analysis of the challenges and possible solutions for learning with big data, which are in terms of the five big data characteristics such as volume, variety, velocity, veracity, and value.</p>
                    </li>
                    <li>
                      <p>We next discuss the great ties of machine learning with SP techniques for the big data processing.</p>
                    </li>
                    <li>
                      <p>We finally provide several open issues and research trends.</p>
                    </li>
                  </ul>
                        <p>The remainder of the paper, as the roadmap given in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#Fig1">1</a> shows, is organized as follows. In Section <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#Sec3">1.2</a>, we start with a review of some essential and relevant concepts about machine learning, followed by some current advanced learning techniques. Section <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#Sec6">1.3</a> provides a comprehensive survey of challenges bringing by big data for machine learning, mainly from five aspects. The relationships between machine learning and signal processing techniques for big data processing are presented in Section <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#Sec13">1.4</a>. Section <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#Sec16">1.5</a> gives some open issues and research trends. Conclusions are drawn in Section <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#Sec17">2</a>.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1"><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 1</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-category="article body" data-track-label="image" data-track-action="view figure" href="/article/10.1186/s13634-016-0355-x/figures/1?shared-article-renderer" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1186%2Fs13634-016-0355-x/MediaObjects/13634_2016_355_Fig1_HTML.gif?as=webp"></source><img aria-describedby="figure-1-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1186%2Fs13634-016-0355-x/MediaObjects/13634_2016_355_Fig1_HTML.gif" alt="figure1" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p>Roadmap of this survey</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-category="article body" data-track-label="button" data-track-action="view figure" href="/article/10.1186/s13634-016-0355-x/figures/1?shared-article-renderer" data-track-dest="link:Figure1 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <h3 class="c-article__sub-heading u-h3" id="Sec3">Brief review of machine learning techniques</h3><p>In this section, we first present some essential concepts and classification of machine learning and then highlight a list of advanced learning techniques.</p><h4 class="c-article__sub-heading u-h3 c-article__sub-heading--light" id="Sec4">Definition and classification of machine learning</h4><p>Machine leaning is a field of research that formally focuses on the theory, performance, and properties of learning systems and algorithms. It is a highly interdisciplinary field building upon ideas from many different kinds of fields such as artificial intelligence, optimization theory, information theory, statistics, cognitive science, optimal control, and many other disciplines of science, engineering, and mathematics [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 15" title="TM Mitchell, Machine learning (McGraw-Hill, New York, 1997)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR15" id="ref-link-section-d69318e492">15</a>–<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 18" title="TM Mitchell, The discipline of machine learning (Carnegie Mellon University, School of Computer Science, Machine Learning Department, 2006)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR18" id="ref-link-section-d69318e495">18</a>]. Because of its implementation in a wide range of applications, machine learning has covered almost every scientific domain, which has brought great impact on the science and society [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 19" title="C Rudin, KL Wagstaff, Machine learning for science and society. Mach Learn 95(1), 1–9 (2014)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR19" id="ref-link-section-d69318e498">19</a>]. It has been used on a variety of problems, including recommendation engines, recognition systems, informatics and data mining, and autonomous control systems [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 20" title="CM Bishop, Pattern recognition and machine learning (Springer, New York, 2006)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR20" id="ref-link-section-d69318e501">20</a>].</p><p>Generally, the field of machine learning is divided into three subdomains: supervised learning, unsupervised learning, and reinforcement learning [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 21" title="B Adam, IFC Smith, F Asce, Reinforcement learning for structural control. J Comput Civil Eng 22(2), 133–139 (2008)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR21" id="ref-link-section-d69318e507">21</a>]. Briefly, supervised learning requires training with labeled data which has inputs and desired outputs. In contrast with the supervised learning, unsupervised learning does not require labeled training data and the environment only provides inputs without desired targets. Reinforcement learning enables learning from feedback received through interactions with an external environment. Based on these three essential learning paradigms, a lot of theory mechanisms and application services have been proposed for dealing with data tasks [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 22" title="N Jones, Computer science: the learning machines. Nature 505(7482), 146–148 (2014)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR22" id="ref-link-section-d69318e510">22</a>–<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 24" title="R Bekkerman, EY Ran, N Tishby, Y Winter, Distributional word clusters vs. words for text categorization. J Mach Learn Res 3, 1183–1208 (2003)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR24" id="ref-link-section-d69318e513">24</a>]. For example, in [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 22" title="N Jones, Computer science: the learning machines. Nature 505(7482), 146–148 (2014)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR22" id="ref-link-section-d69318e516">22</a>], Google applies machine learning algorithms to massive chunks of messy data obtained from the Internet for Google’s translator, Google’s street view, Android’s voice recognition, and image search engine. A simple comparison of these three machine learning technologies from different perspectives is given in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#Tab1">1</a> to outline the machine learning technologies for data processing. The “Data Processing Tasks” column of the table gives the problems that need to be solved and the “Learning Algorithms” column describes the methods that may be used. In summary, from data processing perspective, supervised learning and unsupervised learning mainly focus on data analysis while reinforcement learning is preferred for decision-making problems. Another point is that most traditional machine-learning-based systems are designed with the assumption that all the collected data would be completely loaded into memory for centralized processing. However, as the data keeps getting bigger and bigger, the existing machine learning techniques encounter great difficulties when they are required to handle the unprecedented volume of data. Nowadays, there is a great need to develop efficient and intelligent learning methods to cope with future data processing demands.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-1"><figure><figcaption class="c-article-table__figcaption"><b id="Tab1" data-test="table-caption">Table 1 Comparison of machine learning technologies</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-category="article body" data-track-label="button" rel="nofollow" href="/article/10.1186/s13634-016-0355-x/tables/1?shared-article-renderer"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <h4 class="c-article__sub-heading u-h3 c-article__sub-heading--light" id="Sec5">Advanced learning methods</h4><p>In this subsection, we introduce a few recent learning methods that may be either promising or much needed for solving the big data problems. The outstanding characteristic of these methods is to focus on the idea of learning, rather than just a single algorithm.</p><ol class="u-list-style-none">
                      <li>
                        <span class="u-custom-list-number">1.</span>
                        
                          <p>
                                          <i>Representation Learning</i>: Datasets with high-dimensional features have become increasingly common nowadays, which challenge the current learning algorithms to extract and organize the discriminative information from the data. Fortunately, representation learning [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 25" title="Y Bengio, A Courville, P Vincent, Representation learning: a review and new perspectives. IEEE Trans Pattern Anal 35(8), 1798–1828 (2012)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR25" id="ref-link-section-d69318e893">25</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 26" title="F Huang, E Yates, Biased representation learning for domain adaptation, in Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (Jeju Island, 2012), pp. 1313–1323" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR26" id="ref-link-section-d69318e896">26</a>], a promising solution to learn the meaningful and useful representations of the data that make it easier to extract useful information when building classifiers or other predictors, has been presented and achieved impressive performance on many dimensionality reduction tasks [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 27" title="W Tu, S Sun, Cross-domain representation-learning framework with combination of class-separate and domain-merge objectives, in Proceedings of the 1st International Workshop on Cross Domain Knowledge Discovery in Web and Social Network Mining (Beijing, 2012), pp. 18–25" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR27" id="ref-link-section-d69318e899">27</a>]. Representation learning aims to achieve that a reasonably sized learned representation can capture a huge number of possible input configurations, which can greatly facilitate improvements in both computational efficiency and statistical efficiency [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 25" title="Y Bengio, A Courville, P Vincent, Representation learning: a review and new perspectives. IEEE Trans Pattern Anal 35(8), 1798–1828 (2012)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR25" id="ref-link-section-d69318e902">25</a>].</p>
                          <p>There are mainly three subtopics on representation learning: feature selection, feature extraction, and distance metric learning [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 27" title="W Tu, S Sun, Cross-domain representation-learning framework with combination of class-separate and domain-merge objectives, in Proceedings of the 1st International Workshop on Cross Domain Knowledge Discovery in Web and Social Network Mining (Beijing, 2012), pp. 18–25" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR27" id="ref-link-section-d69318e908">27</a>]. In order to give impetus to the multidomain learning ability of representation learning, automatic representation learning [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 28" title="S Li, C Huang, C Zong, Multi-domain sentiment classification with classifier combination. J Comput Sci Technol 26(1), 25–33 (2011)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR28" id="ref-link-section-d69318e911">28</a>], biased representation learning [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 26" title="F Huang, E Yates, Biased representation learning for domain adaptation, in Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (Jeju Island, 2012), pp. 1313–1323" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR26" id="ref-link-section-d69318e914">26</a>], cross-domain representation learning [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 27" title="W Tu, S Sun, Cross-domain representation-learning framework with combination of class-separate and domain-merge objectives, in Proceedings of the 1st International Workshop on Cross Domain Knowledge Discovery in Web and Social Network Mining (Beijing, 2012), pp. 18–25" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR27" id="ref-link-section-d69318e917">27</a>], and some other related techniques [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 29" title="F Huang, E Yates, Exploring representation-learning approaches to domain adaptation, in Proceedings of the 2010 Workshop on Domain Adaptation for Natural Language Processing (Uppsala, 2010), pp. 23–30" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR29" id="ref-link-section-d69318e920">29</a>] have been proposed in recent years. The rapid increase in the scientific activity on representation learning has been accompanied and nourished by a remarkable string of empirical successes in real-world applications, such as speech recognition, natural language processing, and intelligent vehicle systems [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 30" title="A Bordes, X Glorot, JWAY Bengio, Joint learning of words and meaning representations for open-text semantic parsing, in Proceedings of 15th International Conference on Artificial Intelligence and Statistics (La Palma, 2012), pp. 127–135" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR30" id="ref-link-section-d69318e924">30</a>–<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 32" title="K Dwivedi, K Biswaranjan, A Sethi, Drowsy driver detection using representation learning, in Proceedings of the IEEE International Advance Computing Conference (Gurgaon, 2014), pp. 995–999" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR32" id="ref-link-section-d69318e927">32</a>].</p>
                        
                      </li>
                      <li>
                        <span class="u-custom-list-number">2.</span>
                        
                          <p>
                                          <i>Deep learning</i>: Nowadays, there is no doubt that deep learning is one of the hottest research trends in machine learning field. In contrast to most traditional learning techniques, which are considered using shallow-structured learning architectures, deep learning mainly uses supervised and/or unsupervised strategies in deep architectures to automatically learn hierarchical representations [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 33" title="D Yu, L Deng, Deep learning and its applications to signal and information processing. IEEE Signal Proc Mag 28(1), 145–154 (2011)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR33" id="ref-link-section-d69318e944">33</a>]. Deep architectures can often capture more complicated, hierarchically launched statistical patterns of inputs for achieving to be adaptive to new areas than traditional learning methods and often outperform state of the art achieved by hand-made features [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 34" title="I Arel, DC Rose, TP Karnowski, Deep machine learning-a new frontier in artificial intelligence research. IEEE Comput Intell Mag 5(4), 13–18 (2010)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR34" id="ref-link-section-d69318e947">34</a>]. Deep belief networks (DBNs) [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 33" title="D Yu, L Deng, Deep learning and its applications to signal and information processing. IEEE Signal Proc Mag 28(1), 145–154 (2011)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR33" id="ref-link-section-d69318e950">33</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 35" title="Y Bengio, Learning deep architectures for AI. Foundations Trends Mach Learn 2(1), 1–127 (2009)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR35" id="ref-link-section-d69318e953">35</a>] and convolutional neural networks (CNNs) [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 36" title="R Collobert, J Weston, L Bottou, M Karlen, K Kavukcuoglu, P Kuksa, Natural language processing (almost) from scratch. J Mach Learn Res 12, 2493–2537 (2011)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR36" id="ref-link-section-d69318e957">36</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 37" title="P Le Callet, C Viard-Gaudin, D Barba, A convolutional neural network approach for objective video quality assessment. IEEE Trans Neural Networ 17(5), 1316–1327 (2006)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR37" id="ref-link-section-d69318e960">37</a>] are two mainstream deep learning approaches and research directions proposed over the past decade, which have been well established in the deep learning field and shown great promise for future work [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 13" title="XW Chen, X Lin, Big data deep learning: challenges and perspectives. IEEE Access 2, 514–525 (2014)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR13" id="ref-link-section-d69318e963">13</a>].</p>
                          <p>Due to the state-of-the-art performance of deep learning, it has attracted much attention from the academic community in recent years such as speech recognition, computer vision, language processing, and information retrieval [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 33" title="D Yu, L Deng, Deep learning and its applications to signal and information processing. IEEE Signal Proc Mag 28(1), 145–154 (2011)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR33" id="ref-link-section-d69318e969">33</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 38" title="GE Dahl, D Yu, L Deng, A Acero, Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition. IEEE Trans Audio Speech Lang Proc 20(1), 30–42 (2012)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR38" id="ref-link-section-d69318e972">38</a>–<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 40" title="DC Ciresan, U Meier, LM Gambardella, J Schmidhuber, Deep, big, simple neural nets for handwritten digit recognition. Neural Comput 22(12), 3207–3220 (2010)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR40" id="ref-link-section-d69318e975">40</a>]. As the data keeps getting bigger, deep learning is coming to play a pivotal role in providing predictive analytics solutions for large-scale data sets, particularly with the increased processing power and the advances in graphics processors [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 13" title="XW Chen, X Lin, Big data deep learning: challenges and perspectives. IEEE Access 2, 514–525 (2014)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR13" id="ref-link-section-d69318e978">13</a>]. For example, IBM’s brain-like computer [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 22" title="N Jones, Computer science: the learning machines. Nature 505(7482), 146–148 (2014)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR22" id="ref-link-section-d69318e981">22</a>] and Microsoft’s real-time language translation in Bing voice search [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 41" title="Y Wang, D Yu, Y Ju, A Acero, Voice search, in Language understanding: systems for extracting semantic information from speech (Wiley, New York, 2011)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR41" id="ref-link-section-d69318e985">41</a>] have used techniques like deep learning to leverage big data for competitive advantage.</p>
                        
                      </li>
                      <li>
                        <span class="u-custom-list-number">3.</span>
                        
                          <p>
                                          <i>Distributed and parallel learning</i>: There is often exciting information hidden in the unprecedented volumes of data. Learning from these massive data is expected to bring significant science and engineering advances which can facilitate the development of more intelligent systems. However, a bottleneck preventing such a big blessing is the inability of learning algorithms to use all the data to learn within a reasonable time. In this context, distributed learning seems to be a promising research since allocating the learning process among several workstations is a natural way of scaling up learning algorithms [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 42" title="D Peteiro-Barral, B Guijarro-Berdiñas, A survey of methods for distributed machine learning. Progress in Artificial Intelligence 2(1), 1–11 (2012)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR42" id="ref-link-section-d69318e1002">42</a>]. Different from the classical learning framework, in which one requires the collection of that data in a database for central processing, in the framework of distributed learning, the learning is carried out in a distributed manner [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 43" title="H Zheng, SR Kulkarni, HV Poor, Attribute-distributed learning: models, limits, and algorithms. IEEE Trans Signal Process 59(1), 386–398 (2011)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR43" id="ref-link-section-d69318e1005">43</a>].</p>
                          <p>In the past years, several popular distributed machine learning algorithms have been proposed, including decision rules [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 44" title="H Chen, T Li, C Luo, SJ Horng, G Wang, A rough set-based method for updating decision rules on attribute values’ coarsening and refining. IEEE Trans Knowl Data Eng 26(12), 2886–2899 (2014)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR44" id="ref-link-section-d69318e1011">44</a>], stacked generalization [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 45" title="J Chen, C Wang, R Wang, Using stacked generalization to combine SVMs in magnitude and shape feature spaces for classification of hyperspectral data. IEEE Trans Geosci Remote 47(7), 2193–2205 (2009)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR45" id="ref-link-section-d69318e1014">45</a>], meta-learning [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 46" title="E Leyva, A González, R Pérez, A set of complexity measures designed for applying meta-learning to instance selection. IEEE Trans Knowl Data Eng 27(2), 354–367 (2014)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR46" id="ref-link-section-d69318e1017">46</a>], and distributed boosting [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 47" title="M Sarnovsky, M Vronc, Distributed boosting algorithm for classification of text documents, in Proceedings of the 12th IEEE International Symposium on Applied Machine Intelligence and Informatics (SAMI) (Herl'any, 2014), pp. 217–220" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR47" id="ref-link-section-d69318e1020">47</a>]. With the advantage of distributed computing for managing big volumes of data, distributed learning avoids the necessity of gathering data into a single workstation for central processing, saving time and energy. It is expected that more widespread applications of the distributed learning are on the way [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 42" title="D Peteiro-Barral, B Guijarro-Berdiñas, A survey of methods for distributed machine learning. Progress in Artificial Intelligence 2(1), 1–11 (2012)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR42" id="ref-link-section-d69318e1023">42</a>]. Similar to distributed learning, another popular learning technique for scaling up traditional learning algorithms is parallel machine learning [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 48" title="SR Upadhyaya, Parallel approaches to machine learning—a comprehensive survey. J Parallel Distr Com 73(3), 284–292 (2013)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR48" id="ref-link-section-d69318e1027">48</a>]. With the power of multicore processors and cloud computing platforms, parallel and distributed computing systems have recently become widely accessible [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 42" title="D Peteiro-Barral, B Guijarro-Berdiñas, A survey of methods for distributed machine learning. Progress in Artificial Intelligence 2(1), 1–11 (2012)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR42" id="ref-link-section-d69318e1030">42</a>]. A more detailed description about distributed and parallel learning can be found in [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 49" title="R Bekkerman, M Bilenko, J Langford, Scaling up machine learning: parallel and distributed approaches (Cambridge University Press, Oxford, 2011)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR49" id="ref-link-section-d69318e1033">49</a>].</p>
                        
                      </li>
                      <li>
                        <span class="u-custom-list-number">4.</span>
                        
                          <p>
                                          <i>Transfer learning</i>: A major assumption in many traditional machine learning algorithms is that the training and test data are drawn from the same feature space and have the same distribution. However, with the data explosion from variety of sources, great heterogeneity of the collected data destroys the hypothesis. To tackle this issue, transfer learning has been proposed to allow the domains, tasks, and distributions to be different, which can extract knowledge from one or more source tasks and apply the knowledge to a target task [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 50" title="EW Xiang, B Cao, DH Hu, Q Yang, Bridging domains using world wide knowledge for transfer learning. IEEE Trans Knowl Data Eng 22(6), 770–783 (2010)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR50" id="ref-link-section-d69318e1050">50</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 51" title="SJ Pan, Q Yang, A survey on transfer learning. IEEE Trans Knowl Data Eng 22(10), 1345–1359 (2010)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR51" id="ref-link-section-d69318e1053">51</a>]. The advantage of transfer learning is that it can intelligently apply knowledge learned previously to solve new problems faster.</p>
                          <p>Based on different situations between the source and target domains and tasks, transfer learning is categorized into three subsettings: inductive transfer learning, transductive transfer learning, and unsupervised transfer learning [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 51" title="SJ Pan, Q Yang, A survey on transfer learning. IEEE Trans Knowl Data Eng 22(10), 1345–1359 (2010)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR51" id="ref-link-section-d69318e1059">51</a>]. In terms of inductive transfer learning, the source and target tasks are different, no matter when the source and target domains are the same or not. Transductive transfer learning, in contrast, the target domain is different from the source domain, while the source and target tasks are the same. Finally, in the unsupervised transfer learning setting, the target task is different from but related to the source task. Furthermore, approaches to transfer learning in the above three different settings can be classified into four contexts based on “What to transfer,” such as the instance transfer approach, the feature representation transfer approach, the parameter transfer approach, and the relational knowledge transfer approach [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 51" title="SJ Pan, Q Yang, A survey on transfer learning. IEEE Trans Knowl Data Eng 22(10), 1345–1359 (2010)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR51" id="ref-link-section-d69318e1062">51</a>–<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 54" title="C Wang, S Mahadevan, Manifold alignment using procrustes analysis, in Proceedings of the 25th International Conference on Machine Learning (ICML) (Helsinki, 2008), pp. 1120–1127" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR54" id="ref-link-section-d69318e1065">54</a>]. Recently, transfer learning techniques have been applied successfully in many real-world data processing applications, such as cross-domain text classification, constructing informative priors, and large-scale document classification [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 55" title="X Ling, W Dai, GR Xue, Q Yang, Y Yu, Spectral domain-transfer learning, in Proceedings of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (Las Vegas, 2008), pp. 488–496" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR55" id="ref-link-section-d69318e1068">55</a>–<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 57" title=" J Zhang, Deep transfer learning via restricted Boltzmann machine for document classification, in Proceedings of the 10th International Conference on Machine Learning and Applications and Workshops (ICMLA) (Honolulu, 2011), pp. 323–326" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR57" id="ref-link-section-d69318e1071">57</a>].</p>
                        
                      </li>
                      <li>
                        <span class="u-custom-list-number">5.</span>
                        
                          <p>
                                          <i>Active learning</i>: In many real-world applications, we have to face such a situation: data may be abundant but labels are scarce or expensive to obtain. Frequently, learning from massive amounts of unlabeled data is difficult and time-consuming. Active learning attempts to address this issue by selecting a subset of most critical instances for labeling [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 58" title="Y Fu, B Li, X Zhu, C Zhang, Active learning without knowing individual instance labels: a pairwise label homogeneity query approach. IEEE Trans Knowl Data Eng 26(4), 808–822 (2014)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR58" id="ref-link-section-d69318e1088">58</a>]. In this way, the active learner aims to achieve high accuracy using as few labeled instances as possible, thereby minimizing the cost of obtaining labeled data [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 59" title="B Settles, Active learning literature survey (University of Wisconsin, Madison, 2010)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR59" id="ref-link-section-d69318e1091">59</a>]. It can obtain satisfactory classification performance with fewer labeled samples via query strategies than those of conventional passive learning [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 60" title="MM Crawford, D Tuia, HL Yang, Active learning: any value for classification of remotely sensed data? P IEEE 101(3), 593–608 (2013)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR60" id="ref-link-section-d69318e1094">60</a>].</p>
                          <p>There are three main active learning scenarios, comprising membership query synthesis, stream-based selective sampling and pool-based sampling [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 59" title="B Settles, Active learning literature survey (University of Wisconsin, Madison, 2010)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR59" id="ref-link-section-d69318e1100">59</a>]. Popular active learning approaches can be found in [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 61" title="MM Haque, LB Holder, MK Skinner, DJ Cook, Generalized query-based active learning to identify differentially methylated regions in DNA. IEEE ACM Trans Comput Bi 10(3), 632–644 (2013)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR61" id="ref-link-section-d69318e1103">61</a>]. They have been studied extensively in the field of machine learning and applied to many data processing problems such as image classification and biological DNA identification [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 61" title="MM Haque, LB Holder, MK Skinner, DJ Cook, Generalized query-based active learning to identify differentially methylated regions in DNA. IEEE ACM Trans Comput Bi 10(3), 632–644 (2013)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR61" id="ref-link-section-d69318e1106">61</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 62" title="D Tuia, M Volpi, L Copa, M Kanevski, J Munoz-Mari, A survey of active learning algorithms for supervised remote sensing image classification. IEEE J Sel Top Sign Proces 5(3), 606–617 (2011)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR62" id="ref-link-section-d69318e1109">62</a>].</p>
                        
                      </li>
                      <li>
                        <span class="u-custom-list-number">6.</span>
                        
                          <p>
                                          <i>Kernel-based learning</i>: Over the last decade, kernel-based learning has established itself as a very powerful technique to increase the computational capability based on a breakthrough in the design of efficient nonlinear learning algorithms [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 63" title="G Ding, Q Wu, YD Yao, J Wang, Y Chen, Kernel-based learning for statistical signal processing in cognitive radio networks. IEEE Signal Proc Mag 30(4), 126–136 (2013)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR63" id="ref-link-section-d69318e1127">63</a>]. The outstanding advantage of kernel methods is their elegant property of implicitly mapping samples from the original space into a potentially infinite-dimensional feature space, in which inner products can be calculated directly via a kernel function [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 64" title="C Li, M Georgiopoulos, GC Anagnostopoulos, A unifying framework for typical multitask multiple kernel learning problems. IEEE Trans Neur Net Lear Syst 25(7), 1287–1297 (2014)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR64" id="ref-link-section-d69318e1130">64</a>]. For example, in kernel-based learning theory, data <i>x</i> in the input space <span class="mathjax-tex">\( \mathcal{X} \)</span> is projected onto a potentially much higher dimensional feature space <span class="mathjax-tex">\( \mathcal{F} \)</span> via a nonlinear mapping Φ as follows:</p><div id="Equ1" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ \varPhi :\kern0.3em \mathcal{X}\to \mathrm{\mathcal{F}},\kern0.3em \mathrm{x}\mapsto \varPhi \left(\mathrm{x}\right) $$</span></div><div class="c-article-equation__number">
                    (1)
                </div></div>
                                       
                          <p>In this context, for a given learning problem, one now works with the mapped data Φ(x) ∈ ℱ instead of <span class="mathjax-tex">\( \mathrm{x}\in \mathcal{X} \)</span> [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 63" title="G Ding, Q Wu, YD Yao, J Wang, Y Chen, Kernel-based learning for statistical signal processing in cognitive radio networks. IEEE Signal Proc Mag 30(4), 126–136 (2013)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR63" id="ref-link-section-d69318e1256">63</a>]. The data in the input space can be projected onto different feature spaces with different mappings. The diversity of feature spaces gives us more choices to gain better performance, while in practice, the choice itself of a proper mapping for any given real-world problem may generally be nontrivial. Fortunately, the kernel trick provides an elegant mathematical means to construct powerful nonlinear variants of most well-known statistical linear techniques, without knowing the mapping explicitly. Indeed, one only needs to replace the inner product operator of a linear technique with an appropriate kernel function <i>k</i> (i.e., a positive semi-definite symmetric function), which arises as a similarity measure that can be thought as an inner product between pairs of data in the feature space. Here, the original nonlinear problem can be transformed into a linear formulation in a higher dimensional space ℱ with an appropriate kernel <i>k</i> [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 65" title="G Montavon, M Braun, T Krueger, KR Muller, Analyzing local structure in kernel-based learning: explanation, complexity, and reliability assessment. IEEE Signal Proc Mag 30(4), 62–74 (2013)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR65" id="ref-link-section-d69318e1265">65</a>]:</p><div id="Equ2" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ k\left(\mathrm{x},\kern0.2em {\mathrm{x}}^{\prime}\right)={\left\langle \Phi \left(\mathrm{x}\right),\kern0.2em \Phi \left({\mathrm{x}}^{\prime}\right)\kern0.2em \right\rangle}_{\mathcal{F}},\forall \mathrm{x},\kern0.2em {\mathrm{x}}^{\prime}\in \mathcal{X} $$</span></div><div class="c-article-equation__number">
                    (2)
                </div></div>
                                       
                          <p>The most widely used kernel functions include Gaussian kernels and Polynomial kernels. These kernels implicitly map the data onto high-dimensional spaces, even infinite-dimensional spaces [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 63" title="G Ding, Q Wu, YD Yao, J Wang, Y Chen, Kernel-based learning for statistical signal processing in cognitive radio networks. IEEE Signal Proc Mag 30(4), 126–136 (2013)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR63" id="ref-link-section-d69318e1383">63</a>]. Kernel functions provide the nonlinear means to infuse correlation or side information in big data, which can obtain significant performance improvement over their linear counterparts at the price of generally higher computational complexity. Moreover, for a specific problem, the selection of the best kernel function is still an open issue, although ample experimental evidence in the literature supports that the popular kernel functions such as Gaussian kernels and polynomial kernels perform well in most cases.</p>
                          <p>At the root of the success of kernel-based learning, the combination of high expressive power with the possibility to perform the numerous analyses has been developed in many challenging applications [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 65" title="G Montavon, M Braun, T Krueger, KR Muller, Analyzing local structure in kernel-based learning: explanation, complexity, and reliability assessment. IEEE Signal Proc Mag 30(4), 62–74 (2013)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR65" id="ref-link-section-d69318e1389">65</a>], e.g., online classification [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 66" title="K Slavakis, S Theodoridis, I Yamada, Online kernel-based classification using adaptive projection algorithms. IEEE Trans Signal Process 56(7), 2781–2796 (2008)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR66" id="ref-link-section-d69318e1392">66</a>], convexly constrained parameter/function estimation [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 67" title="S Theodoridis, K Slavakis, I Yamada, Adaptive learning in a world of projections. IEEE Signal Proc Mag 28(1), 97–123 (2011)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR67" id="ref-link-section-d69318e1395">67</a>], beamforming problems [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 68" title="K Slavakis, S Theodoridis, I Yamada, Adaptive constrained learning in reproducing kernel Hilbert spaces: the robust beamforming case. IEEE Trans Signal Process 57(12), 4744–4764 (2009)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR68" id="ref-link-section-d69318e1398">68</a>], and adaptive multiregression [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 69" title="K Slavakis, P Bouboulis, S Theodoridis, Adaptive multiregression in reproducing kernel Hilbert spaces: the multiaccess MIMO channel case. IEEE Trans Neural Netw Learn Syst 23(2), 260–276 (2012)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR69" id="ref-link-section-d69318e1401">69</a>]. One of the most popular surveys about introducing kernel-based learning algorithms is [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 70" title="KR Müller, S Mika, G Rätsch, K Tsuda, B Schölkopf, An introduction to kernel-based learning algorithms. IEEE Trans Neural Networ 12(2), 181–201 (2001)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR70" id="ref-link-section-d69318e1405">70</a>], in which an introduction of the exciting field of kernel-based learning methods and applications was given.</p>
                        
                      </li>
                    </ol>
                           <h3 class="c-article__sub-heading u-h3" id="Sec6">The critical issues of machine learning for big data</h3><p>In spite of the recent achievement in machine learning is great as mentioned in Section <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#Sec3">1.2</a>, with the emergence of big data, much more needs to be done to address many significant challenges posted by big data. In this section, we present a discussion about the critical issues of machine learning techniques for big data from five different perspectives, as described in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#Fig2">2</a>, including learning for large scale of data, learning for different types of data, learning for high speed of streaming data, learning for uncertain and incomplete data, and learning for extracting valuable information from massive amounts of data. Also, corresponding possible remedies to surmount the obstacles in recent researches are introduced in the discussion.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2"><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 2</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-category="article body" data-track-label="image" data-track-action="view figure" href="/article/10.1186/s13634-016-0355-x/figures/2?shared-article-renderer" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1186%2Fs13634-016-0355-x/MediaObjects/13634_2016_355_Fig2_HTML.gif?as=webp"></source><img aria-describedby="figure-2-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1186%2Fs13634-016-0355-x/MediaObjects/13634_2016_355_Fig2_HTML.gif" alt="figure2" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p>The critical issues of machine learning for big data</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-category="article body" data-track-label="button" data-track-action="view figure" href="/article/10.1186/s13634-016-0355-x/figures/2?shared-article-renderer" data-track-dest="link:Figure2 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <h4 class="c-article__sub-heading u-h3 c-article__sub-heading--light" id="Sec7">Critical issue one: learning for large scale of data</h4>
                    <h3 class="c-article__sub-heading u-h3">
                      <i>Critical issue</i>
                    </h3>
                    <p>It is obvious that data <i>volume</i> is the primary attribute of big data, which presents a great challenge for machine learning. Taking only the digital data as an instance, every day, Google alone needs to process about 24 petabytes (petabyte = 2<sup>10</sup> × 2<sup>10</sup> × 2<sup>10</sup> × 2<sup>10</sup> × 2<sup>10</sup> bytes) of data [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 71" title="TH Davenport, P Barth, R Bean, How “big data” is different. MIT Sloan Manage Rev 54(1), 22–24 (2012)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR71" id="ref-link-section-d69318e1474">71</a>]. Moreover, if we further take into consideration other data sources, the data scale will become much bigger. Under current development trends, data stored and analyzed by big organizations will undoubtedly reach the petabyte to exabyte (exa byte = 2<sup>10</sup>petabytes) magnitude soon [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 6" title="H Hu, Y Wen, T Chua, X Li, Toward scalable systems for big data analytics: a technology tutorial. IEEE Access 2, 652–687 (2014)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR6" id="ref-link-section-d69318e1479">6</a>].</p>
                  
                    <h3 class="c-article__sub-heading u-h3">
                      <i>Possible remedies</i>
                    </h3>
                    <p>There is no doubt that we are now swimming in an expanding sea of data that is too voluminous to train a machine learning algorithm with a central processor and storage. Instead, <i>distributed frameworks with parallel computing</i> are preferred. Alternating direction method of multipliers (ADMM) [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 72" title="F Andersson, M Carlsson, JY Tourneret, H Wendt, A new frequency estimation method for equally and unequally spaced data. IEEE Trans Signal Process 62(21), 5761–5774 (2014)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR72" id="ref-link-section-d69318e1497">72</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 73" title="F Lin, M Fardad, MR Jovanovic, Design of optimal sparse feedback gains via the alternating direction method of multipliers. IEEE Trans Automat Contr 58(9), 2426–2431 (2013)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR73" id="ref-link-section-d69318e1500">73</a>] serving as a promising computing framework to develop distributed, scalable, online convex optimization algorithms is well suited to accomplish parallel and distributed large-scale data processing. The key merits of ADMM is its ability to split or decouple multiple variables in optimization problems, which enables one to find a solution to a large-scale global optimization problem by coordinating solutions to smaller sub-problems. Generally, ADMM is convergent for convex optimization, but it is lack of convergence and theoretical performance guarantees for nonconvex optimization. However, vast experimental evidence in the literature supports empirical convergence and good performance of ADMM [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 74" title="S Boyd, N Parikh, E Chu, B Peleato, J Eckstein, Distributed optimization and statistical learning via the alternating direction method of multipliers. Foundations Trends Mach Learn 3(1), 1–122 (2011)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR74" id="ref-link-section-d69318e1503">74</a>]. A wide variety of applications of ADMM to machine learning problems for large-scale datasets have been discussed in [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 74" title="S Boyd, N Parikh, E Chu, B Peleato, J Eckstein, Distributed optimization and statistical learning via the alternating direction method of multipliers. Foundations Trends Mach Learn 3(1), 1–122 (2011)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR74" id="ref-link-section-d69318e1506">74</a>].</p>
                    <p>In addition to distributed theoretical framework for machine learning to mitigate the challenges related to high volumes, some practicable <i>parallel programming methods</i> are also proposed and applied to learning algorithms to deal with large-scale data sets. <i>MapReduce</i> [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 75" title="J Dean, S Ghemawat, MapReduce: simplified data processing on large clusters. Commun ACM 51(1), 107–113 (2008)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR75" id="ref-link-section-d69318e1518">75</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 76" title="J Dean, S Ghemawat, MapReduce: a flexible data processing tool. Commun ACM 53(1), 72–77 (2010)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR76" id="ref-link-section-d69318e1521">76</a>], a powerful programming framework, enables the automatic paralleling and distribution of computation applications on large clusters of commodity machines. What is more, MapReduce can also provide great fault tolerance ability, which is important for tackling the large data sets. The core idea of MapReduce is to divide massive data into small chunks firstly, then, deal with these chunks in parallel and in a distributed manner to generate intermediate results. By aggregating all the intermediate results, the final result is derived. A general means of programming machine learning algorithms on multicore with the advantage of MapReduce has been investigated in [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 77" title="C Chu, SK Kim, YA Lin, Y Yu, G Bradski, AY Ng, K Olukotun, Map-reduce for machine learning on multicore, in Proceedings of 20th Annual Conference on Neural Information Processing Systems (NIPS) (Vancouver, 2006), pp. 281–288" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR77" id="ref-link-section-d69318e1524">77</a>]. Cloud-computing-assisted learning method is another impressive progress which has been made for data systems to deal with the volume challenge of big data. <i>Cloud computing</i> [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 78" title="M Armbrust, A Fox, R Griffith, AD Joseph, R Katz, A Konwinski, G Lee, D Patterson, A Rabkin, I Stoica, M Zaharia, A view of cloud computing. Commun ACM 53(4), 50–58 (2010)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR78" id="ref-link-section-d69318e1531">78</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 79" title="MD Dikaiakos, D Katsaros, P Mehra, G Pallis, A Vakali, Cloud computing: distributed internet computing for IT and scientific research. IEEE Internet Comput 13(5), 10–13 (2009)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR79" id="ref-link-section-d69318e1534">79</a>] has already demonstrated admirable elasticity that bears the hope of realizing the needed scalability for machine learning algorithms. It can enhance computing and storage capacity through cloud infrastructure. In this context, <i>distributed GraphLab</i>, a framework for machine learning in the cloud, has been proposed in [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 80" title="Y Low, D Bickson, J Gonzalez, C Guestrin, A Kyrola, JM Hellerstein, Distributed GraphLab: a framework for machine learning and data mining in the cloud. Proc VLDB Endow 5(8), 716–727 (2012)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR80" id="ref-link-section-d69318e1540">80</a>].</p>
                  <h4 class="c-article__sub-heading u-h3 c-article__sub-heading--light" id="Sec8">Critical issue two: learning for different types of data</h4>
                    <h3 class="c-article__sub-heading u-h3">
                      <i>Critical issue</i>
                    </h3>
                    <p>The enormous <i>variety</i> of data is the second dimension that makes big data both interesting and challenging. This is resulted from the phenomenon that data generally come from various sources and are of different types. Structured, semi-structured, and even entirely unstructured data sources stimulate the generation of <i>heterogeneous</i>, <i>high-dimensional</i>, <i>and nonlinear</i> data with different representation forms. Learning with such a dataset, the great challenge is perceivable and the degree of complexity is not even imaginable before we deeply get there.</p>
                  
                    <h3 class="c-article__sub-heading u-h3">
                      <i>Possible remedies</i>
                    </h3>
                    <p>In terms of heterogeneous data, <i>data integration</i> [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 81" title="M Lenzerini, Data integration: a theoretical perspective, in Proceedings of the twenty-first ACM SIGMOD-SIGACT-SIGART Symposium on Principles of Database Systems (Madison, 2002), pp. 233–246" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR81" id="ref-link-section-d69318e1587">81</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 82" title="A Halevy, A Rajaraman, J Ordille, Data integration: the teenage years, in Proceedings of the 32nd International Conference on Very Large Data Bases (VLDB) (Seoul, 2006), pp. 9–16" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR82" id="ref-link-section-d69318e1590">82</a>], which aims to combine data residing at different sources and provide the user with a unified view of these data, is a key method. An effect solution to address the data integration problem is to learn good data representations from each individual data source and then to integrate the learned features at different levels [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 13" title="XW Chen, X Lin, Big data deep learning: challenges and perspectives. IEEE Access 2, 514–525 (2014)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR13" id="ref-link-section-d69318e1593">13</a>]. Thus, representation learning is preferred in this issue. In [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 83" title="Q Wu, G Ding, J Wang, YD Yao, Spatial-temporal opportunity detection for spectrum-heterogeneous cognitive radio networks: two-dimensional sensing. IEEE Trans Wirel Commun 12(2), 516–526 (2013)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR83" id="ref-link-section-d69318e1596">83</a>], the authors proposed a data fusion theory based on statistical learning for the two-dimensional spectrum heterogeneous data. In addition, deep learning methods have also been shown to be very effective in integrating data from different sources. For example, Srivastava and Salakhutdinov [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 84" title="N Srivastava, RR Salakhutdinov, Multimodal learning with deep boltzmann machines, in Proceedings of Neural Information Processing Systems Conference (NIPS) (Nevada, 2012), pp. 2222–2230" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR84" id="ref-link-section-d69318e1600">84</a>] developed a novel application of deep learning algorithms to learn a unified representation by integrating real-valued dense image data and text data.</p>
                    <p>Another challenge associated with high variety is that the data are often high dimensional and nonlinear, such as global climate patterns, stellar spectra, and human gene distributions. Clearly, to deal with high-dimensional data, <i>dimensionality reduction</i> is an effective solution through finding meaningful low-dimensional structures hidden in their high-dimensional observations. Common approaches are to employ feature selection or extraction to reduce the data dimensions. For example, Sun et al. [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 85" title="Y Sun, S Todorovic, S Goodison, Local-learning-based feature selection for high-dimensional data analysis. IEEE Trans Pattern Anal Mach Intell 32(9), 1610–1626 (2010)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR85" id="ref-link-section-d69318e1609">85</a>] proposed a local-learning-based feature selection algorithm for high-dimensional data analysis. The existing typical machine learning algorithms for data dimensionality reduction include principal component analysis (PCA), linear discriminant analysis (LDA), locally linear embedding(LLE), and laplacian Eigenmaps [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 86" title="LJP van der Maaten, EO Postma, HJ van den Herik, Dimensionality reduction: a comparative review. J Mach Learn Res 10(1-41), 66–71 (2009)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR86" id="ref-link-section-d69318e1612">86</a>]. Most recently, <i>low-rank matrix</i> plays a more and more central role in large-scale data analysis and dimensionality reduction [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 8" title="Q Wu, G Ding, Y Xu, S Feng, Z Du, J Wang, K Long, Cognitive internet of things: a new paradigm beyond connection. IEEE Internet Things J 1(2), 129–143 (2014)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR8" id="ref-link-section-d69318e1618">8</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 87" title="M Mardani, G Mateos, GB Giannakis, Subspace learning and imputation for streaming big data matrices and tensors. IEEE Trans Signal Process 63(10), 2663–2677 (2015)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR87" id="ref-link-section-d69318e1622">87</a>]. The problem of recovering a low-rank matrix is a fundamental problem with applications in machine learning [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 88" title=" K Mohan, M Fazel, New restricted isometry results for noisy low-rank recovery, in Proceedings of IEEE International Symposium on Information Theory Proceedings (ISIT) (Texas, 2010), pp. 1573–1577" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR88" id="ref-link-section-d69318e1625">88</a>]. Here, we provide a simple example of using low-rank matrix recovery algorithms for high-dimensional data processing. Let us assume that we are given a large data matrix <b>N</b> and know that it may be decomposed as <b>N</b> = <b>M</b> + <b>Λ</b>, where <b>M</b> has low rank and Λ is a noise matrix. Due to the low-dimensional column or row space of <b>M</b>, not even their dimensions are not known, it is necessary to recover the matrix <b>M</b> from the data matrix <b>N</b> and the problem can be formulated as classical PCA [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 8" title="Q Wu, G Ding, Y Xu, S Feng, Z Du, J Wang, K Long, Cognitive internet of things: a new paradigm beyond connection. IEEE Internet Things J 1(2), 129–143 (2014)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR8" id="ref-link-section-d69318e1653">8</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 89" title="EJ Candès, X Li, Y Ma, J Wright, Robust principal component analysis? J ACM 58(3), 1–37 (2011)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR89" id="ref-link-section-d69318e1656">89</a>]:</p><div id="Equ3" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ \begin{array}{l}\underset{\left\{\mathbf{M}\right\}}{ \min }{\left\Vert \mathbf{M}\right\Vert}_{*}\kern1.3em \\ {}\kern0.2em s.t.\kern0.5em {\left\Vert \mathbf{N}-\mathbf{M}\right\Vert}_F\le \varepsilon \end{array} $$</span></div><div class="c-article-equation__number">
                    (3)
                </div></div>
                              
                  <p>where <i>ε</i> is a noise related parameter, ‖ ⋅ ‖<sub>*</sub> and ‖ ⋅ ‖<sub>
                      <i>F</i>
                    </sub> is defined by the nuclear norm and the Frobenious norm of a matrix, respectively. The problem formulated in (3) shows the fundamental task of the research on matrix recovery for high-dimensional data processing, which can be efficiently solved by some existing algorithms including augmented Lagrange multipliers (ALM) algorithm and accelerated proximal gradient (APG) algorithm [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 90" title="Z Lin, R Liu, Z Su, Linearized alternating direction method with adaptive penalty for low-rank representation, in Proceedings of Neural Information Processing Systems Conference (NIPS) (Granada, 2011), pp. 612–620" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR90" id="ref-link-section-d69318e1769">90</a>]. As for nonlinear properties of data related to high variety, kernel-based learning methods can provide commendable solutions which have been discussed in Section <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#Sec5">1.2.2</a>; thus, the repetitious details will not be given here. Of course, in terms of challenges brought by different types, transfer learning is also a very good choice owning to its powerful knowledge transfer ability which enables multidomain learning to be possible.</p><h4 class="c-article__sub-heading u-h3 c-article__sub-heading--light" id="Sec9">Critical issue three: learning for high speed of streaming data</h4>
                    <h3 class="c-article__sub-heading u-h3">
                      <i>Critical issue</i>
                    </h3>
                    <p>For big data, speed or <i>velocity</i> really matters, which is another emerging challenge for learning. In many real-world applications, we have to finish a task within a certain period of time; otherwise, the processing results become less valuable or even worthless, such as earthquake prediction, stock market prediction and agent-based autonomous exchange (buying/selling) systems, and so on. In these time-sensitive cases, the potential value of data depends on data freshness that needs to be processed in a real-time manner.</p>
                  
                    <h3 class="c-article__sub-heading u-h3">
                      <i>Possible remedies</i>
                    </h3>
                    <p>One promising solution for learning from such high speed of data is online learning approaches. Online learning [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 91" title="S Shalev-Shwartz, Online learning and online convex optimization. Foundations Trends Mach Learn 4, 107–194 (2011)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR91" id="ref-link-section-d69318e1806">91</a>–<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 94" title="M Bilenko, S Basil, M Sahami, Adaptive product normalization: using online learning for record linkage in comparison shopping, in Proceedings of the 5th IEEE International Conference on Data Mining (ICDM) (Texas, 2005), p. 8" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR94" id="ref-link-section-d69318e1809">94</a>] is a well-established learning paradigm whose strategy is learning one instance at a time, instead of in an offline or batch learning fashion, which needs to collect the full information of training data. This sequential learning mechanism works well for big data as current machines cannot hold the entire dataset in memory. To speed up learning, recently, a novel learning algorithm for single hidden-layer feed forward neural networks (SLFNs) named extreme learning machine (ELM) [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 95" title="GB Huang, QY Zhu, CK Siew, Extreme learning machine: theory and applications. Neurocomputing 70(1), 489–501 (2006)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR95" id="ref-link-section-d69318e1812">95</a>] was proposed. Compared with some other traditional learning algorithms, ELM provides extremely faster learning speed, better generalization performance, and with least human intervention [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 96" title="S Ding, X Xu, R Nie, Extreme learning machine and its applications. Neural Comput Appl 25(3-4), 549–556 (2014)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR96" id="ref-link-section-d69318e1815">96</a>]. Thus, ELM has strong advantages in dealing with high velocity of data.</p>
                    <p>Another challenging issue associated with the high velocity is that data are often nonstationary [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 13" title="XW Chen, X Lin, Big data deep learning: challenges and perspectives. IEEE Access 2, 514–525 (2014)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR13" id="ref-link-section-d69318e1821">13</a>], i.e., data distribution is changing over time, which needs the learning algorithms to learn the data as a stream. To tackle this problem, the potential superiority of streaming processing theory and technology [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 97" title="N Tatbul, Streaming data integration: challenges and opportunities, in Proceedings of the 26th IEEE International Conference on Data Engineering Workshops (ICDEW) (Long Beach, 2010), pp. 155–158" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR97" id="ref-link-section-d69318e1824">97</a>] have been found out compared with batch-processing paradigm, as they aim to analyze data as soon as possible to derive its results. Representative streaming processing systems include Borealis [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 98" title=" DJ Abadi, Y Ahmad, M Balazinska, U Cetintemel, M Cherniack, JH Hwang, W Lindner, A Maskey, A Rasin, E Ryvkina, N Tatbul, Y Xing, SB Zdonik, The design of the borealis stream processing engine, in Proceedings of the Second Biennial Conference on Innovative Data Systems Research (CIDR) (Asilomar, 2005), pp. 277–289" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR98" id="ref-link-section-d69318e1827">98</a>], S4 [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 99" title="L Neumeyer, B Robbins, A Nair, A Kesari, S4: Distributed stream computing platform, in Proceedings of IEEE International Conference on Data Mining Workshops (ICDMW) (Sydney, 2010), pp. 170–177" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR99" id="ref-link-section-d69318e1830">99</a>], Kafka [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 100" title="K Goodhope, J Koshy, J Kreps, N Narkhede, R Park, J Rao, VY Ye, Building Linkedin’s real-time activity data pipeline. IEEE Data Eng Bull 35(2), 33–45 (2012)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR100" id="ref-link-section-d69318e1833">100</a>], and many other recent architectures proposed to provide real-time analytics over big data [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 101" title="W Yang, X Liu, L Zhang, LT Yang, Big data real-time processing based on storm, in Proceedings of the 12th IEEE International Conference on Trust, Security and Privacy in Computing and Communications (TrustCom) (Melbourne, 2013), pp. 1784–1787" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR101" id="ref-link-section-d69318e1837">101</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 102" title="B SkieS, Streaming big data processing in datacenter clouds. IEEE Cloud Comput 1, 78–83 (2014)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR102" id="ref-link-section-d69318e1840">102</a>]. A scalable machine learning online service with the power of streaming processing for big data real-time analysis is introduced in [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 103" title="A Baldominos, E Albacete, Y Saez, P Isasi, A scalable machine learning online service for big data real-time analysis, in Proceedings of IEEE Symposium on Computational Intelligence in Big Data (CIBD) (Orlando, 2014), pp. 1–8" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR103" id="ref-link-section-d69318e1843">103</a>]. In addition, the professor G. B. Giannakis have paid more attention to the real-time processing of streaming data by using machine learning techniques in recent studies; more details can be referred to in [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 87" title="M Mardani, G Mateos, GB Giannakis, Subspace learning and imputation for streaming big data matrices and tensors. IEEE Trans Signal Process 63(10), 2663–2677 (2015)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR87" id="ref-link-section-d69318e1846">87</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 104" title="NY Soltani, SJ Kim, GB Giannakis, Real-time load elasticity tracking and pricing for electric vehicle charging. IEEE Trans Smart Grid 6(3), 1303–1313 (2014)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR104" id="ref-link-section-d69318e1849">104</a>].</p>
                  <h4 class="c-article__sub-heading u-h3 c-article__sub-heading--light" id="Sec10">Critical issue four: learning for uncertain and incomplete data</h4>
                    <h3 class="c-article__sub-heading u-h3">
                      <i>Critical issue</i>
                    </h3>
                    <p>In the past, machine learning algorithms were typically fed with relatively accurate data from well-known and quite limited sources, so the learning results tend to be unerring, too; thus, veracity has never been a serious issue for concern. However, with the sheer size of data available today, the precision and trust of the source data quickly become an issue, due to the data sources are often of many different origins and data quality is not all verifiable. Therefore, we include <i>veracity</i> as the fourth critical issue for learning with big data to emphasize the importance of addressing and managing the uncertainty and incompleteness on data quality.</p>
                  
                    <h3 class="c-article__sub-heading u-h3">
                      <i>Possible remedies</i>
                    </h3>
                    <p>Uncertain data are a special type of data reality where data readings and collections are no longer deterministic but are subject to some random or probability distributions. In many applications, data uncertainty is common. For example, in wireless networks, some spectrum data are inherently uncertain resulted from ubiquitous noise, fading, and shadowing and the technology barrier of the GPS sensor equipment also limits the accuracy of the data to certain levels. For uncertain data, the major challenge is that the data feature or attribute is captured not by a single point value but represented as sample distributions [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 11" title="X Wu, X Zhu, G Wu, W Ding, Data mining with big data. IEEE Trans Knowl Data Eng 26(1), 97–107 (2014)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR11" id="ref-link-section-d69318e1885">11</a>]. A simple way to handle data uncertainty is to apply summary statistics such as means and variances to abstract sample distributions. Another approach is to utilize the complete information carried by the probability distributions to construct a decision tree, which is called distribution-based approach in [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 105" title="S Tsang, B Kao, KY Yip, WS Ho, SD Lee, Decision trees for uncertain data. IEEE Trans Knowl Data Eng 23(1), 64–78 (2011)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR105" id="ref-link-section-d69318e1888">105</a>]. In [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 105" title="S Tsang, B Kao, KY Yip, WS Ho, SD Lee, Decision trees for uncertain data. IEEE Trans Knowl Data Eng 23(1), 64–78 (2011)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR105" id="ref-link-section-d69318e1891">105</a>], the authors firstly discussed the sources of data uncertainty and gave some examples and then devised an algorithm for building decision trees from uncertain data using the distribution-based approach. At last, a theoretical foundation was established on which pruning techniques were derived which can significantly improve the computational efficiency of the distribution-based algorithms for uncertain data.</p>
                    <p>The incomplete data problem, in which certain data field values or features are missing, exists in a wide range of domains with the emerging big data, which may be caused by different realities, such as data device malfunction. Learning from these imperfect data is a challenging task, due to most existing machine learning algorithms that cannot be directly applied. Taking classifier learning as an example, dealing with incomplete data is an important issue, since data incompleteness not only impacts interpretations of the data or the models created from the data but may also affect the prediction accuracy of learned classifiers. To tackle the challenges associated with data incompleteness, Chen and Lin [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 13" title="XW Chen, X Lin, Big data deep learning: challenges and perspectives. IEEE Access 2, 514–525 (2014)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR13" id="ref-link-section-d69318e1897">13</a>] investigated to apply the advanced deep learning methods to handle noisy data and tolerate some messiness. Furthermore, integrating the matrix completion technologies into machine learning to solve the problem of incomplete data is also a very promising direction [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 106" title="F Nie, H Wang, X Cai, H Huang, C Ding, Robust matrix completion via joint schatten p-norm and lp-norm minimization, in Proceedings of the 12th IEEE International Conference on Data Mining (ICDM) (Brussels, 2012), p. 566" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR106" id="ref-link-section-d69318e1900">106</a>]. In the following, we provide a case of using matrix completion for incomplete data processing. In this case, it is assumed that a noise matrix <b>Ỹ</b> is defined by</p><div id="Equ4" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ {\mathcal{P}}_{\varOmega}\left(\tilde{\mathbf{Y}}\right)={\mathcal{P}}_{\varOmega}\left(\mathbf{A}+\mathbf{Z}\right) $$</span></div><div class="c-article-equation__number">
                    (4)
                </div></div><p>where <b>A</b> is a sampled set of entries we would like to know as precisely as possible, <b>Z</b> is a noise term which may be stochastic or deterministic, <i>Ω</i> is the set of indices of the acquired entries, and <span class="mathjax-tex">\( {\mathcal{P}}_{\varOmega } \)</span> is the orthogonal projection onto the linear subspace of matrices supported on <i>Ω</i> [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 8" title="Q Wu, G Ding, Y Xu, S Feng, Z Du, J Wang, K Long, Cognitive internet of things: a new paradigm beyond connection. IEEE Internet Things J 1(2), 129–143 (2014)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR8" id="ref-link-section-d69318e2008">8</a>]. To recover the unknown matrix, the problem can be formulated as [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 8" title="Q Wu, G Ding, Y Xu, S Feng, Z Du, J Wang, K Long, Cognitive internet of things: a new paradigm beyond connection. IEEE Internet Things J 1(2), 129–143 (2014)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR8" id="ref-link-section-d69318e2011">8</a>]:</p><div id="Equ5" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ \begin{array}{l}\underset{\left\{\mathbf{M}\right\}}{ \min }{\left\Vert \mathbf{A}\right\Vert}_{*}\kern1.3em \\ {}\kern0.2em s.t.\kern0.5em {\left\Vert {\mathcal{P}}_{\varOmega}\left(\mathbf{A}-\mathbf{Y}\right)\right\Vert}_F\le \varepsilon \end{array} $$</span></div><div class="c-article-equation__number">
                    (5)
                </div></div>
                              
                    <p>To efficiently solve the problem (5), existing algorithms have been explained in [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 90" title="Z Lin, R Liu, Z Su, Linearized alternating direction method with adaptive penalty for low-rank representation, in Proceedings of Neural Information Processing Systems Conference (NIPS) (Granada, 2011), pp. 612–620" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR90" id="ref-link-section-d69318e2127">90</a>] in detail. Furthermore, in terms of the abnormal data, the authors in [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 107" title="G Ding, J Wang, Q Wu, L Zhang, Y Zou, YD Yao, Y Chen, Robust spectrum sensing with crowd sensors. IEEE Trans Commun 62(9), 3129–3143 (2014)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR107" id="ref-link-section-d69318e2130">107</a>] also investigated to use the statistical learning theory of sparse matrix with data cleansing for the robust spectrum sensing.</p>
                  <h4 class="c-article__sub-heading u-h3 c-article__sub-heading--light" id="Sec11">Critical issue five: learning for data with low value density and meaning diversity</h4>
                    <h3 class="c-article__sub-heading u-h3">
                      <i>Critical issue</i>
                    </h3>
                    <p>In fact, by exploiting a variety of learning methods to analyze big datasets, the final purpose is to extract valuable information from massive amounts of data in the form of deep insight or commercial benefits. Therefore, <i>value</i> is also characterized as a salient feature of big data [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2" title="J Gantz, D Reinsel, Extracting value from chaos (EMC, Hopkinton, 2011)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR2" id="ref-link-section-d69318e2153">2</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 6" title="H Hu, Y Wen, T Chua, X Li, Toward scalable systems for big data analytics: a technology tutorial. IEEE Access 2, 652–687 (2014)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR6" id="ref-link-section-d69318e2156">6</a>]. However, to derive significant value from high volumes of data with a <i>low value density</i> is not straightforward. For example, the police often need to look through some surveillance videos to handle criminal cases. Unfortunately, a few valuable data frames are frequently hidden in a large amount of video sources.</p>
                  
                    <h3 class="c-article__sub-heading u-h3">
                      <i>Possible remedies</i>
                    </h3>
                    <p>To handle this challenge, knowledge discovery in databases (KDD) and data mining technologies [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 9" title="CW Tsai, CF Lai, MC Chiang, LT Yang, Data mining for internet of things: a survey. IEEE Commun Surv Tut 16(1), 77–97 (2014)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR9" id="ref-link-section-d69318e2174">9</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 11" title="X Wu, X Zhu, G Wu, W Ding, Data mining with big data. IEEE Trans Knowl Data Eng 26(1), 97–107 (2014)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR11" id="ref-link-section-d69318e2177">11</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 108" title="U Fayyad, G Piatetsky-Shapiro, P Smyth, From data mining to knowledge discovery in databases. AI Mag 17(3), 37–54 (1996)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR108" id="ref-link-section-d69318e2180">108</a>] come into play, for these technologies provide possible solutions to find out the required information hidden in the massive data. In [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 9" title="CW Tsai, CF Lai, MC Chiang, LT Yang, Data mining for internet of things: a survey. IEEE Commun Surv Tut 16(1), 77–97 (2014)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR9" id="ref-link-section-d69318e2183">9</a>], the authors reviewed studies on applying data mining and KDD technologies to the IoT. Particularly, utilizing clustering, classification, and frequent patterns technologies to mine value from massive data in IoT, from the perspective of infrastructures and from the perspective of services were discussed in detail. In [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 11" title="X Wu, X Zhu, G Wu, W Ding, Data mining with big data. IEEE Trans Knowl Data Eng 26(1), 97–107 (2014)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR11" id="ref-link-section-d69318e2186">11</a>], Wu et al. characterized the features of the big data revolution and proposed big data processing methods with machine learning and data mining algorithms.</p>
                    <p>Another challenging problem associated with the value of big data is the <i>diversity of data meaning</i>, i.e., the economic value of different data varies significantly, even the same data have different value if considering from different perspectives or contexts. Therefore, some new cognition-assisted learning technologies should be developed to make current learning systems more flexible and intelligent. The most dramatic example of such devices is IBM’s “Watson” [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 109" title="J Kelly III, S Hamm, Smart machines: IBM’s Watson and the era of cognitive computing (Columbia University Press, New York, 2013)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR109" id="ref-link-section-d69318e2195">109</a>], constructed with several subsystems that use different machine learning strategies with the great power of cognitive technologies to analyze the questions and arrive at the most likely answer. With the scientists’ ingenuity, it is possible for this system to excel at a game which requires both encyclopedic knowledge and lightning-quick recall. Some humanlike characteristics—learning, adapting, interacting, and understanding enable Watson to be smarter and gain more computing power to deal with complexity and big data. It is expected that the era of cognitive computing will come [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 109" title="J Kelly III, S Hamm, Smart machines: IBM’s Watson and the era of cognitive computing (Columbia University Press, New York, 2013)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR109" id="ref-link-section-d69318e2198">109</a>].</p>
                  <h4 class="c-article__sub-heading u-h3 c-article__sub-heading--light" id="Sec12">Discussions</h4><p>In summary, the five aspects mentioned above reflect the primary characteristics of big data, which refers to <i>volume</i>, <i>variety</i>, <i>velocity</i>, <i>veracity</i>, and <i>value</i> [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2" title="J Gantz, D Reinsel, Extracting value from chaos (EMC, Hopkinton, 2011)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR2" id="ref-link-section-d69318e2226">2</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 4" title="D Che, M Safran, Z Peng, From big data to big data mining: challenges, issues, and opportunities, in Proceedings of the 18th International Conference on DASFAA (Wuhan, 2013), pp. 1–15" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR4" id="ref-link-section-d69318e2229">4</a>–<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 6" title="H Hu, Y Wen, T Chua, X Li, Toward scalable systems for big data analytics: a technology tutorial. IEEE Access 2, 652–687 (2014)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR6" id="ref-link-section-d69318e2232">6</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 13" title="XW Chen, X Lin, Big data deep learning: challenges and perspectives. IEEE Access 2, 514–525 (2014)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR13" id="ref-link-section-d69318e2235">13</a>]. The five salient features bring different challenges for machine learning techniques, respectively. To surmount these obstacles, machine learning in the context of big data is significantly different from the traditional learning methods, as discussed above, some scalable, multidomain, parallel, flexible, and intelligent learning methods are preferred. What is more, several enabling technologies are needed to be integrated into the learning progress to improve the effectiveness of learning. A hierarchical framework is described in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#Fig3">3</a> to summarize the efficient machine learning for big data processing.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-3"><figure><figcaption><b id="Fig3" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 3</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-category="article body" data-track-label="image" data-track-action="view figure" href="/article/10.1186/s13634-016-0355-x/figures/3?shared-article-renderer" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1186%2Fs13634-016-0355-x/MediaObjects/13634_2016_355_Fig3_HTML.gif?as=webp"></source><img aria-describedby="figure-3-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1186%2Fs13634-016-0355-x/MediaObjects/13634_2016_355_Fig3_HTML.gif" alt="figure3" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc"><p>Hierarchical framework of efficient machine learning for big data processing</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-category="article body" data-track-label="button" data-track-action="view figure" href="/article/10.1186/s13634-016-0355-x/figures/3?shared-article-renderer" data-track-dest="link:Figure3 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <p>In fact, for big data processing, most machine learning techniques are not universal, that is to say, we often need to use specific learning methods according to different data. For example, in terms of high-dimensional datasets, representation learning seems to be a promising solution, which can learn the meaningful representations of the data that make it easier to extract useful information for achieving impressive performance on many dimensionality reduction tasks. While for large volumes of data, distributed and parallel learning methods have stronger advantages. If the data needed to be processed are drawn from different feature spaces and have different distributions, transfer learning will be a good choice which can intelligently apply knowledge learned previously to solve new problems faster. Frequently, in the context of big data, we have to face such a situation: data may be abundant but labels are scarce or expensive to obtain. To tackle this issue, active learning can achieve high accuracy using as few labeled instances as possible. In addition, nonlinear data processing is also another thorny problem, at this moment, kernel-based learning will be here with its powerful computational capability. Of course, if we want to deal with some data in a timely or (nearly) real-time manner, online learning and extreme learning machine can give us more help.</p><p>Therefore, such a context is needed to be clear, in other words, what are the data tasks, data analysis or decision making?; what are the data types, video data or text data?; what are the data characteristics, high volume or high velocity?; and so on. In terms of different data tasks, types, and characteristics, the required learning techniques are different, even a machine learning methods base is needed for big data processing. The learning systems can fast refer to the algorithm base to handle data. What is more, in order to improve the effectiveness of data processing, the combination of machine learning with some other techniques have been proposed in recent years. For example, in [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 80" title="Y Low, D Bickson, J Gonzalez, C Guestrin, A Kyrola, JM Hellerstein, Distributed GraphLab: a framework for machine learning and data mining in the cloud. Proc VLDB Endow 5(8), 716–727 (2012)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR80" id="ref-link-section-d69318e2265">80</a>], the authors presented a cloud-assisted learning framework to enhance store and computing abilities. A general means of programming machine learning algorithms on multicore with the advantage of MapReduce were investigated to enable the parallel and distributed processing to be possible [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 77" title="C Chu, SK Kim, YA Lin, Y Yu, G Bradski, AY Ng, K Olukotun, Map-reduce for machine learning on multicore, in Proceedings of 20th Annual Conference on Neural Information Processing Systems (NIPS) (Vancouver, 2006), pp. 281–288" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR77" id="ref-link-section-d69318e2268">77</a>]. IBM’s brain-like computer, Watson, applied cognition techniques to machine learning field to make learning systems more intelligent [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 109" title="J Kelly III, S Hamm, Smart machines: IBM’s Watson and the era of cognitive computing (Columbia University Press, New York, 2013)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR109" id="ref-link-section-d69318e2271">109</a>]. Such enabling technologies have brought great benefits for machine learning, especially for large data processing, which are more worthy of study.</p><h3 class="c-article__sub-heading u-h3" id="Sec13">Connection of machine learning with SP techniques for big data</h3><p>There is no doubt that SP is of uttermost relevance to timely big data applications such as real-time medical imaging, sentiment analysis from online social media, smart cities, and so on [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 110" title="K Slavakis, SJ Kim, G Mateos, GB Giannakis, Stochastic approximation vis-a-vis online learning for big data analytics. IEEE Signal Proc Mag 31(6), 124–129 (2014)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR110" id="ref-link-section-d69318e2283">110</a>]. The interest in big-data-related research from the SP community is evident from the increasing number of papers submitted on this topic to SP-oriented journals, workshops, and conferences. In this section, we mainly discuss the close connections of machine learning with SP techniques for big data processing. Specifically, in Section <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#Sec14">1.4.1</a>, we analyze the existing studies on SP for big data from four different perspectives. Several representative literatures are presented. In Section <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#Sec15">1.4.2</a>, we provide a review of the latest research progress which is based on these typical works.</p><h4 class="c-article__sub-heading u-h3 c-article__sub-heading--light" id="Sec14">An overview of representative work</h4><p>In this section, we analyze the relationships between machine learning and SP techniques for big data processing from four perspectives: (1) statistical learning for big data analysis, (2) convex optimization for big data analytics, (3) stochastic approximation for big data analytics, and (4) outlying sequence detection for big data. The diagram is summarized in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#Fig4">4</a>. Several typical research papers are presented, which delineate the theoretical and algorithmic underpinnings together with the relevance of SP tools to the big data and also show the challenges and opportunities for SP research on large-scale data analytics.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-4"><figure><figcaption><b id="Fig4" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 4</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-category="article body" data-track-label="image" data-track-action="view figure" href="/article/10.1186/s13634-016-0355-x/figures/4?shared-article-renderer" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1186%2Fs13634-016-0355-x/MediaObjects/13634_2016_355_Fig4_HTML.gif?as=webp"></source><img aria-describedby="figure-4-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1186%2Fs13634-016-0355-x/MediaObjects/13634_2016_355_Fig4_HTML.gif" alt="figure4" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc"><p>Connection of machine learning with SP techniques for big data from different perspectives</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-category="article body" data-track-label="button" data-track-action="view figure" href="/article/10.1186/s13634-016-0355-x/figures/4?shared-article-renderer" data-track-dest="link:Figure4 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                              <ul class="u-list-style-bullet">
                      <li>
                        <p>
                                       <i>Statistical learning for big data analysis</i>: There is no doubt this is an era of data deluge where learning from these large volumes of data by central processors and storage units seems infeasible. Therefore, the SP and statistical learning tools have to be re-examined. It is preferable to perform learning in real time for the advent of streaming data sources, typically without a chance to revisit past entries. In [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 14" title="K Slavakis, GB Giannakis, G Mateos, Modeling and optimization for big data analytics: (statistical) learning tools for our era of data deluge. IEEE Signal Proc Mag 31(5), 18–31 (2014)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR14" id="ref-link-section-d69318e2329">14</a>], the authors mainly focused on the modeling and optimization for big data analysis by using statistical learning tools. We can conclude from [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 14" title="K Slavakis, GB Giannakis, G Mateos, Modeling and optimization for big data analytics: (statistical) learning tools for our era of data deluge. IEEE Signal Proc Mag 31(5), 18–31 (2014)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR14" id="ref-link-section-d69318e2332">14</a>] that, from the SP and learning perspective, big data themes in terms of tasks, challenges, models, and optimization can be revealed as follows. SP-relevant big data tasks mainly comprise massive scale, outliers and missing values, real-time constraints, and cloud storage. There are great big data challenges we have to face, such as prediction and forecasting, cleansing and imputation, dimensionality reduction, regression, classification, and clustering. In terms of these tasks and challenges, outstanding models and optimization with the SP and learning techniques for big data include parallel and decentralized, time or data adaptive, robust, succinct, and sparse technologies.</p>
                      </li>
                      <li>
                        <p>
                                       <i>Convex optimization for big data analytics</i>: While the importance of convex formulations and optimization has increased dramatically in the last decade and these formulations have been employed in a wide variety of signal processing applications, due to the data size of optimization problems that are too large to process locally in the context of big data, thus convex optimization needs reinvent itself. Cevher et al. [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 111" title="V Cevher, S Becker, M Schmidt, Convex optimization for big data: scalable, randomized, and parallel algorithms for big data analytics. IEEE Signal Proc Mag 31(5), 32–43 (2014)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR111" id="ref-link-section-d69318e2344">111</a>] reviewed recent advances in convex optimization algorithms tailored for big data, having as ultimate goal to markedly reduce the computational, storage, and communication bottlenecks. For example, given a big data optimization problem formulated as</p><div id="Equ6" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ {F}^{*}=\underset{x}{ \min}\left\{\operatorname{F}(x)=\operatorname{f}(x)+\operatorname{g}(x);\kern0.1em x\in {\mathrm{\mathbb{R}}}^p\right\} $$</span></div><div class="c-article-equation__number">
                    (6)
                </div></div>
                                    
                        <p>where <i>f</i> and <i>g</i> are convex functions. To obtain an optimal solution <i>x</i>
                                       <sup>*</sup> of (6) and the required assumptions on <i>f</i> and <i>g</i>, in this article, the authors presented three efficient big data approximation techniques, including first-order methods, randomization and parallel and distributed computation. They mainly referred to the scalable, randomized, and parallel algorithms for big data analytics. In addition, for the optimization problem in (6), ADMM can provide a simple distributed algorithm to solve its composite form, by leveraging powerful augmented Lagrangian and dual decomposition techniques. Although there are two caveats for ADMM, i.e., one is that closed-form solutions do not always exist and the other is that no convergence guarantees for more than two optimization objective terms, there are several recent solutions to address the two drawbacks, such as proximal gradient methods and parallel computing [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 111" title="V Cevher, S Becker, M Schmidt, Convex optimization for big data: scalable, randomized, and parallel algorithms for big data analytics. IEEE Signal Proc Mag 31(5), 32–43 (2014)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR111" id="ref-link-section-d69318e2452">111</a>]. Specifically, from machine learning perspective, those bright techniques like scalable, parallel, and distributed mechanisms are also necessitated, and some applications of employing the recent convex optimization algorithms in learning methods such as support vector machines and graph learning have been appeared in recent years.</p>
                      </li>
                      <li>
                        <p>
                                       <i>Stochastic approximation for big data analytics:</i> Although many of online learning approaches were developed within the machine-learning discipline, they had strong connections with workhorse SP techniques. Reference [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 110" title="K Slavakis, SJ Kim, G Mateos, GB Giannakis, Stochastic approximation vis-a-vis online learning for big data analytics. IEEE Signal Proc Mag 31(6), 124–129 (2014)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR110" id="ref-link-section-d69318e2464">110</a>] is a lecture note which presented recent advances in online learning for big data analytics, where the authors highlighted the relations and differences between online learning methods and some prominent statistical SP tools such as stochastic approximation (SA) and stochastic gradient (SG) algorithms. Through perusing [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 110" title="K Slavakis, SJ Kim, G Mateos, GB Giannakis, Stochastic approximation vis-a-vis online learning for big data analytics. IEEE Signal Proc Mag 31(6), 124–129 (2014)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR110" id="ref-link-section-d69318e2467">110</a>], we can know that, on the one hand, the seminal works on SA, such as by Robbins–Monro and Widrow algorithms, and the workhorse behind several classical SP tools, such as LMS and RLS algorithms, carried rich potential in modern learning tasks for big data analytics. On the other hand, it was also demonstrated that online learning schemes together with random sampling or data sketching methods were expected to play instrumental roles in solving large-scale optimization tasks. In summary, the recent advances in online learning methods and several SP techniques mentioned in this lecture note have the unique and complementary strengths with each other.</p>
                      </li>
                      <li>
                        <p>
                                       <i>Outlying sequence detection for big data:</i> As the data scale grows, so does the chance to involve outlying observations, which in turn motivates the demand for outlier-resilient learning algorithms scaling to large-scale application settings. In this context, data-driven outlying sequence detection algorithms have been proposed by some researchers. In [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 112" title="A Tajer, VV Veeravalli, HV Poor, Outlying sequence detection in large data sets: a data-driven approach. IEEE Signal Proc Mag 31(5), 44–56 (2014)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR112" id="ref-link-section-d69318e2479">112</a>], the authors investigated the robust sequential detection schemes for big data. In contrast to the aforementioned three articles [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 14" title="K Slavakis, GB Giannakis, G Mateos, Modeling and optimization for big data analytics: (statistical) learning tools for our era of data deluge. IEEE Signal Proc Mag 31(5), 18–31 (2014)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR14" id="ref-link-section-d69318e2482">14</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 110" title="K Slavakis, SJ Kim, G Mateos, GB Giannakis, Stochastic approximation vis-a-vis online learning for big data analytics. IEEE Signal Proc Mag 31(6), 124–129 (2014)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR110" id="ref-link-section-d69318e2485">110</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 111" title="V Cevher, S Becker, M Schmidt, Convex optimization for big data: scalable, randomized, and parallel algorithms for big data analytics. IEEE Signal Proc Mag 31(5), 32–43 (2014)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR111" id="ref-link-section-d69318e2488">111</a>] that mostly focus on big data analysis, this article paid more attention to the decision mechanisms. Outlier detection has immediate application in a broad range of contexts, particularly, for machine learning techniques, effective decision on the observations with categorizing them as normal or outlying are important for the improvement of learning performance. As mentioned in [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 112" title="A Tajer, VV Veeravalli, HV Poor, Outlying sequence detection in large data sets: a data-driven approach. IEEE Signal Proc Mag 31(5), 44–56 (2014)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR112" id="ref-link-section-d69318e2492">112</a>], the class of supervised outlier detection had been studied extensively under neural networks, naïve Bayes, and support vector machines.</p>
                      </li>
                    </ul>
                           <h4 class="c-article__sub-heading u-h3 c-article__sub-heading--light" id="Sec15">The latest research progress</h4><p>These representative literatures discussed in Section <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#Sec14">1.4.1</a> provide us a lot of heuristic analysis on both machine learning and SP techniques for big data. Based on the ideas proposed in these works, many new studies are increasing continuously. In this section, we provide a review of the latest research progress which is based on these typical works mentioned above.</p><ul class="u-list-style-bullet">
                      <li>
                        <p>
                                       <i>The latest progress based on</i> [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 14" title="K Slavakis, GB Giannakis, G Mateos, Modeling and optimization for big data analytics: (statistical) learning tools for our era of data deluge. IEEE Signal Proc Mag 31(5), 18–31 (2014)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR14" id="ref-link-section-d69318e2518">14</a>]: Based on the statistical learning tools for big data analysis proposed by Slavakis et al. in [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 14" title="K Slavakis, GB Giannakis, G Mateos, Modeling and optimization for big data analytics: (statistical) learning tools for our era of data deluge. IEEE Signal Proc Mag 31(5), 18–31 (2014)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR14" id="ref-link-section-d69318e2521">14</a>], a lot of new study work has emerged. For example, in [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 113" title="S Scardapane, D Wang, M Panella, A Uncini, Distributed learning for random vector functional-link networks. Inf Sci 301, 271–284 (2015)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR113" id="ref-link-section-d69318e2524">113</a>], two distributed learning algorithms for training random vector functional-link (RVFL) networks through interconnected nodes were presented, where training data were distributed under a decentralized information structure. To tackle the huge-scale convex and nonconvex big data optimization problems, a novel parallel, hybrid random/deterministic decomposition scheme with the power of dictionary learning was investigated in [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 114" title="A Daneshmand, F Facchinei, V Kungurtsev, G Scutari, Hybrid random/deterministic parallel algorithms for nonconvex big data optimization. IEEE Trans Signal Process 63(15), 3914–3929 (2015)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR114" id="ref-link-section-d69318e2527">114</a>]. In [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 87" title="M Mardani, G Mateos, GB Giannakis, Subspace learning and imputation for streaming big data matrices and tensors. IEEE Trans Signal Process 63(10), 2663–2677 (2015)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR87" id="ref-link-section-d69318e2531">87</a>], the authors developed a low-complexity, real-time online algorithm for decomposing low-rank tensors with missing entries to deal with the incomplete streaming data, and the performance of the proposed subspace learning was also validated. All these new work presents the application of machine learning and SP technologies in processing big data well.</p>
                      </li>
                      <li>
                        <p>
                                       <i>The latest progress based on</i> [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 111" title="V Cevher, S Becker, M Schmidt, Convex optimization for big data: scalable, randomized, and parallel algorithms for big data analytics. IEEE Signal Proc Mag 31(5), 32–43 (2014)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR111" id="ref-link-section-d69318e2543">111</a>]: A broad class of machine learning and SP problems can be formally stated as optimization problem. Based on the idea of convex optimization for big data analytics in [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 111" title="V Cevher, S Becker, M Schmidt, Convex optimization for big data: scalable, randomized, and parallel algorithms for big data analytics. IEEE Signal Proc Mag 31(5), 32–43 (2014)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR111" id="ref-link-section-d69318e2546">111</a>], a randomized primal-dual algorithm was proposed in [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 115" title="P. Bianchi, W. Hachem, F. Iutzeler, A stochastic coordinate descent primal-dual algorithm and applications to large-scale composite optimization. arXiv preprint (2014). arXiv:1407.0898" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR115" id="ref-link-section-d69318e2549">115</a>] for composite optimization, which could be used in the framework of large-scale machine learning applications. In addition, a consensus-based decentralized algorithm for a class of nonconvex optimization problems was investigated in [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 116" title="HT Wai, TH Chang, A Scaglione, A consensus-based decentralized algorithm for non-convex optimization with application to dictionary learning, in Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) (South Brisbane, 2015), pp. 3546–3550" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR116" id="ref-link-section-d69318e2552">116</a>], with the application to dictionary learning.</p>
                      </li>
                      <li>
                        <p>
                                       <i>The latest progress based on</i> [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 110" title="K Slavakis, SJ Kim, G Mateos, GB Giannakis, Stochastic approximation vis-a-vis online learning for big data analytics. IEEE Signal Proc Mag 31(6), 124–129 (2014)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR110" id="ref-link-section-d69318e2564">110</a>]: Several classical SP tools such as the stochastic approximation methods, have carried rich potential for solving large-scale learning tasks under low computational expense. The SP and online learning techniques for big data analytics described in [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 110" title="K Slavakis, SJ Kim, G Mateos, GB Giannakis, Stochastic approximation vis-a-vis online learning for big data analytics. IEEE Signal Proc Mag 31(6), 124–129 (2014)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR110" id="ref-link-section-d69318e2567">110</a>] provides a good research direction for future work. Based on this, in [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 117" title="D. Berberidis, V. Kekatos, G.B. Giannakis, Online censoring for large-scale regressions with application to streaming big data. arXiv preprint (2015). arXiv:1507.07536" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR117" id="ref-link-section-d69318e2570">117</a>], the authors developed online algorithms for large-scale regressions with application to streaming big data. In addition, Slavakis and Giannakis further used accelerated stochastic approximation method with online and modular learning algorithms to deal with a large class of nonconvex data models [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 118" title="K. Slavakis, G.B. Giannakis, Per-block-convex data modeling by accelerated stochastic approximation. arXiv preprint (2015). arXiv:1501.07315" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR118" id="ref-link-section-d69318e2573">118</a>].</p>
                      </li>
                      <li>
                        <p>
                                       <i>The latest progress based on</i> [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 112" title="A Tajer, VV Veeravalli, HV Poor, Outlying sequence detection in large data sets: a data-driven approach. IEEE Signal Proc Mag 31(5), 44–56 (2014)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR112" id="ref-link-section-d69318e2585">112</a>]: The outlying sequence detection approach proposed in [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 112" title="A Tajer, VV Veeravalli, HV Poor, Outlying sequence detection in large data sets: a data-driven approach. IEEE Signal Proc Mag 31(5), 44–56 (2014)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR112" id="ref-link-section-d69318e2588">112</a>] provides a desirable solution to some big data application problems. In [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 119" title="KC Chen, SL Huang, L Zheng, HV Poor, Communication theoretic data analytics. IEEE J Sel Areas Commun 33(4), 663–675 (2015)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR119" id="ref-link-section-d69318e2591">119</a>], the authors mainly investigated the big data analytics over the communication system with discussions about statistical analysis and machine learning techniques. The authors pointed out that one of the critically associated challenges ahead was how to detect outliers in the context of big data. It so happened that the theoretic methodology described in [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 112" title="A Tajer, VV Veeravalli, HV Poor, Outlying sequence detection in large data sets: a data-driven approach. IEEE Signal Proc Mag 31(5), 44–56 (2014)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR112" id="ref-link-section-d69318e2594">112</a>] gave the answers.</p>
                      </li>
                    </ul>
                           <p>To sum up, it can be seen from the above presented articles in Section <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#Sec14">1.4.1</a> and Section <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#Sec15">1.4.2</a> that the connection of machine learning with modern SP techniques is very strong. SP techniques are originally developed to analyze and handle discrete and continuous signals through using a set of methods from electrical engineering and applied mathematics. In contrast, machine learning research mainly focuses on the design and development of algorithms which allow computers to evolve behavior based on empirical data, whose major concern is to recognize complex patterns and make intelligent decisions based on data by automatically learning. Both the machine learning and SP techniques have the unique and complementary strengths for big data processing. Furthermore, combining SP and machine learning techniques to explore the emerging field of big data are expected to have a bright future. Quoting a sentence from [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 110" title="K Slavakis, SJ Kim, G Mateos, GB Giannakis, Stochastic approximation vis-a-vis online learning for big data analytics. IEEE Signal Proc Mag 31(6), 124–129 (2014)" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#ref-CR110" id="ref-link-section-d69318e2609">110</a>], “Consequently, ample opportunities arise for the SP community to contribute in this growing and inherently cross-disciplinary field, spanning multiple areas across science and engineering”.</p><h3 class="c-article__sub-heading u-h3" id="Sec16">Research trends and open issues</h3><p>While significant progress has been made in the last decade toward achieving the ultimate goal of making sense of big data by machine learning techniques, the consensus is that we are still not quite there. The efficient preprocessing mechanisms to make the learning system capable of dealing with big data and effective learning technologies to find out the rules to describe the data are still of urgent need. Therefore, some of the open issues and possible research trends are given in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1186/s13634-016-0355-x?shared-article-renderer#Fig5">5</a>.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-5"><figure><figcaption><b id="Fig5" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 5</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-category="article body" data-track-label="image" data-track-action="view figure" href="/article/10.1186/s13634-016-0355-x/figures/5?shared-article-renderer" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1186%2Fs13634-016-0355-x/MediaObjects/13634_2016_355_Fig5_HTML.gif?as=webp"></source><img aria-describedby="figure-5-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1186%2Fs13634-016-0355-x/MediaObjects/13634_2016_355_Fig5_HTML.gif" alt="figure5" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-5-desc"><p>Research trends and open issues</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-category="article body" data-track-label="button" data-track-action="view figure" href="/article/10.1186/s13634-016-0355-x/figures/5?shared-article-renderer" data-track-dest="link:Figure5 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <ol class="u-list-style-none">
                    <li>
                      <span class="u-custom-list-number">1.</span>
                      
                        <p>
                                       <i>Data meaning perspective:</i> Due to the fact that, nowadays, most data are dispersed to different regions, systems, or applications, the “meaning” of the collected data from various sources may not be exactly the same, which may significantly impact the quality of the machine learning results. Although the previous mentioned techniques such as transfer learning with the power of knowledge transfer and the cognition-assisted learning methods provide some possible solutions to this problem, it is obvious that they are absolutely not catholicons owing to the limitations of these techniques for achieving context-aware. Ontology, semantic web, and other related technologies seem to be preferred on this issue. Based on ontology modeling and semantic derivation, some valuable patterns or rules can be discovered as knowledge as well, which is a necessity for learning systems to be, or appear to be intelligent. But the problem that arises now is, although the ontology and semantic web technologies can benefit the big data analysis, these two technologies are not mature enough, thus how to employ them in machine learning methods to process big data will be a meaningful research.</p>
                      
                    </li>
                    <li>
                      <span class="u-custom-list-number">2.</span>
                      
                        <p>
                                       <i>Pattern training perspective</i>: In general, for most machine learning techniques, the more the training patterns are, the higher the accuracy rate of learning results is. However, a dilemma we have to face is that, on the one hand, the labeled patterns play a pivotal role for the learning algorithms; but on the other hand, labeling patterns is often expensive in terms of the computation time or cost, particularly for the large-scale streaming data, which is intractable. How many patterns are needed to train the classifier depends to a large extent on the desire to achieve a balance between cost and accuracy. Therefore, the so-called overfitting is another critical open issue.</p>
                      
                    </li>
                    <li>
                      <span class="u-custom-list-number">3.</span>
                      
                        <p>
                                       <i>Technique integration perspective</i>: Once mentioning big data processing, we always like to put data mining, KDD, SP, cloud computing, and machine learning techniques together, partially because these issues and their products may play principal roles for extracting valuable information from massive data, and partially because they have strong ties with each other. It is important to note that each approach has its own merits and faults. That is to say, to get more values out of the big data, a composite model is more needed. As a result, how to integrate several related techniques with machine learning will also become a further research trend.</p>
                      
                    </li>
                    <li>
                      <span class="u-custom-list-number">4.</span>
                      
                        <p>
                                       <i>Privacy and security perspective</i>: The concern of data privacy has become extremely serious with using data mining and machine learning technologies to analyze personal information in order to produce relevant or accurate results. For example, in order to increase the volume and revenue of sales, some companies today try to collect as many personal data of consumers as possible from various kinds of sources or devices and then use data mining and machine learning methods to find highly interconnected information which is conducive to make marketing tactics. However, if all pieces of the information about a person were dug out through the mining and learning technologies and put together, any privacy about that individual instantly would disappear, which will make most people uncomfortable, and even frightened. Thus, an efficient and effective method needs to preserve the performance of mining and learning while protecting the personal information. Hence, how to make use of data mining and machine learning techniques for big data processing with guaranties of privacy and security is very worthy of study.</p>
                      
                    </li>
                    <li>
                      <span class="u-custom-list-number">5.</span>
                      
                        <p>
                                       <i>Realization and application perspective</i>: The ultimate goal of groping for various learning methods to handle big data is to provide better environment for people; thus, more attention should be focused on building the bridge from theory to practice. For instance, how and where might the theoretical studies in big data machine learning research actually be applied?</p>
                      
                    </li>
                  </ol>
                        </div></div></section><section aria-labelledby="Sec17"><div class="c-article-section" id="Sec17-section"><h2 class="c-article-section__title u-h2 js-section-title js-c-reading-companion-sections-item" id="Sec17">Conclusions</h2><div class="c-article-section__content" id="Sec17-content"><p>Big data are now rapidly expanding in all science and engineering domains. Learning from these massive data is expected to bring significant opportunities and transformative potential for various sectors. However, most traditional machine learning techniques are not inherently efficient or scalable enough to handle the data with the characteristics of large volume, different types, high speed, uncertainty and incompleteness, and low value density. In response, machine learning needs to reinvent itself for big data processing. This paper began with a brief review of conventional machine learning algorithms, followed by several current advanced learning methods. Then, a discussion about the challenges of learning with big data and the corresponding possible solutions in recent researches was given. In addition, the connection of machine learning with modern signal processing technologies was analyzed through studying several latest representative research papers. To stimulate more interests for the audience of the paper, at last, open issues and research trends were presented.</p></div></div></section>
                                
                            

                            <section aria-labelledby="Bib1"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title u-h2 js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references"><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="A. Sandryhaila, JMF. Moura, " /><meta itemprop="datePublished" content="2014" /><meta itemprop="headline" content="A Sandryhaila, JMF Moura, Big data analysis with signal processing on graphs: representation and processing of" /><span class="c-article-references__counter">1.</span><p class="c-article-references__text" id="ref-CR1">A Sandryhaila, JMF Moura, Big data analysis with signal processing on graphs: representation and processing of massive data sets with irregular structure. IEEE Signal Proc Mag <b>31</b>(5), 80–90 (2014)</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="https://doi.org/10.1109%2FMSP.2014.2329213" aria-label="View reference 1">Article</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 1 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Big%20data%20analysis%20with%20signal%20processing%20on%20graphs%3A%20representation%20and%20processing%20of%20massive%20data%20sets%20with%20irregular%20structure&amp;journal=IEEE%20Signal%20Proc%20Mag&amp;volume=31&amp;issue=5&amp;pages=80-90&amp;publication_year=2014&amp;author=Sandryhaila%2CA&amp;author=Moura%2CJMF">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="J. Gantz, D. Reinsel, " /><meta itemprop="datePublished" content="2011" /><meta itemprop="headline" content="J Gantz, D Reinsel, Extracting value from chaos (EMC, Hopkinton, 2011)" /><span class="c-article-references__counter">2.</span><p class="c-article-references__text" id="ref-CR2">J Gantz, D Reinsel, <i>Extracting value from chaos</i> (EMC, Hopkinton, 2011)</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 2 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Extracting%20value%20from%20chaos&amp;publication_year=2011&amp;author=Gantz%2CJ&amp;author=Reinsel%2CD">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="J. Gantz, D. Reinsel, " /><meta itemprop="datePublished" content="2010" /><meta itemprop="headline" content="J Gantz, D Reinsel, The digital universe decade—are you ready (EMC, Hopkinton, 2010)" /><span class="c-article-references__counter">3.</span><p class="c-article-references__text" id="ref-CR3">J Gantz, D Reinsel, <i>The digital universe decade—are you ready</i> (EMC, Hopkinton, 2010)</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 3 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20digital%20universe%20decade%E2%80%94are%20you%20ready&amp;publication_year=2010&amp;author=Gantz%2CJ&amp;author=Reinsel%2CD">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="D Che, M Safran, Z Peng, From big data to big data mining: challenges, issues, and opportunities, in Proceedin" /><span class="c-article-references__counter">4.</span><p class="c-article-references__text" id="ref-CR4">D Che, M Safran, Z Peng, From big data to big data mining: challenges, issues, and opportunities, in <i>Proceedings of the 18th International Conference on DASFAA</i> (Wuhan, 2013), pp. 1–15</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="M. Chen, S. Mao, Y. Liu, " /><meta itemprop="datePublished" content="2014" /><meta itemprop="headline" content="M Chen, S Mao, Y Liu, Big data: a survey. Mobile Netw Appl 19(2), 171–209 (2014)" /><span class="c-article-references__counter">5.</span><p class="c-article-references__text" id="ref-CR5">M Chen, S Mao, Y Liu, Big data: a survey. Mobile Netw Appl <b>19</b>(2), 171–209 (2014)</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="http://www.ams.org/mathscinet-getitem?mr=3270668" aria-label="View reference 5 on MathSciNet">MathSciNet</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="https://doi.org/10.1007%2Fs11036-013-0489-0" aria-label="View reference 5">Article</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 5 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Big%20data%3A%20a%20survey&amp;journal=Mobile%20Netw%20Appl&amp;volume=19&amp;issue=2&amp;pages=171-209&amp;publication_year=2014&amp;author=Chen%2CM&amp;author=Mao%2CS&amp;author=Liu%2CY">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="H. Hu, Y. Wen, T. Chua, X. Li, " /><meta itemprop="datePublished" content="2014" /><meta itemprop="headline" content="H Hu, Y Wen, T Chua, X Li, Toward scalable systems for big data analytics: a technology tutorial. IEEE Access " /><span class="c-article-references__counter">6.</span><p class="c-article-references__text" id="ref-CR6">H Hu, Y Wen, T Chua, X Li, Toward scalable systems for big data analytics: a technology tutorial. IEEE Access <b>2</b>, 652–687 (2014)</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="https://doi.org/10.1109%2FACCESS.2014.2332453" aria-label="View reference 6">Article</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 6 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Toward%20scalable%20systems%20for%20big%20data%20analytics%3A%20a%20technology%20tutorial&amp;journal=IEEE%20Access&amp;volume=2&amp;pages=652-687&amp;publication_year=2014&amp;author=Hu%2CH&amp;author=Wen%2CY&amp;author=Chua%2CT&amp;author=Li%2CX">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="J. Manyika, M. Chui, B. Brown, J. Bughin, R. Dobbs, C. Roxburgh, AH. Byers, " /><meta itemprop="datePublished" content="2011" /><meta itemprop="headline" content="J Manyika, M Chui, B Brown, J Bughin, R Dobbs, C Roxburgh, AH Byers, Big data: the next frontier for innovatio" /><span class="c-article-references__counter">7.</span><p class="c-article-references__text" id="ref-CR7">J Manyika, M Chui, B Brown, J Bughin, R Dobbs, C Roxburgh, AH Byers, <i>Big data: the next frontier for innovation, competition, and productivity</i> (McKinsey Global Institute, USA, 2011)</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 7 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Big%20data%3A%20the%20next%20frontier%20for%20innovation%2C%20competition%2C%20and%20productivity&amp;publication_year=2011&amp;author=Manyika%2CJ&amp;author=Chui%2CM&amp;author=Brown%2CB&amp;author=Bughin%2CJ&amp;author=Dobbs%2CR&amp;author=Roxburgh%2CC&amp;author=Byers%2CAH">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="Q. Wu, G. Ding, Y. Xu, S. Feng, Z. Du, J. Wang, K. Long, " /><meta itemprop="datePublished" content="2014" /><meta itemprop="headline" content="Q Wu, G Ding, Y Xu, S Feng, Z Du, J Wang, K Long, Cognitive internet of things: a new paradigm beyond connecti" /><span class="c-article-references__counter">8.</span><p class="c-article-references__text" id="ref-CR8">Q Wu, G Ding, Y Xu, S Feng, Z Du, J Wang, K Long, Cognitive internet of things: a new paradigm beyond connection. IEEE Internet Things J <b>1</b>(2), 129–143 (2014)</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="https://doi.org/10.1109%2FJIOT.2014.2311513" aria-label="View reference 8">Article</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 8 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Cognitive%20internet%20of%20things%3A%20a%20new%20paradigm%20beyond%20connection&amp;journal=IEEE%20Internet%20Things%20J&amp;volume=1&amp;issue=2&amp;pages=129-143&amp;publication_year=2014&amp;author=Wu%2CQ&amp;author=Ding%2CG&amp;author=Xu%2CY&amp;author=Feng%2CS&amp;author=Du%2CZ&amp;author=Wang%2CJ&amp;author=Long%2CK">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="CW. Tsai, CF. Lai, MC. Chiang, LT. Yang, " /><meta itemprop="datePublished" content="2014" /><meta itemprop="headline" content="CW Tsai, CF Lai, MC Chiang, LT Yang, Data mining for internet of things: a survey. IEEE Commun Surv Tut 16(1)," /><span class="c-article-references__counter">9.</span><p class="c-article-references__text" id="ref-CR9">CW Tsai, CF Lai, MC Chiang, LT Yang, Data mining for internet of things: a survey. IEEE Commun Surv Tut <b>16</b>(1), 77–97 (2014)</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="https://doi.org/10.1109%2FSURV.2013.103013.00206" aria-label="View reference 9">Article</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 9 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Data%20mining%20for%20internet%20of%20things%3A%20a%20survey&amp;journal=IEEE%20Commun%20Surv%20Tut&amp;volume=16&amp;issue=1&amp;pages=77-97&amp;publication_year=2014&amp;author=Tsai%2CCW&amp;author=Lai%2CCF&amp;author=Chiang%2CMC&amp;author=Yang%2CLT">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="A. Imran, A. Zoha, " /><meta itemprop="datePublished" content="2014" /><meta itemprop="headline" content="A Imran, A Zoha, Challenges in 5G: how to empower SON with big data for enabling 5G. IEEE Netw 28(6), 27–33 (2" /><span class="c-article-references__counter">10.</span><p class="c-article-references__text" id="ref-CR10">A Imran, A Zoha, Challenges in 5G: how to empower SON with big data for enabling 5G. IEEE Netw <b>28</b>(6), 27–33 (2014)</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="https://doi.org/10.1109%2FMNET.2014.6963801" aria-label="View reference 10">Article</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 10 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Challenges%20in%205G%3A%20how%20to%20empower%20SON%20with%20big%20data%20for%20enabling%205G&amp;journal=IEEE%20Netw&amp;volume=28&amp;issue=6&amp;pages=27-33&amp;publication_year=2014&amp;author=Imran%2CA&amp;author=Zoha%2CA">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="X Wu, X Zhu, G Wu, W Ding, Data mining with big data. IEEE Trans Knowl Data Eng 26(1), 97–107 (2014)" /><span class="c-article-references__counter">11.</span><p class="c-article-references__text" id="ref-CR11">X Wu, X Zhu, G Wu, W Ding, Data mining with big data. IEEE Trans Knowl Data Eng <b>26</b>(1), 97–107 (2014)</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="A. Rajaraman, JD. Ullman, " /><meta itemprop="datePublished" content="2011" /><meta itemprop="headline" content="A Rajaraman, JD Ullman, Mining of massive data sets (Cambridge University Press, Oxford, 2011)" /><span class="c-article-references__counter">12.</span><p class="c-article-references__text" id="ref-CR12">A Rajaraman, JD Ullman, <i>Mining of massive data sets</i> (Cambridge University Press, Oxford, 2011)</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 12 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Mining%20of%20massive%20data%20sets&amp;publication_year=2011&amp;author=Rajaraman%2CA&amp;author=Ullman%2CJD">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="XW. Chen, X. Lin, " /><meta itemprop="datePublished" content="2014" /><meta itemprop="headline" content="XW Chen, X Lin, Big data deep learning: challenges and perspectives. IEEE Access 2, 514–525 (2014)" /><span class="c-article-references__counter">13.</span><p class="c-article-references__text" id="ref-CR13">XW Chen, X Lin, Big data deep learning: challenges and perspectives. IEEE Access <b>2</b>, 514–525 (2014)</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="https://doi.org/10.1109%2FACCESS.2014.2325029" aria-label="View reference 13">Article</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 13 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Big%20data%20deep%20learning%3A%20challenges%20and%20perspectives&amp;journal=IEEE%20Access&amp;volume=2&amp;pages=514-525&amp;publication_year=2014&amp;author=Chen%2CXW&amp;author=Lin%2CX">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="K. Slavakis, GB. Giannakis, G. Mateos, " /><meta itemprop="datePublished" content="2014" /><meta itemprop="headline" content="K Slavakis, GB Giannakis, G Mateos, Modeling and optimization for big data analytics: (statistical) learning t" /><span class="c-article-references__counter">14.</span><p class="c-article-references__text" id="ref-CR14">K Slavakis, GB Giannakis, G Mateos, Modeling and optimization for big data analytics: (statistical) learning tools for our era of data deluge. IEEE Signal Proc Mag <b>31</b>(5), 18–31 (2014)</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="https://doi.org/10.1109%2FMSP.2014.2327238" aria-label="View reference 14">Article</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 14 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Modeling%20and%20optimization%20for%20big%20data%20analytics%3A%20%28statistical%29%20learning%20tools%20for%20our%20era%20of%20data%20deluge&amp;journal=IEEE%20Signal%20Proc%20Mag&amp;volume=31&amp;issue=5&amp;pages=18-31&amp;publication_year=2014&amp;author=Slavakis%2CK&amp;author=Giannakis%2CGB&amp;author=Mateos%2CG">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="TM. Mitchell, " /><meta itemprop="datePublished" content="1997" /><meta itemprop="headline" content="TM Mitchell, Machine learning (McGraw-Hill, New York, 1997)" /><span class="c-article-references__counter">15.</span><p class="c-article-references__text" id="ref-CR15">TM Mitchell, <i>Machine learning</i> (McGraw-Hill, New York, 1997)</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 15 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Machine%20learning&amp;publication_year=1997&amp;author=Mitchell%2CTM">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="S. Russell, P. Norvig, " /><meta itemprop="datePublished" content="1995" /><meta itemprop="headline" content="S Russell, P Norvig, Artificial intelligence: a modern approach (Prentice-Hall, Englewood Cliffs, 1995)" /><span class="c-article-references__counter">16.</span><p class="c-article-references__text" id="ref-CR16">S Russell, P Norvig, <i>Artificial intelligence: a modern approach</i> (Prentice-Hall, Englewood Cliffs, 1995)</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 16 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Artificial%20intelligence%3A%20a%20modern%20approach&amp;publication_year=1995&amp;author=Russell%2CS&amp;author=Norvig%2CP">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="V. Cherkassky, FM. Mulier, " /><meta itemprop="datePublished" content="2007" /><meta itemprop="headline" content="V Cherkassky, FM Mulier, Learning from data: concepts, theory, and methods (John Wiley &amp; Sons, New Jersey, 200" /><span class="c-article-references__counter">17.</span><p class="c-article-references__text" id="ref-CR17">V Cherkassky, FM Mulier, <i>Learning from data: concepts, theory, and methods</i> (John Wiley &amp; Sons, New Jersey, 2007)</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 17 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Learning%20from%20data%3A%20concepts%2C%20theory%2C%20and%20methods&amp;publication_year=2007&amp;author=Cherkassky%2CV&amp;author=Mulier%2CFM">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="TM Mitchell, The discipline of machine learning (Carnegie Mellon University, School of Computer Science, Machi" /><span class="c-article-references__counter">18.</span><p class="c-article-references__text" id="ref-CR18">TM Mitchell, <i>The discipline of machine learning</i> (Carnegie Mellon University, School of Computer Science, Machine Learning Department, 2006)</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="C. Rudin, KL. Wagstaff, " /><meta itemprop="datePublished" content="2014" /><meta itemprop="headline" content="C Rudin, KL Wagstaff, Machine learning for science and society. Mach Learn 95(1), 1–9 (2014)" /><span class="c-article-references__counter">19.</span><p class="c-article-references__text" id="ref-CR19">C Rudin, KL Wagstaff, Machine learning for science and society. Mach Learn <b>95</b>(1), 1–9 (2014)</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="http://www.ams.org/mathscinet-getitem?mr=3179975" aria-label="View reference 19 on MathSciNet">MathSciNet</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="https://doi.org/10.1007%2Fs10994-013-5425-9" aria-label="View reference 19">Article</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 19 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Machine%20learning%20for%20science%20and%20society&amp;journal=Mach%20Learn&amp;volume=95&amp;issue=1&amp;pages=1-9&amp;publication_year=2014&amp;author=Rudin%2CC&amp;author=Wagstaff%2CKL">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="CM. Bishop, " /><meta itemprop="datePublished" content="2006" /><meta itemprop="headline" content="CM Bishop, Pattern recognition and machine learning (Springer, New York, 2006)" /><span class="c-article-references__counter">20.</span><p class="c-article-references__text" id="ref-CR20">CM Bishop, <i>Pattern recognition and machine learning</i> (Springer, New York, 2006)</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 20 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Pattern%20recognition%20and%20machine%20learning&amp;publication_year=2006&amp;author=Bishop%2CCM">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="B. Adam, IFC. Smith, F. Asce, " /><meta itemprop="datePublished" content="2008" /><meta itemprop="headline" content="B Adam, IFC Smith, F Asce, Reinforcement learning for structural control. J Comput Civil Eng 22(2), 133–139 (2" /><span class="c-article-references__counter">21.</span><p class="c-article-references__text" id="ref-CR21">B Adam, IFC Smith, F Asce, Reinforcement learning for structural control. J Comput Civil Eng <b>22</b>(2), 133–139 (2008)</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="https://doi.org/10.1061%2F%28ASCE%290887-3801%282008%2922%3A2%28133%29" aria-label="View reference 21">Article</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 21 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Reinforcement%20learning%20for%20structural%20control&amp;journal=J%20Comput%20Civil%20Eng&amp;volume=22&amp;issue=2&amp;pages=133-139&amp;publication_year=2008&amp;author=Adam%2CB&amp;author=Smith%2CIFC&amp;author=Asce%2CF">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="N. Jones, " /><meta itemprop="datePublished" content="2014" /><meta itemprop="headline" content="N Jones, Computer science: the learning machines. Nature 505(7482), 146–148 (2014)" /><span class="c-article-references__counter">22.</span><p class="c-article-references__text" id="ref-CR22">N Jones, Computer science: the learning machines. Nature <b>505</b>(7482), 146–148 (2014)</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="https://doi.org/10.1038%2F505146a" aria-label="View reference 22">Article</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 22 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Computer%20science%3A%20the%20learning%20machines&amp;journal=Nature&amp;volume=505&amp;issue=7482&amp;pages=146-148&amp;publication_year=2014&amp;author=Jones%2CN">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="J. Langford, " /><meta itemprop="datePublished" content="2005" /><meta itemprop="headline" content="J Langford, Tutorial on practical prediction theory for classification. J Mach Learn Res 6(3), 273–306 (2005)" /><span class="c-article-references__counter">23.</span><p class="c-article-references__text" id="ref-CR23">J Langford, Tutorial on practical prediction theory for classification. J Mach Learn Res <b>6</b>(3), 273–306 (2005)</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="http://www.ams.org/mathscinet-getitem?mr=2249822" aria-label="View reference 23 on MathSciNet">MathSciNet</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="http://www.emis.de/MATH-item?1222.68243" aria-label="View reference 23 on MATH">MATH</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 23 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Tutorial%20on%20practical%20prediction%20theory%20for%20classification&amp;journal=J%20Mach%20Learn%20Res&amp;volume=6&amp;issue=3&amp;pages=273-306&amp;publication_year=2005&amp;author=Langford%2CJ">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="R. Bekkerman, EY. Ran, N. Tishby, Y. Winter, " /><meta itemprop="datePublished" content="2003" /><meta itemprop="headline" content="R Bekkerman, EY Ran, N Tishby, Y Winter, Distributional word clusters vs. words for text categorization. J Mac" /><span class="c-article-references__counter">24.</span><p class="c-article-references__text" id="ref-CR24">R Bekkerman, EY Ran, N Tishby, Y Winter, Distributional word clusters vs. words for text categorization. J Mach Learn Res <b>3</b>, 1183–1208 (2003)</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="http://www.emis.de/MATH-item?1102.68528" aria-label="View reference 24 on MATH">MATH</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 24 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Distributional%20word%20clusters%20vs.%20words%20for%20text%20categorization&amp;journal=J%20Mach%20Learn%20Res&amp;volume=3&amp;pages=1183-1208&amp;publication_year=2003&amp;author=Bekkerman%2CR&amp;author=Ran%2CEY&amp;author=Tishby%2CN&amp;author=Winter%2CY">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="Y. Bengio, A. Courville, P. Vincent, " /><meta itemprop="datePublished" content="2012" /><meta itemprop="headline" content="Y Bengio, A Courville, P Vincent, Representation learning: a review and new perspectives. IEEE Trans Pattern A" /><span class="c-article-references__counter">25.</span><p class="c-article-references__text" id="ref-CR25">Y Bengio, A Courville, P Vincent, Representation learning: a review and new perspectives. IEEE Trans Pattern Anal <b>35</b>(8), 1798–1828 (2012)</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="https://doi.org/10.1109%2FTPAMI.2013.50" aria-label="View reference 25">Article</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 25 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Representation%20learning%3A%20a%20review%20and%20new%20perspectives&amp;journal=IEEE%20Trans%20Pattern%20Anal&amp;volume=35&amp;issue=8&amp;pages=1798-1828&amp;publication_year=2012&amp;author=Bengio%2CY&amp;author=Courville%2CA&amp;author=Vincent%2CP">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="F Huang, E Yates, Biased representation learning for domain adaptation, in Proceedings of the 2012 Joint Confe" /><span class="c-article-references__counter">26.</span><p class="c-article-references__text" id="ref-CR26">F Huang, E Yates, Biased representation learning for domain adaptation, in <i>Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</i> (Jeju Island, 2012), pp. 1313–1323</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="W Tu, S Sun, Cross-domain representation-learning framework with combination of class-separate and domain-merg" /><span class="c-article-references__counter">27.</span><p class="c-article-references__text" id="ref-CR27">W Tu, S Sun, Cross-domain representation-learning framework with combination of class-separate and domain-merge objectives, in <i>Proceedings of the 1st International Workshop on Cross Domain Knowledge Discovery in Web and Social Network Mining</i> (Beijing, 2012), pp. 18–25</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="S. Li, C. Huang, C. Zong, " /><meta itemprop="datePublished" content="2011" /><meta itemprop="headline" content="S Li, C Huang, C Zong, Multi-domain sentiment classification with classifier combination. J Comput Sci Technol" /><span class="c-article-references__counter">28.</span><p class="c-article-references__text" id="ref-CR28">S Li, C Huang, C Zong, Multi-domain sentiment classification with classifier combination. J Comput Sci Technol <b>26</b>(1), 25–33 (2011)</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="https://doi.org/10.1007%2Fs11390-011-9412-y" aria-label="View reference 28">Article</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 28 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Multi-domain%20sentiment%20classification%20with%20classifier%20combination&amp;journal=J%20Comput%20Sci%20Technol&amp;volume=26&amp;issue=1&amp;pages=25-33&amp;publication_year=2011&amp;author=Li%2CS&amp;author=Huang%2CC&amp;author=Zong%2CC">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="F Huang, E Yates, Exploring representation-learning approaches to domain adaptation, in Proceedings of the 201" /><span class="c-article-references__counter">29.</span><p class="c-article-references__text" id="ref-CR29">F Huang, E Yates, Exploring representation-learning approaches to domain adaptation, in <i>Proceedings of the 2010 Workshop on Domain Adaptation for Natural Language Processing</i> (Uppsala, 2010), pp. 23–30</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="A Bordes, X Glorot, JWAY Bengio, Joint learning of words and meaning representations for open-text semantic pa" /><span class="c-article-references__counter">30.</span><p class="c-article-references__text" id="ref-CR30">A Bordes, X Glorot, JWAY Bengio, Joint learning of words and meaning representations for open-text semantic parsing, in <i>Proceedings of 15th International Conference on Artificial Intelligence and Statistics</i> (La Palma, 2012), pp. 127–135</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="N. Boulanger-Lewandowski, Y. Bengio, P. Vincent, Modeling temporal dependencies in high-dimensional sequences:" /><span class="c-article-references__counter">31.</span><p class="c-article-references__text" id="ref-CR31">N. Boulanger-Lewandowski, Y. Bengio, P. Vincent, Modeling temporal dependencies in high-dimensional sequences: application to polyphonic music generation and transcription. arXiv preprint (2012). arXiv:1206.6392</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="K Dwivedi, K Biswaranjan, A Sethi, Drowsy driver detection using representation learning, in Proceedings of th" /><span class="c-article-references__counter">32.</span><p class="c-article-references__text" id="ref-CR32">K Dwivedi, K Biswaranjan, A Sethi, Drowsy driver detection using representation learning, in <i>Proceedings of the IEEE International Advance Computing Conference</i> (Gurgaon, 2014), pp. 995–999</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="D. Yu, L. Deng, " /><meta itemprop="datePublished" content="2011" /><meta itemprop="headline" content="D Yu, L Deng, Deep learning and its applications to signal and information processing. IEEE Signal Proc Mag 28" /><span class="c-article-references__counter">33.</span><p class="c-article-references__text" id="ref-CR33">D Yu, L Deng, Deep learning and its applications to signal and information processing. IEEE Signal Proc Mag <b>28</b>(1), 145–154 (2011)</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="https://doi.org/10.1109%2FMSP.2010.939038" aria-label="View reference 33">Article</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 33 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Deep%20learning%20and%20its%20applications%20to%20signal%20and%20information%20processing&amp;journal=IEEE%20Signal%20Proc%20Mag&amp;volume=28&amp;issue=1&amp;pages=145-154&amp;publication_year=2011&amp;author=Yu%2CD&amp;author=Deng%2CL">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="I. Arel, DC. Rose, TP. Karnowski, " /><meta itemprop="datePublished" content="2010" /><meta itemprop="headline" content="I Arel, DC Rose, TP Karnowski, Deep machine learning-a new frontier in artificial intelligence research. IEEE " /><span class="c-article-references__counter">34.</span><p class="c-article-references__text" id="ref-CR34">I Arel, DC Rose, TP Karnowski, Deep machine learning-a new frontier in artificial intelligence research. IEEE Comput Intell Mag <b>5</b>(4), 13–18 (2010)</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="https://doi.org/10.1109%2FMCI.2010.938364" aria-label="View reference 34">Article</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 34 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Deep%20machine%20learning-a%20new%20frontier%20in%20artificial%20intelligence%20research&amp;journal=IEEE%20Comput%20Intell%20Mag&amp;volume=5&amp;issue=4&amp;pages=13-18&amp;publication_year=2010&amp;author=Arel%2CI&amp;author=Rose%2CDC&amp;author=Karnowski%2CTP">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="Y. Bengio, " /><meta itemprop="datePublished" content="2009" /><meta itemprop="headline" content="Y Bengio, Learning deep architectures for AI. Foundations Trends Mach Learn 2(1), 1–127 (2009)" /><span class="c-article-references__counter">35.</span><p class="c-article-references__text" id="ref-CR35">Y Bengio, Learning deep architectures for AI. Foundations Trends Mach Learn <b>2</b>(1), 1–127 (2009)</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="http://www.ams.org/mathscinet-getitem?mr=2480723" aria-label="View reference 35 on MathSciNet">MathSciNet</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="https://doi.org/10.1561%2F2200000006" aria-label="View reference 35">Article</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="http://www.emis.de/MATH-item?1192.68503" aria-label="View reference 35 on MATH">MATH</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 35 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Learning%20deep%20architectures%20for%20AI&amp;journal=Foundations%20Trends%20Mach%20Learn&amp;volume=2&amp;issue=1&amp;pages=1-127&amp;publication_year=2009&amp;author=Bengio%2CY">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu, P. Kuksa, " /><meta itemprop="datePublished" content="2011" /><meta itemprop="headline" content="R Collobert, J Weston, L Bottou, M Karlen, K Kavukcuoglu, P Kuksa, Natural language processing (almost) from s" /><span class="c-article-references__counter">36.</span><p class="c-article-references__text" id="ref-CR36">R Collobert, J Weston, L Bottou, M Karlen, K Kavukcuoglu, P Kuksa, Natural language processing (almost) from scratch. J Mach Learn Res <b>12</b>, 2493–2537 (2011)</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="http://www.emis.de/MATH-item?1280.68161" aria-label="View reference 36 on MATH">MATH</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 36 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Natural%20language%20processing%20%28almost%29%20from%20scratch&amp;journal=J%20Mach%20Learn%20Res&amp;volume=12&amp;pages=2493-2537&amp;publication_year=2011&amp;author=Collobert%2CR&amp;author=Weston%2CJ&amp;author=Bottou%2CL&amp;author=Karlen%2CM&amp;author=Kavukcuoglu%2CK&amp;author=Kuksa%2CP">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="P. Callet, C. Viard-Gaudin, D. Barba, " /><meta itemprop="datePublished" content="2006" /><meta itemprop="headline" content="P Le Callet, C Viard-Gaudin, D Barba, A convolutional neural network approach for objective video quality asse" /><span class="c-article-references__counter">37.</span><p class="c-article-references__text" id="ref-CR37">P Le Callet, C Viard-Gaudin, D Barba, A convolutional neural network approach for objective video quality assessment. IEEE Trans Neural Networ <b>17</b>(5), 1316–1327 (2006)</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="https://doi.org/10.1109%2FTNN.2006.879766" aria-label="View reference 37">Article</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 37 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20convolutional%20neural%20network%20approach%20for%20objective%20video%20quality%20assessment&amp;journal=IEEE%20Trans%20Neural%20Networ&amp;volume=17&amp;issue=5&amp;pages=1316-1327&amp;publication_year=2006&amp;author=Callet%2CP&amp;author=Viard-Gaudin%2CC&amp;author=Barba%2CD">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="GE. Dahl, D. Yu, L. Deng, A. Acero, " /><meta itemprop="datePublished" content="2012" /><meta itemprop="headline" content="GE Dahl, D Yu, L Deng, A Acero, Context-dependent pre-trained deep neural networks for large-vocabulary speech" /><span class="c-article-references__counter">38.</span><p class="c-article-references__text" id="ref-CR38">GE Dahl, D Yu, L Deng, A Acero, Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition. IEEE Trans Audio Speech Lang Proc <b>20</b>(1), 30–42 (2012)</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="https://doi.org/10.1109%2FTASL.2011.2134090" aria-label="View reference 38">Article</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 38 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Context-dependent%20pre-trained%20deep%20neural%20networks%20for%20large-vocabulary%20speech%20recognition&amp;journal=IEEE%20Trans%20Audio%20Speech%20Lang%20Proc&amp;volume=20&amp;issue=1&amp;pages=30-42&amp;publication_year=2012&amp;author=Dahl%2CGE&amp;author=Yu%2CD&amp;author=Deng%2CL&amp;author=Acero%2CA">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="G. Hinton, L. Deng, Y. Dong, GE. Dahl, A. Mohamed, N. Jaitly, A. Senior, V. Vanhoucke, P. Nguyen, TN. Sainath, B. Kingsbury, " /><meta itemprop="datePublished" content="2012" /><meta itemprop="headline" content="G Hinton, L Deng, Y Dong, GE Dahl, A Mohamed, N Jaitly, A Senior, V Vanhoucke, P Nguyen, TN Sainath, B Kingsbu" /><span class="c-article-references__counter">39.</span><p class="c-article-references__text" id="ref-CR39">G Hinton, L Deng, Y Dong, GE Dahl, A Mohamed, N Jaitly, A Senior, V Vanhoucke, P Nguyen, TN Sainath, B Kingsbury, Deep neural networks for acoustic modeling in speech recognition: the shared views of four research groups. IEEE Signal Proc Mag <b>29</b>(6), 82–97 (2012)</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="https://doi.org/10.1109%2FMSP.2012.2205597" aria-label="View reference 39">Article</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 39 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Deep%20neural%20networks%20for%20acoustic%20modeling%20in%20speech%20recognition%3A%20the%20shared%20views%20of%20four%20research%20groups&amp;journal=IEEE%20Signal%20Proc%20Mag&amp;volume=29&amp;issue=6&amp;pages=82-97&amp;publication_year=2012&amp;author=Hinton%2CG&amp;author=Deng%2CL&amp;author=Dong%2CY&amp;author=Dahl%2CGE&amp;author=Mohamed%2CA&amp;author=Jaitly%2CN&amp;author=Senior%2CA&amp;author=Vanhoucke%2CV&amp;author=Nguyen%2CP&amp;author=Sainath%2CTN&amp;author=Kingsbury%2CB">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="DC. Ciresan, U. Meier, LM. Gambardella, J. Schmidhuber, " /><meta itemprop="datePublished" content="2010" /><meta itemprop="headline" content="DC Ciresan, U Meier, LM Gambardella, J Schmidhuber, Deep, big, simple neural nets for handwritten digit recogn" /><span class="c-article-references__counter">40.</span><p class="c-article-references__text" id="ref-CR40">DC Ciresan, U Meier, LM Gambardella, J Schmidhuber, Deep, big, simple neural nets for handwritten digit recognition. Neural Comput <b>22</b>(12), 3207–3220 (2010)</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="https://doi.org/10.1162%2FNECO_a_00052" aria-label="View reference 40">Article</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 40 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Deep%2C%20big%2C%20simple%20neural%20nets%20for%20handwritten%20digit%20recognition&amp;journal=Neural%20Comput&amp;volume=22&amp;issue=12&amp;pages=3207-3220&amp;publication_year=2010&amp;author=Ciresan%2CDC&amp;author=Meier%2CU&amp;author=Gambardella%2CLM&amp;author=Schmidhuber%2CJ">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="Y. Wang, D. Yu, Y. Ju, A. Acero, " /><meta itemprop="datePublished" content="2011" /><meta itemprop="headline" content="Y Wang, D Yu, Y Ju, A Acero, Voice search, in Language understanding: systems for extracting semantic informat" /><span class="c-article-references__counter">41.</span><p class="c-article-references__text" id="ref-CR41">Y Wang, D Yu, Y Ju, A Acero, Voice search, in <i>Language understanding: systems for extracting semantic information from speech</i> (Wiley, New York, 2011)</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 41 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Language%20understanding%3A%20systems%20for%20extracting%20semantic%20information%20from%20speech&amp;publication_year=2011&amp;author=Wang%2CY&amp;author=Yu%2CD&amp;author=Ju%2CY&amp;author=Acero%2CA">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="D. Peteiro-Barral, B. Guijarro-Berdiñas, " /><meta itemprop="datePublished" content="2012" /><meta itemprop="headline" content="D Peteiro-Barral, B Guijarro-Berdiñas, A survey of methods for distributed machine learning. Progress in Artif" /><span class="c-article-references__counter">42.</span><p class="c-article-references__text" id="ref-CR42">D Peteiro-Barral, B Guijarro-Berdiñas, A survey of methods for distributed machine learning. Progress in Artificial Intelligence <b>2</b>(1), 1–11 (2012)</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="https://doi.org/10.1007%2Fs13748-012-0035-5" aria-label="View reference 42">Article</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 42 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20survey%20of%20methods%20for%20distributed%20machine%20learning&amp;journal=Progress%20in%20Artificial%20Intelligence&amp;volume=2&amp;issue=1&amp;pages=1-11&amp;publication_year=2012&amp;author=Peteiro-Barral%2CD&amp;author=Guijarro-Berdi%C3%B1as%2CB">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="H. Zheng, SR. Kulkarni, HV. Poor, " /><meta itemprop="datePublished" content="2011" /><meta itemprop="headline" content="H Zheng, SR Kulkarni, HV Poor, Attribute-distributed learning: models, limits, and algorithms. IEEE Trans Sign" /><span class="c-article-references__counter">43.</span><p class="c-article-references__text" id="ref-CR43">H Zheng, SR Kulkarni, HV Poor, Attribute-distributed learning: models, limits, and algorithms. IEEE Trans Signal Process <b>59</b>(1), 386–398 (2011)</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="http://www.ams.org/mathscinet-getitem?mr=2790010" aria-label="View reference 43 on MathSciNet">MathSciNet</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="https://doi.org/10.1109%2FTSP.2010.2088393" aria-label="View reference 43">Article</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 43 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Attribute-distributed%20learning%3A%20models%2C%20limits%2C%20and%20algorithms&amp;journal=IEEE%20Trans%20Signal%20Process&amp;volume=59&amp;issue=1&amp;pages=386-398&amp;publication_year=2011&amp;author=Zheng%2CH&amp;author=Kulkarni%2CSR&amp;author=Poor%2CHV">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="H. Chen, T. Li, C. Luo, SJ. Horng, G. Wang, " /><meta itemprop="datePublished" content="2014" /><meta itemprop="headline" content="H Chen, T Li, C Luo, SJ Horng, G Wang, A rough set-based method for updating decision rules on attribute value" /><span class="c-article-references__counter">44.</span><p class="c-article-references__text" id="ref-CR44">H Chen, T Li, C Luo, SJ Horng, G Wang, A rough set-based method for updating decision rules on attribute values’ coarsening and refining. IEEE Trans Knowl Data Eng <b>26</b>(12), 2886–2899 (2014)</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="https://doi.org/10.1109%2FTKDE.2014.2320740" aria-label="View reference 44">Article</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 44 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20rough%20set-based%20method%20for%20updating%20decision%20rules%20on%20attribute%20values%E2%80%99%20coarsening%20and%20refining&amp;journal=IEEE%20Trans%20Knowl%20Data%20Eng&amp;volume=26&amp;issue=12&amp;pages=2886-2899&amp;publication_year=2014&amp;author=Chen%2CH&amp;author=Li%2CT&amp;author=Luo%2CC&amp;author=Horng%2CSJ&amp;author=Wang%2CG">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="J. Chen, C. Wang, R. Wang, " /><meta itemprop="datePublished" content="2009" /><meta itemprop="headline" content="J Chen, C Wang, R Wang, Using stacked generalization to combine SVMs in magnitude and shape feature spaces for" /><span class="c-article-references__counter">45.</span><p class="c-article-references__text" id="ref-CR45">J Chen, C Wang, R Wang, Using stacked generalization to combine SVMs in magnitude and shape feature spaces for classification of hyperspectral data. IEEE Trans Geosci Remote <b>47</b>(7), 2193–2205 (2009)</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="https://doi.org/10.1109%2FTGRS.2008.2010491" aria-label="View reference 45">Article</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 45 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Using%20stacked%20generalization%20to%20combine%20SVMs%20in%20magnitude%20and%20shape%20feature%20spaces%20for%20classification%20of%20hyperspectral%20data&amp;journal=IEEE%20Trans%20Geosci%20Remote&amp;volume=47&amp;issue=7&amp;pages=2193-2205&amp;publication_year=2009&amp;author=Chen%2CJ&amp;author=Wang%2CC&amp;author=Wang%2CR">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="E. Leyva, A. González, R. Pérez, " /><meta itemprop="datePublished" content="2014" /><meta itemprop="headline" content="E Leyva, A González, R Pérez, A set of complexity measures designed for applying meta-learning to instance sel" /><span class="c-article-references__counter">46.</span><p class="c-article-references__text" id="ref-CR46">E Leyva, A González, R Pérez, A set of complexity measures designed for applying meta-learning to instance selection. IEEE Trans Knowl Data Eng <b>27</b>(2), 354–367 (2014)</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="https://doi.org/10.1109%2FTKDE.2014.2327034" aria-label="View reference 46">Article</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 46 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20set%20of%20complexity%20measures%20designed%20for%20applying%20meta-learning%20to%20instance%20selection&amp;journal=IEEE%20Trans%20Knowl%20Data%20Eng&amp;volume=27&amp;issue=2&amp;pages=354-367&amp;publication_year=2014&amp;author=Leyva%2CE&amp;author=Gonz%C3%A1lez%2CA&amp;author=P%C3%A9rez%2CR">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="M Sarnovsky, M Vronc, Distributed boosting algorithm for classification of text documents, in Proceedings of t" /><span class="c-article-references__counter">47.</span><p class="c-article-references__text" id="ref-CR47">M Sarnovsky, M Vronc, Distributed boosting algorithm for classification of text documents, in <i>Proceedings of the 12th IEEE International Symposium on Applied Machine Intelligence and Informatics (SAMI)</i> (Herl'any, 2014), pp. 217–220</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="SR. Upadhyaya, " /><meta itemprop="datePublished" content="2013" /><meta itemprop="headline" content="SR Upadhyaya, Parallel approaches to machine learning—a comprehensive survey. J Parallel Distr Com 73(3), 284–" /><span class="c-article-references__counter">48.</span><p class="c-article-references__text" id="ref-CR48">SR Upadhyaya, Parallel approaches to machine learning—a comprehensive survey. J Parallel Distr Com <b>73</b>(3), 284–292 (2013)</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="https://doi.org/10.1016%2Fj.jpdc.2012.11.001" aria-label="View reference 48">Article</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 48 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Parallel%20approaches%20to%20machine%20learning%E2%80%94a%20comprehensive%20survey&amp;journal=J%20Parallel%20Distr%20Com&amp;volume=73&amp;issue=3&amp;pages=284-292&amp;publication_year=2013&amp;author=Upadhyaya%2CSR">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="R. Bekkerman, M. Bilenko, J. Langford, " /><meta itemprop="datePublished" content="2011" /><meta itemprop="headline" content="R Bekkerman, M Bilenko, J Langford, Scaling up machine learning: parallel and distributed approaches (Cambridg" /><span class="c-article-references__counter">49.</span><p class="c-article-references__text" id="ref-CR49">R Bekkerman, M Bilenko, J Langford, <i>Scaling up machine learning: parallel and distributed approaches</i> (Cambridge University Press, Oxford, 2011)</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 49 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Scaling%20up%20machine%20learning%3A%20parallel%20and%20distributed%20approaches&amp;publication_year=2011&amp;author=Bekkerman%2CR&amp;author=Bilenko%2CM&amp;author=Langford%2CJ">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="EW. Xiang, B. Cao, DH. Hu, Q. Yang, " /><meta itemprop="datePublished" content="2010" /><meta itemprop="headline" content="EW Xiang, B Cao, DH Hu, Q Yang, Bridging domains using world wide knowledge for transfer learning. IEEE Trans " /><span class="c-article-references__counter">50.</span><p class="c-article-references__text" id="ref-CR50">EW Xiang, B Cao, DH Hu, Q Yang, Bridging domains using world wide knowledge for transfer learning. IEEE Trans Knowl Data Eng <b>22</b>(6), 770–783 (2010)</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="https://doi.org/10.1109%2FTKDE.2010.31" aria-label="View reference 50">Article</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 50 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Bridging%20domains%20using%20world%20wide%20knowledge%20for%20transfer%20learning&amp;journal=IEEE%20Trans%20Knowl%20Data%20Eng&amp;volume=22&amp;issue=6&amp;pages=770-783&amp;publication_year=2010&amp;author=Xiang%2CEW&amp;author=Cao%2CB&amp;author=Hu%2CDH&amp;author=Yang%2CQ">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="SJ. Pan, Q. Yang, " /><meta itemprop="datePublished" content="2010" /><meta itemprop="headline" content="SJ Pan, Q Yang, A survey on transfer learning. IEEE Trans Knowl Data Eng 22(10), 1345–1359 (2010)" /><span class="c-article-references__counter">51.</span><p class="c-article-references__text" id="ref-CR51">SJ Pan, Q Yang, A survey on transfer learning. IEEE Trans Knowl Data Eng <b>22</b>(10), 1345–1359 (2010)</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="https://doi.org/10.1109%2FTKDE.2009.191" aria-label="View reference 51">Article</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 51 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20survey%20on%20transfer%20learning&amp;journal=IEEE%20Trans%20Knowl%20Data%20Eng&amp;volume=22&amp;issue=10&amp;pages=1345-1359&amp;publication_year=2010&amp;author=Pan%2CSJ&amp;author=Yang%2CQ">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="W Fan, I Davidson, B Zadrozny, PS Yu, An improved categorization of classifier’s sensitivity on sample selecti" /><span class="c-article-references__counter">52.</span><p class="c-article-references__text" id="ref-CR52">W Fan, I Davidson, B Zadrozny, PS Yu, An improved categorization of classifier’s sensitivity on sample selection bias, in <i>Proceedings of the 5th IEEE International Conference on Data Mining (ICDM)</i> (Brussels, 2012), pp. 605–608</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="J Gao, W Fan, J Jiang, J Han, Knowledge transfer via multiple model local structure mapping, in Proceedings of" /><span class="c-article-references__counter">53.</span><p class="c-article-references__text" id="ref-CR53">J Gao, W Fan, J Jiang, J Han, Knowledge transfer via multiple model local structure mapping, in <i>Proceedings of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</i> (Las Vegas, 2008), pp. 283-291</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="C Wang, S Mahadevan, Manifold alignment using procrustes analysis, in Proceedings of the 25th International Co" /><span class="c-article-references__counter">54.</span><p class="c-article-references__text" id="ref-CR54">C Wang, S Mahadevan, Manifold alignment using procrustes analysis, in <i>Proceedings of the 25th International Conference on Machine Learning (ICML)</i> (Helsinki, 2008), pp. 1120–1127</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="X Ling, W Dai, GR Xue, Q Yang, Y Yu, Spectral domain-transfer learning, in Proceedings of the 14th ACM SIGKDD " /><span class="c-article-references__counter">55.</span><p class="c-article-references__text" id="ref-CR55">X Ling, W Dai, GR Xue, Q Yang, Y Yu, Spectral domain-transfer learning, in <i>Proceedings of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</i> (Las Vegas, 2008), pp. 488–496</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="R Raina, AY Ng, D Koller, 2006, Constructing informative priors using transfer learning, in Proceedings of the" /><span class="c-article-references__counter">56.</span><p class="c-article-references__text" id="ref-CR56">R Raina, AY Ng, D Koller, 2006, Constructing informative priors using transfer learning, in <i>Proceedings of the 23rd International Conference on Machine Learning (ICML)</i> (Pittsburgh, 2006), pp. 713–720</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content=" J Zhang, Deep transfer learning via restricted Boltzmann machine for document classification, in Proceedings " /><span class="c-article-references__counter">57.</span><p class="c-article-references__text" id="ref-CR57"> J Zhang, Deep transfer learning via restricted Boltzmann machine for document classification, in <i>Proceedings of the 10th International Conference on Machine Learning and Applications and Workshops (ICMLA)</i> (Honolulu, 2011), pp. 323–326</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="Y. Fu, B. Li, X. Zhu, C. Zhang, " /><meta itemprop="datePublished" content="2014" /><meta itemprop="headline" content="Y Fu, B Li, X Zhu, C Zhang, Active learning without knowing individual instance labels: a pairwise label homog" /><span class="c-article-references__counter">58.</span><p class="c-article-references__text" id="ref-CR58">Y Fu, B Li, X Zhu, C Zhang, Active learning without knowing individual instance labels: a pairwise label homogeneity query approach. IEEE Trans Knowl Data Eng <b>26</b>(4), 808–822 (2014)</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="https://doi.org/10.1109%2FTKDE.2013.165" aria-label="View reference 58">Article</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 58 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Active%20learning%20without%20knowing%20individual%20instance%20labels%3A%20a%20pairwise%20label%20homogeneity%20query%20approach&amp;journal=IEEE%20Trans%20Knowl%20Data%20Eng&amp;volume=26&amp;issue=4&amp;pages=808-822&amp;publication_year=2014&amp;author=Fu%2CY&amp;author=Li%2CB&amp;author=Zhu%2CX&amp;author=Zhang%2CC">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="B. Settles, " /><meta itemprop="datePublished" content="2010" /><meta itemprop="headline" content="B Settles, Active learning literature survey (University of Wisconsin, Madison, 2010)" /><span class="c-article-references__counter">59.</span><p class="c-article-references__text" id="ref-CR59">B Settles, <i>Active learning literature survey</i> (University of Wisconsin, Madison, 2010)</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 59 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Active%20learning%20literature%20survey&amp;publication_year=2010&amp;author=Settles%2CB">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="MM. Crawford, D. Tuia, HL. Yang, " /><meta itemprop="datePublished" content="2013" /><meta itemprop="headline" content="MM Crawford, D Tuia, HL Yang, Active learning: any value for classification of remotely sensed data? P IEEE 10" /><span class="c-article-references__counter">60.</span><p class="c-article-references__text" id="ref-CR60">MM Crawford, D Tuia, HL Yang, Active learning: any value for classification of remotely sensed data? P IEEE <b>101</b>(3), 593–608 (2013)</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="https://doi.org/10.1109%2FJPROC.2012.2231951" aria-label="View reference 60">Article</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 60 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Active%20learning%3A%20any%20value%20for%20classification%20of%20remotely%20sensed%20data%3F&amp;journal=P%20IEEE&amp;volume=101&amp;issue=3&amp;pages=593-608&amp;publication_year=2013&amp;author=Crawford%2CMM&amp;author=Tuia%2CD&amp;author=Yang%2CHL">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="MM. Haque, LB. Holder, MK. Skinner, DJ. Cook, " /><meta itemprop="datePublished" content="2013" /><meta itemprop="headline" content="MM Haque, LB Holder, MK Skinner, DJ Cook, Generalized query-based active learning to identify differentially m" /><span class="c-article-references__counter">61.</span><p class="c-article-references__text" id="ref-CR61">MM Haque, LB Holder, MK Skinner, DJ Cook, Generalized query-based active learning to identify differentially methylated regions in DNA. IEEE ACM Trans Comput Bi <b>10</b>(3), 632–644 (2013)</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 61 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Generalized%20query-based%20active%20learning%20to%20identify%20differentially%20methylated%20regions%20in%20DNA&amp;journal=IEEE%20ACM%20Trans%20Comput%20Bi&amp;volume=10&amp;issue=3&amp;pages=632-644&amp;publication_year=2013&amp;author=Haque%2CMM&amp;author=Holder%2CLB&amp;author=Skinner%2CMK&amp;author=Cook%2CDJ">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="D. Tuia, M. Volpi, L. Copa, M. Kanevski, J. Munoz-Mari, " /><meta itemprop="datePublished" content="2011" /><meta itemprop="headline" content="D Tuia, M Volpi, L Copa, M Kanevski, J Munoz-Mari, A survey of active learning algorithms for supervised remot" /><span class="c-article-references__counter">62.</span><p class="c-article-references__text" id="ref-CR62">D Tuia, M Volpi, L Copa, M Kanevski, J Munoz-Mari, A survey of active learning algorithms for supervised remote sensing image classification. IEEE J Sel Top Sign Proces <b>5</b>(3), 606–617 (2011)</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="https://doi.org/10.1109%2FJSTSP.2011.2139193" aria-label="View reference 62">Article</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 62 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20survey%20of%20active%20learning%20algorithms%20for%20supervised%20remote%20sensing%20image%20classification&amp;journal=IEEE%20J%20Sel%20Top%20Sign%20Proces&amp;volume=5&amp;issue=3&amp;pages=606-617&amp;publication_year=2011&amp;author=Tuia%2CD&amp;author=Volpi%2CM&amp;author=Copa%2CL&amp;author=Kanevski%2CM&amp;author=Munoz-Mari%2CJ">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="G. Ding, Q. Wu, YD. Yao, J. Wang, Y. Chen, " /><meta itemprop="datePublished" content="2013" /><meta itemprop="headline" content="G Ding, Q Wu, YD Yao, J Wang, Y Chen, Kernel-based learning for statistical signal processing in cognitive rad" /><span class="c-article-references__counter">63.</span><p class="c-article-references__text" id="ref-CR63">G Ding, Q Wu, YD Yao, J Wang, Y Chen, Kernel-based learning for statistical signal processing in cognitive radio networks. IEEE Signal Proc Mag <b>30</b>(4), 126–136 (2013)</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="https://doi.org/10.1109%2FMSP.2013.2251071" aria-label="View reference 63">Article</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 63 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Kernel-based%20learning%20for%20statistical%20signal%20processing%20in%20cognitive%20radio%20networks&amp;journal=IEEE%20Signal%20Proc%20Mag&amp;volume=30&amp;issue=4&amp;pages=126-136&amp;publication_year=2013&amp;author=Ding%2CG&amp;author=Wu%2CQ&amp;author=Yao%2CYD&amp;author=Wang%2CJ&amp;author=Chen%2CY">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="C. Li, M. Georgiopoulos, GC. Anagnostopoulos, " /><meta itemprop="datePublished" content="2014" /><meta itemprop="headline" content="C Li, M Georgiopoulos, GC Anagnostopoulos, A unifying framework for typical multitask multiple kernel learning" /><span class="c-article-references__counter">64.</span><p class="c-article-references__text" id="ref-CR64">C Li, M Georgiopoulos, GC Anagnostopoulos, A unifying framework for typical multitask multiple kernel learning problems. IEEE Trans Neur Net Lear Syst <b>25</b>(7), 1287–1297 (2014)</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="http://www.ams.org/mathscinet-getitem?mr=3479914" aria-label="View reference 64 on MathSciNet">MathSciNet</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="https://doi.org/10.1109%2FTNNLS.2013.2291772" aria-label="View reference 64">Article</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 64 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20unifying%20framework%20for%20typical%20multitask%20multiple%20kernel%20learning%20problems&amp;journal=IEEE%20Trans%20Neur%20Net%20Lear%20Syst&amp;volume=25&amp;issue=7&amp;pages=1287-1297&amp;publication_year=2014&amp;author=Li%2CC&amp;author=Georgiopoulos%2CM&amp;author=Anagnostopoulos%2CGC">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="G. Montavon, M. Braun, T. Krueger, KR. Muller, " /><meta itemprop="datePublished" content="2013" /><meta itemprop="headline" content="G Montavon, M Braun, T Krueger, KR Muller, Analyzing local structure in kernel-based learning: explanation, co" /><span class="c-article-references__counter">65.</span><p class="c-article-references__text" id="ref-CR65">G Montavon, M Braun, T Krueger, KR Muller, Analyzing local structure in kernel-based learning: explanation, complexity, and reliability assessment. IEEE Signal Proc Mag <b>30</b>(4), 62–74 (2013)</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="https://doi.org/10.1109%2FMSP.2013.2249294" aria-label="View reference 65">Article</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 65 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Analyzing%20local%20structure%20in%20kernel-based%20learning%3A%20explanation%2C%20complexity%2C%20and%20reliability%20assessment&amp;journal=IEEE%20Signal%20Proc%20Mag&amp;volume=30&amp;issue=4&amp;pages=62-74&amp;publication_year=2013&amp;author=Montavon%2CG&amp;author=Braun%2CM&amp;author=Krueger%2CT&amp;author=Muller%2CKR">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="K. Slavakis, S. Theodoridis, I. Yamada, " /><meta itemprop="datePublished" content="2008" /><meta itemprop="headline" content="K Slavakis, S Theodoridis, I Yamada, Online kernel-based classification using adaptive projection algorithms. " /><span class="c-article-references__counter">66.</span><p class="c-article-references__text" id="ref-CR66">K Slavakis, S Theodoridis, I Yamada, Online kernel-based classification using adaptive projection algorithms. IEEE Trans Signal Process <b>56</b>(7), 2781–2796 (2008)</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="http://www.ams.org/mathscinet-getitem?mr=1500250" aria-label="View reference 66 on MathSciNet">MathSciNet</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="https://doi.org/10.1109%2FTSP.2008.917376" aria-label="View reference 66">Article</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 66 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Online%20kernel-based%20classification%20using%20adaptive%20projection%20algorithms&amp;journal=IEEE%20Trans%20Signal%20Process&amp;volume=56&amp;issue=7&amp;pages=2781-2796&amp;publication_year=2008&amp;author=Slavakis%2CK&amp;author=Theodoridis%2CS&amp;author=Yamada%2CI">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="S. Theodoridis, K. Slavakis, I. Yamada, " /><meta itemprop="datePublished" content="2011" /><meta itemprop="headline" content="S Theodoridis, K Slavakis, I Yamada, Adaptive learning in a world of projections. IEEE Signal Proc Mag 28(1), " /><span class="c-article-references__counter">67.</span><p class="c-article-references__text" id="ref-CR67">S Theodoridis, K Slavakis, I Yamada, Adaptive learning in a world of projections. IEEE Signal Proc Mag <b>28</b>(1), 97–123 (2011)</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="https://doi.org/10.1109%2FMSP.2010.938752" aria-label="View reference 67">Article</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 67 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Adaptive%20learning%20in%20a%20world%20of%20projections&amp;journal=IEEE%20Signal%20Proc%20Mag&amp;volume=28&amp;issue=1&amp;pages=97-123&amp;publication_year=2011&amp;author=Theodoridis%2CS&amp;author=Slavakis%2CK&amp;author=Yamada%2CI">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="K. Slavakis, S. Theodoridis, I. Yamada, " /><meta itemprop="datePublished" content="2009" /><meta itemprop="headline" content="K Slavakis, S Theodoridis, I Yamada, Adaptive constrained learning in reproducing kernel Hilbert spaces: the r" /><span class="c-article-references__counter">68.</span><p class="c-article-references__text" id="ref-CR68">K Slavakis, S Theodoridis, I Yamada, Adaptive constrained learning in reproducing kernel Hilbert spaces: the robust beamforming case. IEEE Trans Signal Process <b>57</b>(12), 4744–4764 (2009)</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="http://www.ams.org/mathscinet-getitem?mr=2722332" aria-label="View reference 68 on MathSciNet">MathSciNet</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="https://doi.org/10.1109%2FTSP.2009.2027771" aria-label="View reference 68">Article</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 68 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Adaptive%20constrained%20learning%20in%20reproducing%20kernel%20Hilbert%20spaces%3A%20the%20robust%20beamforming%20case&amp;journal=IEEE%20Trans%20Signal%20Process&amp;volume=57&amp;issue=12&amp;pages=4744-4764&amp;publication_year=2009&amp;author=Slavakis%2CK&amp;author=Theodoridis%2CS&amp;author=Yamada%2CI">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="K. Slavakis, P. Bouboulis, S. Theodoridis, " /><meta itemprop="datePublished" content="2012" /><meta itemprop="headline" content="K Slavakis, P Bouboulis, S Theodoridis, Adaptive multiregression in reproducing kernel Hilbert spaces: the mul" /><span class="c-article-references__counter">69.</span><p class="c-article-references__text" id="ref-CR69">K Slavakis, P Bouboulis, S Theodoridis, Adaptive multiregression in reproducing kernel Hilbert spaces: the multiaccess MIMO channel case. IEEE Trans Neural Netw Learn Syst <b>23</b>(2), 260–276 (2012)</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="https://doi.org/10.1109%2FTNNLS.2011.2178321" aria-label="View reference 69">Article</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 69 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Adaptive%20multiregression%20in%20reproducing%20kernel%20Hilbert%20spaces%3A%20the%20multiaccess%20MIMO%20channel%20case&amp;journal=IEEE%20Trans%20Neural%20Netw%20Learn%20Syst&amp;volume=23&amp;issue=2&amp;pages=260-276&amp;publication_year=2012&amp;author=Slavakis%2CK&amp;author=Bouboulis%2CP&amp;author=Theodoridis%2CS">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="KR. Müller, S. Mika, G. Rätsch, K. Tsuda, B. Schölkopf, " /><meta itemprop="datePublished" content="2001" /><meta itemprop="headline" content="KR Müller, S Mika, G Rätsch, K Tsuda, B Schölkopf, An introduction to kernel-based learning algorithms. IEEE T" /><span class="c-article-references__counter">70.</span><p class="c-article-references__text" id="ref-CR70">KR Müller, S Mika, G Rätsch, K Tsuda, B Schölkopf, An introduction to kernel-based learning algorithms. IEEE Trans Neural Networ <b>12</b>(2), 181–201 (2001)</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="https://doi.org/10.1109%2F72.914517" aria-label="View reference 70">Article</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 70 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=An%20introduction%20to%20kernel-based%20learning%20algorithms&amp;journal=IEEE%20Trans%20Neural%20Networ&amp;volume=12&amp;issue=2&amp;pages=181-201&amp;publication_year=2001&amp;author=M%C3%BCller%2CKR&amp;author=Mika%2CS&amp;author=R%C3%A4tsch%2CG&amp;author=Tsuda%2CK&amp;author=Sch%C3%B6lkopf%2CB">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="TH. Davenport, P. Barth, R. Bean, " /><meta itemprop="datePublished" content="2012" /><meta itemprop="headline" content="TH Davenport, P Barth, R Bean, How “big data” is different. MIT Sloan Manage Rev 54(1), 22–24 (2012)" /><span class="c-article-references__counter">71.</span><p class="c-article-references__text" id="ref-CR71">TH Davenport, P Barth, R Bean, How “big data” is different. MIT Sloan Manage Rev <b>54</b>(1), 22–24 (2012)</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 71 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=How%20%E2%80%9Cbig%20data%E2%80%9D%20is%20different&amp;journal=MIT%20Sloan%20Manage%20Rev&amp;volume=54&amp;issue=1&amp;pages=22-24&amp;publication_year=2012&amp;author=Davenport%2CTH&amp;author=Barth%2CP&amp;author=Bean%2CR">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="F. Andersson, M. Carlsson, JY. Tourneret, H. Wendt, " /><meta itemprop="datePublished" content="2014" /><meta itemprop="headline" content="F Andersson, M Carlsson, JY Tourneret, H Wendt, A new frequency estimation method for equally and unequally sp" /><span class="c-article-references__counter">72.</span><p class="c-article-references__text" id="ref-CR72">F Andersson, M Carlsson, JY Tourneret, H Wendt, A new frequency estimation method for equally and unequally spaced data. IEEE Trans Signal Process <b>62</b>(21), 5761–5774 (2014)</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="http://www.ams.org/mathscinet-getitem?mr=3273530" aria-label="View reference 72 on MathSciNet">MathSciNet</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="https://doi.org/10.1109%2FTSP.2014.2358961" aria-label="View reference 72">Article</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 72 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20new%20frequency%20estimation%20method%20for%20equally%20and%20unequally%20spaced%20data&amp;journal=IEEE%20Trans%20Signal%20Process&amp;volume=62&amp;issue=21&amp;pages=5761-5774&amp;publication_year=2014&amp;author=Andersson%2CF&amp;author=Carlsson%2CM&amp;author=Tourneret%2CJY&amp;author=Wendt%2CH">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="F. Lin, M. Fardad, MR. Jovanovic, " /><meta itemprop="datePublished" content="2013" /><meta itemprop="headline" content="F Lin, M Fardad, MR Jovanovic, Design of optimal sparse feedback gains via the alternating direction method of" /><span class="c-article-references__counter">73.</span><p class="c-article-references__text" id="ref-CR73">F Lin, M Fardad, MR Jovanovic, Design of optimal sparse feedback gains via the alternating direction method of multipliers. IEEE Trans Automat Contr <b>58</b>(9), 2426–2431 (2013)</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="http://www.ams.org/mathscinet-getitem?mr=3247161" aria-label="View reference 73 on MathSciNet">MathSciNet</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="https://doi.org/10.1109%2FTAC.2013.2257618" aria-label="View reference 73">Article</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 73 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Design%20of%20optimal%20sparse%20feedback%20gains%20via%20the%20alternating%20direction%20method%20of%20multipliers&amp;journal=IEEE%20Trans%20Automat%20Contr&amp;volume=58&amp;issue=9&amp;pages=2426-2431&amp;publication_year=2013&amp;author=Lin%2CF&amp;author=Fardad%2CM&amp;author=Jovanovic%2CMR">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="S. Boyd, N. Parikh, E. Chu, B. Peleato, J. Eckstein, " /><meta itemprop="datePublished" content="2011" /><meta itemprop="headline" content="S Boyd, N Parikh, E Chu, B Peleato, J Eckstein, Distributed optimization and statistical learning via the alte" /><span class="c-article-references__counter">74.</span><p class="c-article-references__text" id="ref-CR74">S Boyd, N Parikh, E Chu, B Peleato, J Eckstein, Distributed optimization and statistical learning via the alternating direction method of multipliers. Foundations Trends Mach Learn <b>3</b>(1), 1–122 (2011)</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="https://doi.org/10.1561%2F2200000016" aria-label="View reference 74">Article</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="http://www.emis.de/MATH-item?1229.90122" aria-label="View reference 74 on MATH">MATH</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 74 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Distributed%20optimization%20and%20statistical%20learning%20via%20the%20alternating%20direction%20method%20of%20multipliers&amp;journal=Foundations%20Trends%20Mach%20Learn&amp;volume=3&amp;issue=1&amp;pages=1-122&amp;publication_year=2011&amp;author=Boyd%2CS&amp;author=Parikh%2CN&amp;author=Chu%2CE&amp;author=Peleato%2CB&amp;author=Eckstein%2CJ">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="J. Dean, S. Ghemawat, " /><meta itemprop="datePublished" content="2008" /><meta itemprop="headline" content="J Dean, S Ghemawat, MapReduce: simplified data processing on large clusters. Commun ACM 51(1), 107–113 (2008)" /><span class="c-article-references__counter">75.</span><p class="c-article-references__text" id="ref-CR75">J Dean, S Ghemawat, MapReduce: simplified data processing on large clusters. Commun ACM <b>51</b>(1), 107–113 (2008)</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="https://doi.org/10.1145%2F1327452.1327492" aria-label="View reference 75">Article</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 75 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=MapReduce%3A%20simplified%20data%20processing%20on%20large%20clusters&amp;journal=Commun%20ACM&amp;volume=51&amp;issue=1&amp;pages=107-113&amp;publication_year=2008&amp;author=Dean%2CJ&amp;author=Ghemawat%2CS">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="J. Dean, S. Ghemawat, " /><meta itemprop="datePublished" content="2010" /><meta itemprop="headline" content="J Dean, S Ghemawat, MapReduce: a flexible data processing tool. Commun ACM 53(1), 72–77 (2010)" /><span class="c-article-references__counter">76.</span><p class="c-article-references__text" id="ref-CR76">J Dean, S Ghemawat, MapReduce: a flexible data processing tool. Commun ACM <b>53</b>(1), 72–77 (2010)</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="https://doi.org/10.1145%2F1629175.1629198" aria-label="View reference 76">Article</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 76 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=MapReduce%3A%20a%20flexible%20data%20processing%20tool&amp;journal=Commun%20ACM&amp;volume=53&amp;issue=1&amp;pages=72-77&amp;publication_year=2010&amp;author=Dean%2CJ&amp;author=Ghemawat%2CS">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="C Chu, SK Kim, YA Lin, Y Yu, G Bradski, AY Ng, K Olukotun, Map-reduce for machine learning on multicore, in Pr" /><span class="c-article-references__counter">77.</span><p class="c-article-references__text" id="ref-CR77">C Chu, SK Kim, YA Lin, Y Yu, G Bradski, AY Ng, K Olukotun, Map-reduce for machine learning on multicore, in <i>Proceedings of 20th Annual Conference on Neural Information Processing Systems (NIPS)</i> (Vancouver, 2006), pp. 281–288</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="M. Armbrust, A. Fox, R. Griffith, AD. Joseph, R. Katz, A. Konwinski, G. Lee, D. Patterson, A. Rabkin, I. Stoica, M. Zaharia, " /><meta itemprop="datePublished" content="2010" /><meta itemprop="headline" content="M Armbrust, A Fox, R Griffith, AD Joseph, R Katz, A Konwinski, G Lee, D Patterson, A Rabkin, I Stoica, M Zahar" /><span class="c-article-references__counter">78.</span><p class="c-article-references__text" id="ref-CR78">M Armbrust, A Fox, R Griffith, AD Joseph, R Katz, A Konwinski, G Lee, D Patterson, A Rabkin, I Stoica, M Zaharia, A view of cloud computing. Commun ACM <b>53</b>(4), 50–58 (2010)</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="https://doi.org/10.1145%2F1721654.1721672" aria-label="View reference 78">Article</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 78 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20view%20of%20cloud%20computing&amp;journal=Commun%20ACM&amp;volume=53&amp;issue=4&amp;pages=50-58&amp;publication_year=2010&amp;author=Armbrust%2CM&amp;author=Fox%2CA&amp;author=Griffith%2CR&amp;author=Joseph%2CAD&amp;author=Katz%2CR&amp;author=Konwinski%2CA&amp;author=Lee%2CG&amp;author=Patterson%2CD&amp;author=Rabkin%2CA&amp;author=Stoica%2CI&amp;author=Zaharia%2CM">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="MD. Dikaiakos, D. Katsaros, P. Mehra, G. Pallis, A. Vakali, " /><meta itemprop="datePublished" content="2009" /><meta itemprop="headline" content="MD Dikaiakos, D Katsaros, P Mehra, G Pallis, A Vakali, Cloud computing: distributed internet computing for IT " /><span class="c-article-references__counter">79.</span><p class="c-article-references__text" id="ref-CR79">MD Dikaiakos, D Katsaros, P Mehra, G Pallis, A Vakali, Cloud computing: distributed internet computing for IT and scientific research. IEEE Internet Comput <b>13</b>(5), 10–13 (2009)</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="https://doi.org/10.1109%2FMIC.2009.103" aria-label="View reference 79">Article</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 79 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Cloud%20computing%3A%20distributed%20internet%20computing%20for%20IT%20and%20scientific%20research&amp;journal=IEEE%20Internet%20Comput&amp;volume=13&amp;issue=5&amp;pages=10-13&amp;publication_year=2009&amp;author=Dikaiakos%2CMD&amp;author=Katsaros%2CD&amp;author=Mehra%2CP&amp;author=Pallis%2CG&amp;author=Vakali%2CA">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="Y. Low, D. Bickson, J. Gonzalez, C. Guestrin, A. Kyrola, JM. Hellerstein, " /><meta itemprop="datePublished" content="2012" /><meta itemprop="headline" content="Y Low, D Bickson, J Gonzalez, C Guestrin, A Kyrola, JM Hellerstein, Distributed GraphLab: a framework for mach" /><span class="c-article-references__counter">80.</span><p class="c-article-references__text" id="ref-CR80">Y Low, D Bickson, J Gonzalez, C Guestrin, A Kyrola, JM Hellerstein, Distributed GraphLab: a framework for machine learning and data mining in the cloud. Proc VLDB Endow <b>5</b>(8), 716–727 (2012)</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="https://doi.org/10.14778%2F2212351.2212354" aria-label="View reference 80">Article</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 80 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Distributed%20GraphLab%3A%20a%20framework%20for%20machine%20learning%20and%20data%20mining%20in%20the%20cloud&amp;journal=Proc%20VLDB%20Endow&amp;volume=5&amp;issue=8&amp;pages=716-727&amp;publication_year=2012&amp;author=Low%2CY&amp;author=Bickson%2CD&amp;author=Gonzalez%2CJ&amp;author=Guestrin%2CC&amp;author=Kyrola%2CA&amp;author=Hellerstein%2CJM">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="M Lenzerini, Data integration: a theoretical perspective, in Proceedings of the twenty-first ACM SIGMOD-SIGACT" /><span class="c-article-references__counter">81.</span><p class="c-article-references__text" id="ref-CR81">M Lenzerini, Data integration: a theoretical perspective, in <i>Proceedings of the twenty-first ACM SIGMOD-SIGACT-SIGART Symposium on Principles of Database Systems</i> (Madison, 2002), pp. 233–246</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="A Halevy, A Rajaraman, J Ordille, Data integration: the teenage years, in Proceedings of the 32nd Internationa" /><span class="c-article-references__counter">82.</span><p class="c-article-references__text" id="ref-CR82">A Halevy, A Rajaraman, J Ordille, Data integration: the teenage years, in <i>Proceedings of the 32nd International Conference on Very Large Data Bases (VLDB)</i> (Seoul, 2006), pp. 9–16</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="Q. Wu, G. Ding, J. Wang, YD. Yao, " /><meta itemprop="datePublished" content="2013" /><meta itemprop="headline" content="Q Wu, G Ding, J Wang, YD Yao, Spatial-temporal opportunity detection for spectrum-heterogeneous cognitive radi" /><span class="c-article-references__counter">83.</span><p class="c-article-references__text" id="ref-CR83">Q Wu, G Ding, J Wang, YD Yao, Spatial-temporal opportunity detection for spectrum-heterogeneous cognitive radio networks: two-dimensional sensing. IEEE Trans Wirel Commun <b>12</b>(2), 516–526 (2013)</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="https://doi.org/10.1109%2FTWC.2012.122212.111638" aria-label="View reference 83">Article</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 83 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Spatial-temporal%20opportunity%20detection%20for%20spectrum-heterogeneous%20cognitive%20radio%20networks%3A%20two-dimensional%20sensing&amp;journal=IEEE%20Trans%20Wirel%20Commun&amp;volume=12&amp;issue=2&amp;pages=516-526&amp;publication_year=2013&amp;author=Wu%2CQ&amp;author=Ding%2CG&amp;author=Wang%2CJ&amp;author=Yao%2CYD">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="N Srivastava, RR Salakhutdinov, Multimodal learning with deep boltzmann machines, in Proceedings of Neural Inf" /><span class="c-article-references__counter">84.</span><p class="c-article-references__text" id="ref-CR84">N Srivastava, RR Salakhutdinov, Multimodal learning with deep boltzmann machines, in <i>Proceedings of Neural Information Processing Systems Conference (NIPS)</i> (Nevada, 2012), pp. 2222–2230</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="Y. Sun, S. Todorovic, S. Goodison, " /><meta itemprop="datePublished" content="2010" /><meta itemprop="headline" content="Y Sun, S Todorovic, S Goodison, Local-learning-based feature selection for high-dimensional data analysis. IEE" /><span class="c-article-references__counter">85.</span><p class="c-article-references__text" id="ref-CR85">Y Sun, S Todorovic, S Goodison, Local-learning-based feature selection for high-dimensional data analysis. IEEE Trans Pattern Anal Mach Intell <b>32</b>(9), 1610–1626 (2010)</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="https://doi.org/10.1109%2FTPAMI.2009.190" aria-label="View reference 85">Article</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 85 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Local-learning-based%20feature%20selection%20for%20high-dimensional%20data%20analysis&amp;journal=IEEE%20Trans%20Pattern%20Anal%20Mach%20Intell&amp;volume=32&amp;issue=9&amp;pages=1610-1626&amp;publication_year=2010&amp;author=Sun%2CY&amp;author=Todorovic%2CS&amp;author=Goodison%2CS">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="LJP. Maaten, EO. Postma, HJ. Herik, " /><meta itemprop="datePublished" content="2009" /><meta itemprop="headline" content="LJP van der Maaten, EO Postma, HJ van den Herik, Dimensionality reduction: a comparative review. J Mach Learn " /><span class="c-article-references__counter">86.</span><p class="c-article-references__text" id="ref-CR86">LJP van der Maaten, EO Postma, HJ van den Herik, Dimensionality reduction: a comparative review. J Mach Learn Res <b>10</b>(1-41), 66–71 (2009)</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 86 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Dimensionality%20reduction%3A%20a%20comparative%20review&amp;journal=J%20Mach%20Learn%20Res&amp;volume=10&amp;issue=1-41&amp;pages=66-71&amp;publication_year=2009&amp;author=Maaten%2CLJP&amp;author=Postma%2CEO&amp;author=Herik%2CHJ">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="M. Mardani, G. Mateos, GB. Giannakis, " /><meta itemprop="datePublished" content="2015" /><meta itemprop="headline" content="M Mardani, G Mateos, GB Giannakis, Subspace learning and imputation for streaming big data matrices and tensor" /><span class="c-article-references__counter">87.</span><p class="c-article-references__text" id="ref-CR87">M Mardani, G Mateos, GB Giannakis, Subspace learning and imputation for streaming big data matrices and tensors. IEEE Trans Signal Process <b>63</b>(10), 2663–2677 (2015)</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="http://www.ams.org/mathscinet-getitem?mr=3341765" aria-label="View reference 87 on MathSciNet">MathSciNet</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="https://doi.org/10.1109%2FTSP.2015.2417491" aria-label="View reference 87">Article</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 87 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Subspace%20learning%20and%20imputation%20for%20streaming%20big%20data%20matrices%20and%20tensors&amp;journal=IEEE%20Trans%20Signal%20Process&amp;volume=63&amp;issue=10&amp;pages=2663-2677&amp;publication_year=2015&amp;author=Mardani%2CM&amp;author=Mateos%2CG&amp;author=Giannakis%2CGB">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content=" K Mohan, M Fazel, New restricted isometry results for noisy low-rank recovery, in Proceedings of IEEE Interna" /><span class="c-article-references__counter">88.</span><p class="c-article-references__text" id="ref-CR88"> K Mohan, M Fazel, New restricted isometry results for noisy low-rank recovery, in <i>Proceedings of IEEE International Symposium on Information Theory Proceedings (ISIT)</i> (Texas, 2010), pp. 1573–1577</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="EJ. Candès, X. Li, Y. Ma, J. Wright, " /><meta itemprop="datePublished" content="2011" /><meta itemprop="headline" content="EJ Candès, X Li, Y Ma, J Wright, Robust principal component analysis? J ACM 58(3), 1–37 (2011)" /><span class="c-article-references__counter">89.</span><p class="c-article-references__text" id="ref-CR89">EJ Candès, X Li, Y Ma, J Wright, Robust principal component analysis? J ACM <b>58</b>(3), 1–37 (2011)</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="http://www.ams.org/mathscinet-getitem?mr=2811000" aria-label="View reference 89 on MathSciNet">MathSciNet</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="https://doi.org/10.1145%2F1970392.1970395" aria-label="View reference 89">Article</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="http://www.emis.de/MATH-item?1327.62369" aria-label="View reference 89 on MATH">MATH</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 89 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Robust%20principal%20component%20analysis%3F&amp;journal=J%20ACM&amp;volume=58&amp;issue=3&amp;pages=1-37&amp;publication_year=2011&amp;author=Cand%C3%A8s%2CEJ&amp;author=Li%2CX&amp;author=Ma%2CY&amp;author=Wright%2CJ">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Z Lin, R Liu, Z Su, Linearized alternating direction method with adaptive penalty for low-rank representation," /><span class="c-article-references__counter">90.</span><p class="c-article-references__text" id="ref-CR90">Z Lin, R Liu, Z Su, Linearized alternating direction method with adaptive penalty for low-rank representation, in <i>Proceedings of Neural Information Processing Systems Conference (NIPS)</i> (Granada, 2011), pp. 612–620</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="S. Shalev-Shwartz, " /><meta itemprop="datePublished" content="2011" /><meta itemprop="headline" content="S Shalev-Shwartz, Online learning and online convex optimization. Foundations Trends Mach Learn 4, 107–194 (20" /><span class="c-article-references__counter">91.</span><p class="c-article-references__text" id="ref-CR91">S Shalev-Shwartz, Online learning and online convex optimization. Foundations Trends Mach Learn <b>4</b>, 107–194 (2011)</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="https://doi.org/10.1561%2F2200000018" aria-label="View reference 91">Article</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="http://www.emis.de/MATH-item?1253.68190" aria-label="View reference 91 on MATH">MATH</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 91 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Online%20learning%20and%20online%20convex%20optimization&amp;journal=Foundations%20Trends%20Mach%20Learn&amp;volume=4&amp;pages=107-194&amp;publication_year=2011&amp;author=Shalev-Shwartz%2CS">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="J. Wang, P. Zhao, SC. Hoi, R. Jin, " /><meta itemprop="datePublished" content="2014" /><meta itemprop="headline" content="J Wang, P Zhao, SC Hoi, R Jin, Online feature selection and its applications. IEEE Trans Knowl Data Eng 26(3)," /><span class="c-article-references__counter">92.</span><p class="c-article-references__text" id="ref-CR92">J Wang, P Zhao, SC Hoi, R Jin, Online feature selection and its applications. IEEE Trans Knowl Data Eng <b>26</b>(3), 698–710 (2014)</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="https://doi.org/10.1109%2FTKDE.2013.32" aria-label="View reference 92">Article</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 92 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Online%20feature%20selection%20and%20its%20applications&amp;journal=IEEE%20Trans%20Knowl%20Data%20Eng&amp;volume=26&amp;issue=3&amp;pages=698-710&amp;publication_year=2014&amp;author=Wang%2CJ&amp;author=Zhao%2CP&amp;author=Hoi%2CSC&amp;author=Jin%2CR">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="J. Kivinen, AJ. Smola, RC. Williamson, " /><meta itemprop="datePublished" content="2004" /><meta itemprop="headline" content="J Kivinen, AJ Smola, RC Williamson, Online learning with kernels. IEEE Trans Signal Process 52(8), 2165–2176 (" /><span class="c-article-references__counter">93.</span><p class="c-article-references__text" id="ref-CR93">J Kivinen, AJ Smola, RC Williamson, Online learning with kernels. IEEE Trans Signal Process <b>52</b>(8), 2165–2176 (2004)</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="http://www.ams.org/mathscinet-getitem?mr=2085578" aria-label="View reference 93 on MathSciNet">MathSciNet</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="https://doi.org/10.1109%2FTSP.2004.830991" aria-label="View reference 93">Article</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 93 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Online%20learning%20with%20kernels&amp;journal=IEEE%20Trans%20Signal%20Process&amp;volume=52&amp;issue=8&amp;pages=2165-2176&amp;publication_year=2004&amp;author=Kivinen%2CJ&amp;author=Smola%2CAJ&amp;author=Williamson%2CRC">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="M Bilenko, S Basil, M Sahami, Adaptive product normalization: using online learning for record linkage in comp" /><span class="c-article-references__counter">94.</span><p class="c-article-references__text" id="ref-CR94">M Bilenko, S Basil, M Sahami, Adaptive product normalization: using online learning for record linkage in comparison shopping, in <i>Proceedings of the 5th IEEE International Conference on Data Mining (ICDM)</i> (Texas, 2005), p. 8</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="GB. Huang, QY. Zhu, CK. Siew, " /><meta itemprop="datePublished" content="2006" /><meta itemprop="headline" content="GB Huang, QY Zhu, CK Siew, Extreme learning machine: theory and applications. Neurocomputing 70(1), 489–501 (2" /><span class="c-article-references__counter">95.</span><p class="c-article-references__text" id="ref-CR95">GB Huang, QY Zhu, CK Siew, Extreme learning machine: theory and applications. Neurocomputing <b>70</b>(1), 489–501 (2006)</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="https://doi.org/10.1016%2Fj.neucom.2005.12.126" aria-label="View reference 95">Article</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 95 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Extreme%20learning%20machine%3A%20theory%20and%20applications&amp;journal=Neurocomputing&amp;volume=70&amp;issue=1&amp;pages=489-501&amp;publication_year=2006&amp;author=Huang%2CGB&amp;author=Zhu%2CQY&amp;author=Siew%2CCK">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="S. Ding, X. Xu, R. Nie, " /><meta itemprop="datePublished" content="2014" /><meta itemprop="headline" content="S Ding, X Xu, R Nie, Extreme learning machine and its applications. Neural Comput Appl 25(3-4), 549–556 (2014)" /><span class="c-article-references__counter">96.</span><p class="c-article-references__text" id="ref-CR96">S Ding, X Xu, R Nie, Extreme learning machine and its applications. Neural Comput Appl <b>25</b>(3-4), 549–556 (2014)</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="https://doi.org/10.1007%2Fs00521-013-1522-8" aria-label="View reference 96">Article</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 96 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Extreme%20learning%20machine%20and%20its%20applications&amp;journal=Neural%20Comput%20Appl&amp;volume=25&amp;issue=3-4&amp;pages=549-556&amp;publication_year=2014&amp;author=Ding%2CS&amp;author=Xu%2CX&amp;author=Nie%2CR">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="N Tatbul, Streaming data integration: challenges and opportunities, in Proceedings of the 26th IEEE Internatio" /><span class="c-article-references__counter">97.</span><p class="c-article-references__text" id="ref-CR97">N Tatbul, Streaming data integration: challenges and opportunities, in <i>Proceedings of the 26th IEEE International Conference on Data Engineering Workshops (ICDEW)</i> (Long Beach, 2010), pp. 155–158</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content=" DJ Abadi, Y Ahmad, M Balazinska, U Cetintemel, M Cherniack, JH Hwang, W Lindner, A Maskey, A Rasin, E Ryvkina" /><span class="c-article-references__counter">98.</span><p class="c-article-references__text" id="ref-CR98"> DJ Abadi, Y Ahmad, M Balazinska, U Cetintemel, M Cherniack, JH Hwang, W Lindner, A Maskey, A Rasin, E Ryvkina, N Tatbul, Y Xing, SB Zdonik, The design of the borealis stream processing engine, in <i>Proceedings of the Second Biennial Conference on Innovative Data Systems Research (CIDR)</i> (Asilomar, 2005), pp. 277–289</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="L Neumeyer, B Robbins, A Nair, A Kesari, S4: Distributed stream computing platform, in Proceedings of IEEE Int" /><span class="c-article-references__counter">99.</span><p class="c-article-references__text" id="ref-CR99">L Neumeyer, B Robbins, A Nair, A Kesari, S4: Distributed stream computing platform, in <i>Proceedings of IEEE International Conference on Data Mining Workshops (ICDMW)</i> (Sydney, 2010), pp. 170–177</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="K. Goodhope, J. Koshy, J. Kreps, N. Narkhede, R. Park, J. Rao, VY. Ye, " /><meta itemprop="datePublished" content="2012" /><meta itemprop="headline" content="K Goodhope, J Koshy, J Kreps, N Narkhede, R Park, J Rao, VY Ye, Building Linkedin’s real-time activity data pi" /><span class="c-article-references__counter">100.</span><p class="c-article-references__text" id="ref-CR100">K Goodhope, J Koshy, J Kreps, N Narkhede, R Park, J Rao, VY Ye, Building Linkedin’s real-time activity data pipeline. IEEE Data Eng Bull <b>35</b>(2), 33–45 (2012)</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 100 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Building%20Linkedin%E2%80%99s%20real-time%20activity%20data%20pipeline&amp;journal=IEEE%20Data%20Eng%20Bull&amp;volume=35&amp;issue=2&amp;pages=33-45&amp;publication_year=2012&amp;author=Goodhope%2CK&amp;author=Koshy%2CJ&amp;author=Kreps%2CJ&amp;author=Narkhede%2CN&amp;author=Park%2CR&amp;author=Rao%2CJ&amp;author=Ye%2CVY">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="W Yang, X Liu, L Zhang, LT Yang, Big data real-time processing based on storm, in Proceedings of the 12th IEEE" /><span class="c-article-references__counter">101.</span><p class="c-article-references__text" id="ref-CR101">W Yang, X Liu, L Zhang, LT Yang, Big data real-time processing based on storm, in <i>Proceedings of the 12th IEEE International Conference on Trust, Security and Privacy in Computing and Communications (TrustCom)</i> (Melbourne, 2013), pp. 1784–1787</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="B. SkieS, " /><meta itemprop="datePublished" content="2014" /><meta itemprop="headline" content="B SkieS, Streaming big data processing in datacenter clouds. IEEE Cloud Comput 1, 78–83 (2014)" /><span class="c-article-references__counter">102.</span><p class="c-article-references__text" id="ref-CR102">B SkieS, Streaming big data processing in datacenter clouds. IEEE Cloud Comput <b>1</b>, 78–83 (2014)</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 102 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Streaming%20big%20data%20processing%20in%20datacenter%20clouds&amp;journal=IEEE%20Cloud%20Comput&amp;volume=1&amp;pages=78-83&amp;publication_year=2014&amp;author=SkieS%2CB">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="A Baldominos, E Albacete, Y Saez, P Isasi, A scalable machine learning online service for big data real-time a" /><span class="c-article-references__counter">103.</span><p class="c-article-references__text" id="ref-CR103">A Baldominos, E Albacete, Y Saez, P Isasi, A scalable machine learning online service for big data real-time analysis, in <i>Proceedings of IEEE Symposium on Computational Intelligence in Big Data (CIBD)</i> (Orlando, 2014), pp. 1–8</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="NY. Soltani, SJ. Kim, GB. Giannakis, " /><meta itemprop="datePublished" content="2014" /><meta itemprop="headline" content="NY Soltani, SJ Kim, GB Giannakis, Real-time load elasticity tracking and pricing for electric vehicle charging" /><span class="c-article-references__counter">104.</span><p class="c-article-references__text" id="ref-CR104">NY Soltani, SJ Kim, GB Giannakis, Real-time load elasticity tracking and pricing for electric vehicle charging. IEEE Trans Smart Grid <b>6</b>(3), 1303–1313 (2014)</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="https://doi.org/10.1109%2FTSG.2014.2363837" aria-label="View reference 104">Article</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 104 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Real-time%20load%20elasticity%20tracking%20and%20pricing%20for%20electric%20vehicle%20charging&amp;journal=IEEE%20Trans%20Smart%20Grid&amp;volume=6&amp;issue=3&amp;pages=1303-1313&amp;publication_year=2014&amp;author=Soltani%2CNY&amp;author=Kim%2CSJ&amp;author=Giannakis%2CGB">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="S. Tsang, B. Kao, KY. Yip, WS. Ho, SD. Lee, " /><meta itemprop="datePublished" content="2011" /><meta itemprop="headline" content="S Tsang, B Kao, KY Yip, WS Ho, SD Lee, Decision trees for uncertain data. IEEE Trans Knowl Data Eng 23(1), 64–" /><span class="c-article-references__counter">105.</span><p class="c-article-references__text" id="ref-CR105">S Tsang, B Kao, KY Yip, WS Ho, SD Lee, Decision trees for uncertain data. IEEE Trans Knowl Data Eng <b>23</b>(1), 64–78 (2011)</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="https://doi.org/10.1109%2FTKDE.2009.175" aria-label="View reference 105">Article</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 105 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Decision%20trees%20for%20uncertain%20data&amp;journal=IEEE%20Trans%20Knowl%20Data%20Eng&amp;volume=23&amp;issue=1&amp;pages=64-78&amp;publication_year=2011&amp;author=Tsang%2CS&amp;author=Kao%2CB&amp;author=Yip%2CKY&amp;author=Ho%2CWS&amp;author=Lee%2CSD">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="F Nie, H Wang, X Cai, H Huang, C Ding, Robust matrix completion via joint schatten p-norm and lp-norm minimiza" /><span class="c-article-references__counter">106.</span><p class="c-article-references__text" id="ref-CR106">F Nie, H Wang, X Cai, H Huang, C Ding, Robust matrix completion via joint schatten p-norm and lp-norm minimization, in <i>Proceedings of the 12th IEEE International Conference on Data Mining (ICDM)</i> (Brussels, 2012), p. 566</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="G. Ding, J. Wang, Q. Wu, L. Zhang, Y. Zou, YD. Yao, Y. Chen, " /><meta itemprop="datePublished" content="2014" /><meta itemprop="headline" content="G Ding, J Wang, Q Wu, L Zhang, Y Zou, YD Yao, Y Chen, Robust spectrum sensing with crowd sensors. IEEE Trans C" /><span class="c-article-references__counter">107.</span><p class="c-article-references__text" id="ref-CR107">G Ding, J Wang, Q Wu, L Zhang, Y Zou, YD Yao, Y Chen, Robust spectrum sensing with crowd sensors. IEEE Trans Commun <b>62</b>(9), 3129–3143 (2014)</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="https://doi.org/10.1109%2FTCOMM.2014.2346775" aria-label="View reference 107">Article</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 107 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Robust%20spectrum%20sensing%20with%20crowd%20sensors&amp;journal=IEEE%20Trans%20Commun&amp;volume=62&amp;issue=9&amp;pages=3129-3143&amp;publication_year=2014&amp;author=Ding%2CG&amp;author=Wang%2CJ&amp;author=Wu%2CQ&amp;author=Zhang%2CL&amp;author=Zou%2CY&amp;author=Yao%2CYD&amp;author=Chen%2CY">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="U. Fayyad, G. Piatetsky-Shapiro, P. Smyth, " /><meta itemprop="datePublished" content="1996" /><meta itemprop="headline" content="U Fayyad, G Piatetsky-Shapiro, P Smyth, From data mining to knowledge discovery in databases. AI Mag 17(3), 37" /><span class="c-article-references__counter">108.</span><p class="c-article-references__text" id="ref-CR108">U Fayyad, G Piatetsky-Shapiro, P Smyth, From data mining to knowledge discovery in databases. AI Mag <b>17</b>(3), 37–54 (1996)</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 108 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=From%20data%20mining%20to%20knowledge%20discovery%20in%20databases&amp;journal=AI%20Mag&amp;volume=17&amp;issue=3&amp;pages=37-54&amp;publication_year=1996&amp;author=Fayyad%2CU&amp;author=Piatetsky-Shapiro%2CG&amp;author=Smyth%2CP">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="J. Kelly, S. Hamm, " /><meta itemprop="datePublished" content="2013" /><meta itemprop="headline" content="J Kelly III, S Hamm, Smart machines: IBM’s Watson and the era of cognitive computing (Columbia University Pres" /><span class="c-article-references__counter">109.</span><p class="c-article-references__text" id="ref-CR109">J Kelly III, S Hamm, <i>Smart machines: IBM’s Watson and the era of cognitive computing</i> (Columbia University Press, New York, 2013)</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 109 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Smart%20machines%3A%20IBM%E2%80%99s%20Watson%20and%20the%20era%20of%20cognitive%20computing&amp;publication_year=2013&amp;author=Kelly%2CJ&amp;author=Hamm%2CS">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="K. Slavakis, SJ. Kim, G. Mateos, GB. Giannakis, " /><meta itemprop="datePublished" content="2014" /><meta itemprop="headline" content="K Slavakis, SJ Kim, G Mateos, GB Giannakis, Stochastic approximation vis-a-vis online learning for big data an" /><span class="c-article-references__counter">110.</span><p class="c-article-references__text" id="ref-CR110">K Slavakis, SJ Kim, G Mateos, GB Giannakis, Stochastic approximation vis-a-vis online learning for big data analytics. IEEE Signal Proc Mag <b>31</b>(6), 124–129 (2014)</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="https://doi.org/10.1109%2FMSP.2014.2345536" aria-label="View reference 110">Article</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 110 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Stochastic%20approximation%20vis-a-vis%20online%20learning%20for%20big%20data%20analytics&amp;journal=IEEE%20Signal%20Proc%20Mag&amp;volume=31&amp;issue=6&amp;pages=124-129&amp;publication_year=2014&amp;author=Slavakis%2CK&amp;author=Kim%2CSJ&amp;author=Mateos%2CG&amp;author=Giannakis%2CGB">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="V. Cevher, S. Becker, M. Schmidt, " /><meta itemprop="datePublished" content="2014" /><meta itemprop="headline" content="V Cevher, S Becker, M Schmidt, Convex optimization for big data: scalable, randomized, and parallel algorithms" /><span class="c-article-references__counter">111.</span><p class="c-article-references__text" id="ref-CR111">V Cevher, S Becker, M Schmidt, Convex optimization for big data: scalable, randomized, and parallel algorithms for big data analytics. IEEE Signal Proc Mag <b>31</b>(5), 32–43 (2014)</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="https://doi.org/10.1109%2FMSP.2014.2329397" aria-label="View reference 111">Article</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 111 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Convex%20optimization%20for%20big%20data%3A%20scalable%2C%20randomized%2C%20and%20parallel%20algorithms%20for%20big%20data%20analytics&amp;journal=IEEE%20Signal%20Proc%20Mag&amp;volume=31&amp;issue=5&amp;pages=32-43&amp;publication_year=2014&amp;author=Cevher%2CV&amp;author=Becker%2CS&amp;author=Schmidt%2CM">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="A. Tajer, VV. Veeravalli, HV. Poor, " /><meta itemprop="datePublished" content="2014" /><meta itemprop="headline" content="A Tajer, VV Veeravalli, HV Poor, Outlying sequence detection in large data sets: a data-driven approach. IEEE " /><span class="c-article-references__counter">112.</span><p class="c-article-references__text" id="ref-CR112">A Tajer, VV Veeravalli, HV Poor, Outlying sequence detection in large data sets: a data-driven approach. IEEE Signal Proc Mag <b>31</b>(5), 44–56 (2014)</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="https://doi.org/10.1109%2FMSP.2014.2329428" aria-label="View reference 112">Article</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 112 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Outlying%20sequence%20detection%20in%20large%20data%20sets%3A%20a%20data-driven%20approach&amp;journal=IEEE%20Signal%20Proc%20Mag&amp;volume=31&amp;issue=5&amp;pages=44-56&amp;publication_year=2014&amp;author=Tajer%2CA&amp;author=Veeravalli%2CVV&amp;author=Poor%2CHV">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="S. Scardapane, D. Wang, M. Panella, A. Uncini, " /><meta itemprop="datePublished" content="2015" /><meta itemprop="headline" content="S Scardapane, D Wang, M Panella, A Uncini, Distributed learning for random vector functional-link networks. In" /><span class="c-article-references__counter">113.</span><p class="c-article-references__text" id="ref-CR113">S Scardapane, D Wang, M Panella, A Uncini, Distributed learning for random vector functional-link networks. Inf Sci <b>301</b>, 271–284 (2015)</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="http://www.ams.org/mathscinet-getitem?mr=3311793" aria-label="View reference 113 on MathSciNet">MathSciNet</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="https://doi.org/10.1016%2Fj.ins.2015.01.007" aria-label="View reference 113">Article</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 113 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Distributed%20learning%20for%20random%20vector%20functional-link%20networks&amp;journal=Inf%20Sci&amp;volume=301&amp;pages=271-284&amp;publication_year=2015&amp;author=Scardapane%2CS&amp;author=Wang%2CD&amp;author=Panella%2CM&amp;author=Uncini%2CA">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="A. Daneshmand, F. Facchinei, V. Kungurtsev, G. Scutari, " /><meta itemprop="datePublished" content="2015" /><meta itemprop="headline" content="A Daneshmand, F Facchinei, V Kungurtsev, G Scutari, Hybrid random/deterministic parallel algorithms for noncon" /><span class="c-article-references__counter">114.</span><p class="c-article-references__text" id="ref-CR114">A Daneshmand, F Facchinei, V Kungurtsev, G Scutari, Hybrid random/deterministic parallel algorithms for nonconvex big data optimization. IEEE Trans Signal Process <b>63</b>(15), 3914–3929 (2015)</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="http://www.ams.org/mathscinet-getitem?mr=3359873" aria-label="View reference 114 on MathSciNet">MathSciNet</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="https://doi.org/10.1109%2FTSP.2015.2436357" aria-label="View reference 114">Article</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 114 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Hybrid%20random%2Fdeterministic%20parallel%20algorithms%20for%20nonconvex%20big%20data%20optimization&amp;journal=IEEE%20Trans%20Signal%20Process&amp;volume=63&amp;issue=15&amp;pages=3914-3929&amp;publication_year=2015&amp;author=Daneshmand%2CA&amp;author=Facchinei%2CF&amp;author=Kungurtsev%2CV&amp;author=Scutari%2CG">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="P. Bianchi, W. Hachem, F. Iutzeler, A stochastic coordinate descent primal-dual algorithm and applications to " /><span class="c-article-references__counter">115.</span><p class="c-article-references__text" id="ref-CR115">P. Bianchi, W. Hachem, F. Iutzeler, A stochastic coordinate descent primal-dual algorithm and applications to large-scale composite optimization. arXiv preprint (2014). arXiv:1407.0898</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="HT Wai, TH Chang, A Scaglione, A consensus-based decentralized algorithm for non-convex optimization with appl" /><span class="c-article-references__counter">116.</span><p class="c-article-references__text" id="ref-CR116">HT Wai, TH Chang, A Scaglione, A consensus-based decentralized algorithm for non-convex optimization with application to dictionary learning, in <i>Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</i> (South Brisbane, 2015), pp. 3546–3550</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="D. Berberidis, V. Kekatos, G.B. Giannakis, Online censoring for large-scale regressions with application to st" /><span class="c-article-references__counter">117.</span><p class="c-article-references__text" id="ref-CR117">D. Berberidis, V. Kekatos, G.B. Giannakis, Online censoring for large-scale regressions with application to streaming big data. arXiv preprint (2015). arXiv:1507.07536</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="K. Slavakis, G.B. Giannakis, Per-block-convex data modeling by accelerated stochastic approximation. arXiv pre" /><span class="c-article-references__counter">118.</span><p class="c-article-references__text" id="ref-CR118">K. Slavakis, G.B. Giannakis, Per-block-convex data modeling by accelerated stochastic approximation. arXiv preprint (2015). arXiv:1501.07315</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="KC. Chen, SL. Huang, L. Zheng, HV. Poor, " /><meta itemprop="datePublished" content="2015" /><meta itemprop="headline" content="KC Chen, SL Huang, L Zheng, HV Poor, Communication theoretic data analytics. IEEE J Sel Areas Commun 33(4), 66" /><span class="c-article-references__counter">119.</span><p class="c-article-references__text" id="ref-CR119">KC Chen, SL Huang, L Zheng, HV Poor, Communication theoretic data analytics. IEEE J Sel Areas Commun <b>33</b>(4), 663–675 (2015)</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="https://doi.org/10.1109%2FJSAC.2015.2393471" aria-label="View reference 119">Article</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 119 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Communication%20theoretic%20data%20analytics&amp;journal=IEEE%20J%20Sel%20Areas%20Commun&amp;volume=33&amp;issue=4&amp;pages=663-675&amp;publication_year=2015&amp;author=Chen%2CKC&amp;author=Huang%2CSL&amp;author=Zheng%2CL&amp;author=Poor%2CHV">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="J. Zheng, F. Shen, H. Fan, J. Zhao, " /><meta itemprop="datePublished" content="2013" /><meta itemprop="headline" content="J Zheng, F Shen, H Fan, J Zhao, An online incremental learning support vector machine for large-scale data. Ne" /><span class="c-article-references__counter">120.</span><p class="c-article-references__text" id="ref-CR120">J Zheng, F Shen, H Fan, J Zhao, An online incremental learning support vector machine for large-scale data. Neural Comput Appl <b>22</b>(5), 1023–1035 (2013)</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="https://doi.org/10.1007%2Fs00521-011-0793-1" aria-label="View reference 120">Article</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 120 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=An%20online%20incremental%20learning%20support%20vector%20machine%20for%20large-scale%20data&amp;journal=Neural%20Comput%20Appl&amp;volume=22&amp;issue=5&amp;pages=1023-1035&amp;publication_year=2013&amp;author=Zheng%2CJ&amp;author=Shen%2CF&amp;author=Fan%2CH&amp;author=Zhao%2CJ">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="C Ghosh, C Cordeiro, DP Agrawal, M Bhaskara Rao, Markov chain existence and hidden Markov models in spectrum s" /><span class="c-article-references__counter">121.</span><p class="c-article-references__text" id="ref-CR121">C Ghosh, C Cordeiro, DP Agrawal, M Bhaskara Rao, Markov chain existence and hidden Markov models in spectrum sensing, in <i>Proceedings of the IEEE International Conference on Pervasive Computing &amp; Communications (PERCOM)</i> (Galveston, 2009), pp. 1–6</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="K. Yue, Q. Fang, X. Wang, J. Li, W. Weiy, " /><meta itemprop="datePublished" content="2015" /><meta itemprop="headline" content="K Yue, Q Fang, X Wang, J Li, W Weiy, A parallel and incremental approach for data-intensive learning of Bayesi" /><span class="c-article-references__counter">122.</span><p class="c-article-references__text" id="ref-CR122">K Yue, Q Fang, X Wang, J Li, W Weiy, A parallel and incremental approach for data-intensive learning of Bayesian networks. IEEE Trans Cybern <b>99</b>, 1–15 (2015)</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="https://doi.org/10.1109%2FTCYB.2015.2478154" aria-label="View reference 122">Article</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 122 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20parallel%20and%20incremental%20approach%20for%20data-intensive%20learning%20of%20Bayesian%20networks&amp;journal=IEEE%20Trans%20Cybern&amp;volume=99&amp;pages=1-15&amp;publication_year=2015&amp;author=Yue%2CK&amp;author=Fang%2CQ&amp;author=Wang%2CX&amp;author=Li%2CJ&amp;author=Weiy%2CW">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="X Dong, Y Li, C Wu, Y Cai, A learner based on neural network for cognitive radio, in Proceedings of the 12th I" /><span class="c-article-references__counter">123.</span><p class="c-article-references__text" id="ref-CR123">X Dong, Y Li, C Wu, Y Cai, A learner based on neural network for cognitive radio, in <i>Proceedings of the 12th IEEE International Conference on Communication Technology (ICCT)</i> (Nanjing, 2010), pp. 893–896</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="A. El-Hajj, L. Safatly, M. Bkassiny, M. Husseini, " /><meta itemprop="datePublished" content="2014" /><meta itemprop="headline" content="A El-Hajj, L Safatly, M Bkassiny, M Husseini, Cognitive radio transceivers: RF, spectrum sensing, and learning" /><span class="c-article-references__counter">124.</span><p class="c-article-references__text" id="ref-CR124">A El-Hajj, L Safatly, M Bkassiny, M Husseini, Cognitive radio transceivers: RF, spectrum sensing, and learning algorithms review. Int J Antenn Propag <b>11</b>(5), 479–482 (2014)</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 124 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Cognitive%20radio%20transceivers%3A%20RF%2C%20spectrum%20sensing%2C%20and%20learning%20algorithms%20review&amp;journal=Int%20J%20Antenn%20Propag&amp;volume=11&amp;issue=5&amp;pages=479-482&amp;publication_year=2014&amp;author=El-Hajj%2CA&amp;author=Safatly%2CL&amp;author=Bkassiny%2CM&amp;author=Husseini%2CM">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="M. Bkassiny, SK. Jayaweera, Y. Li, " /><meta itemprop="datePublished" content="2013" /><meta itemprop="headline" content="M Bkassiny, SK Jayaweera, Y Li, Multidimensional dirichlet process-based non-parametric signal classification " /><span class="c-article-references__counter">125.</span><p class="c-article-references__text" id="ref-CR125">M Bkassiny, SK Jayaweera, Y Li, Multidimensional dirichlet process-based non-parametric signal classification for autonomous self-learning cognitive radios. IEEE Trans Wirel Commun <b>12</b>(11), 5413–5423 (2013)</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="https://doi.org/10.1109%2FTWC.2013.092013.120688" aria-label="View reference 125">Article</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 125 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Multidimensional%20dirichlet%20process-based%20non-parametric%20signal%20classification%20for%20autonomous%20self-learning%20cognitive%20radios&amp;journal=IEEE%20Trans%20Wirel%20Commun&amp;volume=12&amp;issue=11&amp;pages=5413-5423&amp;publication_year=2013&amp;author=Bkassiny%2CM&amp;author=Jayaweera%2CSK&amp;author=Li%2CY">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="A. Galindo-Serrano, L. Giupponi, " /><meta itemprop="datePublished" content="2010" /><meta itemprop="headline" content="A Galindo-Serrano, L Giupponi, Distributed Q-learning for aggregated interference control in cognitive radio n" /><span class="c-article-references__counter">126.</span><p class="c-article-references__text" id="ref-CR126">A Galindo-Serrano, L Giupponi, Distributed Q-learning for aggregated interference control in cognitive radio networks. IEEE Trans Veh Technol <b>59</b>(4), 1823–1834 (2010)</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="https://doi.org/10.1109%2FTVT.2010.2043124" aria-label="View reference 126">Article</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 126 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Distributed%20Q-learning%20for%20aggregated%20interference%20control%20in%20cognitive%20radio%20networks&amp;journal=IEEE%20Trans%20Veh%20Technol&amp;volume=59&amp;issue=4&amp;pages=1823-1834&amp;publication_year=2010&amp;author=Galindo-Serrano%2CA&amp;author=Giupponi%2CL">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="TK. Das, A. Gosavi, S. Mahadevan, N. Marchalleck, " /><meta itemprop="datePublished" content="1999" /><meta itemprop="headline" content="TK Das, A Gosavi, S Mahadevan, N Marchalleck, Solving semi-markov decision problems using average reward reinf" /><span class="c-article-references__counter">127.</span><p class="c-article-references__text" id="ref-CR127">TK Das, A Gosavi, S Mahadevan, N Marchalleck, Solving semi-markov decision problems using average reward reinforcement learning. Manage Sci <b>45</b>(4), 560–574 (1999)</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="https://doi.org/10.1287%2Fmnsc.45.4.560" aria-label="View reference 127">Article</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="http://www.emis.de/MATH-item?1231.90225" aria-label="View reference 127 on MATH">MATH</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 127 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Solving%20semi-markov%20decision%20problems%20using%20average%20reward%20reinforcement%20learning&amp;journal=Manage%20Sci&amp;volume=45&amp;issue=4&amp;pages=560-574&amp;publication_year=1999&amp;author=Das%2CTK&amp;author=Gosavi%2CA&amp;author=Mahadevan%2CS&amp;author=Marchalleck%2CN">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="RS. Sutton, " /><meta itemprop="datePublished" content="1988" /><meta itemprop="headline" content="RS Sutton, Learning to predict by the methods of temporal differences. Mach Learn 3(1), 9–44 (1988)" /><span class="c-article-references__counter">128.</span><p class="c-article-references__text" id="ref-CR128">RS Sutton, Learning to predict by the methods of temporal differences. Mach Learn <b>3</b>(1), 9–44 (1988)</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 128 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Learning%20to%20predict%20by%20the%20methods%20of%20temporal%20differences&amp;journal=Mach%20Learn&amp;volume=3&amp;issue=1&amp;pages=9-44&amp;publication_year=1988&amp;author=Sutton%2CRS">
                        Google Scholar</a></li></ul></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="S. Singh, T. Jaakkola, ML. Littman, C. Szepesvári, " /><meta itemprop="datePublished" content="2000" /><meta itemprop="headline" content="S Singh, T Jaakkola, ML Littman, C Szepesvári, Convergence results for single-step on-policy reinforcement-lea" /><span class="c-article-references__counter">129.</span><p class="c-article-references__text" id="ref-CR129">S Singh, T Jaakkola, ML Littman, C Szepesvári, Convergence results for single-step on-policy reinforcement-learning algorithms. Mach Learn <b>38</b>, 287–308 (2000)</p><ul class="c-article-references__links u-hide-print"><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="https://doi.org/10.1023%2FA%3A1007678930559" aria-label="View reference 129">Article</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" href="http://www.emis.de/MATH-item?0954.68127" aria-label="View reference 129 on MATH">MATH</a></li><li><a data-track="click" data-track-action="outbound reference" data-track-category="article body" data-track-label="link" aria-label="Search for reference 129 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Convergence%20results%20for%20single-step%20on-policy%20reinforcement-learning%20algorithms&amp;journal=Mach%20Learn&amp;volume=38&amp;pages=287-308&amp;publication_year=2000&amp;author=Singh%2CS&amp;author=Jaakkola%2CT&amp;author=Littman%2CML&amp;author=Szepesv%C3%A1ri%2CC">
                        Google Scholar</a></li></ul></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-category="article body" data-track-label="link" href="/article/10.1186/s13634-016-0355-x-references.ris?shared-article-renderer">Download references<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p></div></div></div></section><section aria-labelledby="Ack1"><div class="c-article-section" id="Ack1-section"><h2 class="c-article-section__title u-h2 js-section-title js-c-reading-companion-sections-item" id="Ack1">Acknowledgements</h2><div class="c-article-section__content" id="Ack1-content"><p>We gratefully acknowledge the financial support from the National Natural Science Foundation of China (Grant No. 61301160 and No. 61172062).</p></div></div></section><section aria-labelledby="author-information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title u-h2 js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><h3 class="c-article-author-information__subtitle u-h3" id="affiliations">Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><span class="c-article-author-affiliation__address u-h3">College of Communications Engineering, PLA University of Science and Technology, Nanjing, 210007, China</span><ul class="c-article-author-affiliation__authors-list"><li class="c-article-author-affiliation__authors-item">Junfei Qiu</li><li class="c-article-author-affiliation__authors-item">, Qihui Wu</li><li class="c-article-author-affiliation__authors-item">, Guoru Ding</li><li class="c-article-author-affiliation__authors-item">, Yuhua Xu</li><li class="c-article-author-affiliation__authors-item"> &amp; Shuo Feng</li></ul></li></ol><div class="js-hide u-hide-print" data-test="author-info"><span class="c-article-author-information__subtitle u-h3">Authors</span><ol class="c-article-author-authors-search"><li id="auth-1"><span class="c-article-author-authors-search__title u-h3 js-search-name">Search for Junfei Qiu in:</span><ul class="c-article-author-authors-search__list"><li class="c-article-author-authors-search__item"><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Junfei+Qiu">PubMed</a><span class="bullet"> • </span></li><li class="c-article-author-authors-search__item"><a href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Junfei+Qiu%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en">
                        Google Scholar
                    </a></li></ul></li><li id="auth-2"><span class="c-article-author-authors-search__title u-h3 js-search-name">Search for Qihui Wu in:</span><ul class="c-article-author-authors-search__list"><li class="c-article-author-authors-search__item"><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Qihui+Wu">PubMed</a><span class="bullet"> • </span></li><li class="c-article-author-authors-search__item"><a href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Qihui+Wu%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en">
                        Google Scholar
                    </a></li></ul></li><li id="auth-3"><span class="c-article-author-authors-search__title u-h3 js-search-name">Search for Guoru Ding in:</span><ul class="c-article-author-authors-search__list"><li class="c-article-author-authors-search__item"><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Guoru+Ding">PubMed</a><span class="bullet"> • </span></li><li class="c-article-author-authors-search__item"><a href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Guoru+Ding%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en">
                        Google Scholar
                    </a></li></ul></li><li id="auth-4"><span class="c-article-author-authors-search__title u-h3 js-search-name">Search for Yuhua Xu in:</span><ul class="c-article-author-authors-search__list"><li class="c-article-author-authors-search__item"><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Yuhua+Xu">PubMed</a><span class="bullet"> • </span></li><li class="c-article-author-authors-search__item"><a href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Yuhua+Xu%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en">
                        Google Scholar
                    </a></li></ul></li><li id="auth-5"><span class="c-article-author-authors-search__title u-h3 js-search-name">Search for Shuo Feng in:</span><ul class="c-article-author-authors-search__list"><li class="c-article-author-authors-search__item"><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Shuo+Feng">PubMed</a><span class="bullet"> • </span></li><li class="c-article-author-authors-search__item"><a href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Shuo+Feng%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en">
                        Google Scholar
                    </a></li></ul></li></ol></div><h3 class="c-article-author-information__subtitle u-h3" id="corresponding-author">Corresponding author</h3><p>Correspondence to
                <a id="corresp-c1" rel="nofollow" href="/article/10.1186/s13634-016-0355-x/email/correspondent/c1/new?shared-article-renderer">Guoru Ding</a>.</p></div></div></section><section aria-labelledby="additional-information"><div class="c-article-section" id="additional-information-section"><h2 class="c-article-section__title u-h2 js-section-title js-c-reading-companion-sections-item" id="additional-information">Additional information</h2><div class="c-article-section__content" id="additional-information-content"><h3 class="c-article__sub-heading u-h3">Competing interests</h3><p>The authors declare that they have no competing interests.</p><p>An erratum to this article can be found at <a href="http://dx.doi.org/10.1186/s13634-016-0382-7">http://dx.doi.org/10.1186/s13634-016-0382-7</a>.</p></div></div></section><section aria-labelledby="rightslink"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title u-h2 js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content"><div class="c-article-license">
                <p>
                           <b>Open Access</b> This article is distributed under the terms of the Creative Commons Attribution 4.0 International License (<a href="http://creativecommons.org/licenses/by/4.0/" rel="license" itemprop="license">http://creativecommons.org/licenses/by/4.0/</a>), which permits unrestricted use, distribution, and reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made.</p>
              </div><p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-category="article body" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?imprint=Nature&amp;oa=CC%20BY&amp;title=A%20survey%20of%20machine%20learning%20for%20big%20data%20processing&amp;author=Junfei%20Qiu%20et%20al&amp;contentID=10.1186%2Fs13634-016-0355-x&amp;publication=EURASIP%20Journal%20on%20Advances%20in%20Signal%20Processing&amp;publicationDate=2016-05-28&amp;publisherName=SpringerNature&amp;orderBeanReset=true">Reprints and Permissions</a></p></div></div></section><section aria-labelledby="article-info"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title u-h2 js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="u-hide-print c-bibliographic-information__column c-bibliographic-information__column--border"><a data-crossmark="10.1186/s13634-016-0355-x" target="_blank" rel="noopener" href="https://crossmark.crossref.org/dialog/?doi=10.1186/s13634-016-0355-x" data-track="click" data-track-action="Click Crossmark" data-track-category="article body" data-track-label="link" data-test="crossmark"><img width="57" height="81" alt="Verify currency and authenticity via CrossMark" src="data:image/svg+xml;base64,<svg height="81" width="57" xmlns="http://www.w3.org/2000/svg"><g fill="none" fill-rule="evenodd"><path d="m17.35 35.45 21.3-14.2v-17.03h-21.3" fill="#989898"/><path d="m38.65 35.45-21.3-14.2v-17.03h21.3" fill="#747474"/><path d="m28 .5c-12.98 0-23.5 10.52-23.5 23.5s10.52 23.5 23.5 23.5 23.5-10.52 23.5-23.5c0-6.23-2.48-12.21-6.88-16.62-4.41-4.4-10.39-6.88-16.62-6.88zm0 41.25c-9.8 0-17.75-7.95-17.75-17.75s7.95-17.75 17.75-17.75 17.75 7.95 17.75 17.75c0 4.71-1.87 9.22-5.2 12.55s-7.84 5.2-12.55 5.2z" fill="#535353"/><path d="m41 36c-5.81 6.23-15.23 7.45-22.43 2.9-7.21-4.55-10.16-13.57-7.03-21.5l-4.92-3.11c-4.95 10.7-1.19 23.42 8.78 29.71 9.97 6.3 23.07 4.22 30.6-4.86z" fill="#9c9c9c"/><path d="m.2 58.45c0-.75.11-1.42.33-2.01s.52-1.09.91-1.5c.38-.41.83-.73 1.34-.94.51-.22 1.06-.32 1.65-.32.56 0 1.06.11 1.51.35.44.23.81.5 1.1.81l-.91 1.01c-.24-.24-.49-.42-.75-.56-.27-.13-.58-.2-.93-.2-.39 0-.73.08-1.05.23-.31.16-.58.37-.81.66-.23.28-.41.63-.53 1.04-.13.41-.19.88-.19 1.39 0 1.04.23 1.86.68 2.46.45.59 1.06.88 1.84.88.41 0 .77-.07 1.07-.23s.59-.39.85-.68l.91 1c-.38.43-.8.76-1.28.99-.47.22-1 .34-1.58.34-.59 0-1.13-.1-1.64-.31-.5-.2-.94-.51-1.31-.91-.38-.4-.67-.9-.88-1.48-.22-.59-.33-1.26-.33-2.02zm8.4-5.33h1.61v2.54l-.05 1.33c.29-.27.61-.51.96-.72s.76-.31 1.24-.31c.73 0 1.27.23 1.61.71.33.47.5 1.14.5 2.02v4.31h-1.61v-4.1c0-.57-.08-.97-.25-1.21-.17-.23-.45-.35-.83-.35-.3 0-.56.08-.79.22-.23.15-.49.36-.78.64v4.8h-1.61zm7.37 6.45c0-.56.09-1.06.26-1.51.18-.45.42-.83.71-1.14.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.36c.07.62.29 1.1.65 1.44.36.33.82.5 1.38.5.29 0 .57-.04.83-.13s.51-.21.76-.37l.55 1.01c-.33.21-.69.39-1.09.53-.41.14-.83.21-1.26.21-.48 0-.92-.08-1.34-.25-.41-.16-.76-.4-1.07-.7-.31-.31-.55-.69-.72-1.13-.18-.44-.26-.95-.26-1.52zm4.6-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.07.45-.31.29-.5.73-.58 1.3zm2.5.62c0-.57.09-1.08.28-1.53.18-.44.43-.82.75-1.13s.69-.54 1.1-.71c.42-.16.85-.24 1.31-.24.45 0 .84.08 1.17.23s.61.34.85.57l-.77 1.02c-.19-.16-.38-.28-.56-.37-.19-.09-.39-.14-.61-.14-.56 0-1.01.21-1.35.63-.35.41-.52.97-.52 1.67 0 .69.17 1.24.51 1.66.34.41.78.62 1.32.62.28 0 .54-.06.78-.17.24-.12.45-.26.64-.42l.67 1.03c-.33.29-.69.51-1.08.65-.39.15-.78.23-1.18.23-.46 0-.9-.08-1.31-.24-.4-.16-.75-.39-1.05-.7s-.53-.69-.7-1.13c-.17-.45-.25-.96-.25-1.53zm6.91-6.45h1.58v6.17h.05l2.54-3.16h1.77l-2.35 2.8 2.59 4.07h-1.75l-1.77-2.98-1.08 1.23v1.75h-1.58zm13.69 1.27c-.25-.11-.5-.17-.75-.17-.58 0-.87.39-.87 1.16v.75h1.34v1.27h-1.34v5.6h-1.61v-5.6h-.92v-1.2l.92-.07v-.72c0-.35.04-.68.13-.98.08-.31.21-.57.4-.79s.42-.39.71-.51c.28-.12.63-.18 1.04-.18.24 0 .48.02.69.07.22.05.41.1.57.17zm.48 5.18c0-.57.09-1.08.27-1.53.17-.44.41-.82.72-1.13.3-.31.65-.54 1.04-.71.39-.16.8-.24 1.23-.24s.84.08 1.24.24c.4.17.74.4 1.04.71s.54.69.72 1.13c.19.45.28.96.28 1.53s-.09 1.08-.28 1.53c-.18.44-.42.82-.72 1.13s-.64.54-1.04.7-.81.24-1.24.24-.84-.08-1.23-.24-.74-.39-1.04-.7c-.31-.31-.55-.69-.72-1.13-.18-.45-.27-.96-.27-1.53zm1.65 0c0 .69.14 1.24.43 1.66.28.41.68.62 1.18.62.51 0 .9-.21 1.19-.62.29-.42.44-.97.44-1.66 0-.7-.15-1.26-.44-1.67-.29-.42-.68-.63-1.19-.63-.5 0-.9.21-1.18.63-.29.41-.43.97-.43 1.67zm6.48-3.44h1.33l.12 1.21h.05c.24-.44.54-.79.88-1.02.35-.24.7-.36 1.07-.36.32 0 .59.05.78.14l-.28 1.4-.33-.09c-.11-.01-.23-.02-.38-.02-.27 0-.56.1-.86.31s-.55.58-.77 1.1v4.2h-1.61zm-47.87 15h1.61v4.1c0 .57.08.97.25 1.2.17.24.44.35.81.35.3 0 .57-.07.8-.22.22-.15.47-.39.73-.73v-4.7h1.61v6.87h-1.32l-.12-1.01h-.04c-.3.36-.63.64-.98.86-.35.21-.76.32-1.24.32-.73 0-1.27-.24-1.61-.71-.33-.47-.5-1.14-.5-2.02zm9.46 7.43v2.16h-1.61v-9.59h1.33l.12.72h.05c.29-.24.61-.45.97-.63.35-.17.72-.26 1.1-.26.43 0 .81.08 1.15.24.33.17.61.4.84.71.24.31.41.68.53 1.11.13.42.19.91.19 1.44 0 .59-.09 1.11-.25 1.57-.16.47-.38.85-.65 1.16-.27.32-.58.56-.94.73-.35.16-.72.25-1.1.25-.3 0-.6-.07-.9-.2s-.59-.31-.87-.56zm0-2.3c.26.22.5.37.73.45.24.09.46.13.66.13.46 0 .84-.2 1.15-.6.31-.39.46-.98.46-1.77 0-.69-.12-1.22-.35-1.61-.23-.38-.61-.57-1.13-.57-.49 0-.99.26-1.52.77zm5.87-1.69c0-.56.08-1.06.25-1.51.16-.45.37-.83.65-1.14.27-.3.58-.54.93-.71s.71-.25 1.08-.25c.39 0 .73.07 1 .2.27.14.54.32.81.55l-.06-1.1v-2.49h1.61v9.88h-1.33l-.11-.74h-.06c-.25.25-.54.46-.88.64-.33.18-.69.27-1.06.27-.87 0-1.56-.32-2.07-.95s-.76-1.51-.76-2.65zm1.67-.01c0 .74.13 1.31.4 1.7.26.38.65.58 1.15.58.51 0 .99-.26 1.44-.77v-3.21c-.24-.21-.48-.36-.7-.45-.23-.08-.46-.12-.7-.12-.45 0-.82.19-1.13.59-.31.39-.46.95-.46 1.68zm6.35 1.59c0-.73.32-1.3.97-1.71.64-.4 1.67-.68 3.08-.84 0-.17-.02-.34-.07-.51-.05-.16-.12-.3-.22-.43s-.22-.22-.38-.3c-.15-.06-.34-.1-.58-.1-.34 0-.68.07-1 .2s-.63.29-.93.47l-.59-1.08c.39-.24.81-.45 1.28-.63.47-.17.99-.26 1.54-.26.86 0 1.51.25 1.93.76s.63 1.25.63 2.21v4.07h-1.32l-.12-.76h-.05c-.3.27-.63.48-.98.66s-.73.27-1.14.27c-.61 0-1.1-.19-1.48-.56-.38-.36-.57-.85-.57-1.46zm1.57-.12c0 .3.09.53.27.67.19.14.42.21.71.21.28 0 .54-.07.77-.2s.48-.31.73-.56v-1.54c-.47.06-.86.13-1.18.23-.31.09-.57.19-.76.31s-.33.25-.41.4c-.09.15-.13.31-.13.48zm6.29-3.63h-.98v-1.2l1.06-.07.2-1.88h1.34v1.88h1.75v1.27h-1.75v3.28c0 .8.32 1.2.97 1.2.12 0 .24-.01.37-.04.12-.03.24-.07.34-.11l.28 1.19c-.19.06-.4.12-.64.17-.23.05-.49.08-.76.08-.4 0-.74-.06-1.02-.18-.27-.13-.49-.3-.67-.52-.17-.21-.3-.48-.37-.78-.08-.3-.12-.64-.12-1.01zm4.36 2.17c0-.56.09-1.06.27-1.51s.41-.83.71-1.14c.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.37c.08.62.29 1.1.65 1.44.36.33.82.5 1.38.5.3 0 .58-.04.84-.13.25-.09.51-.21.76-.37l.54 1.01c-.32.21-.69.39-1.09.53s-.82.21-1.26.21c-.47 0-.92-.08-1.33-.25-.41-.16-.77-.4-1.08-.7-.3-.31-.54-.69-.72-1.13-.17-.44-.26-.95-.26-1.52zm4.61-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.08.45-.31.29-.5.73-.57 1.3zm3.01 2.23c.31.24.61.43.92.57.3.13.63.2.98.2.38 0 .65-.08.83-.23s.27-.35.27-.6c0-.14-.05-.26-.13-.37-.08-.1-.2-.2-.34-.28-.14-.09-.29-.16-.47-.23l-.53-.22c-.23-.09-.46-.18-.69-.3-.23-.11-.44-.24-.62-.4s-.33-.35-.45-.55c-.12-.21-.18-.46-.18-.75 0-.61.23-1.1.68-1.49.44-.38 1.06-.57 1.83-.57.48 0 .91.08 1.29.25s.71.36.99.57l-.74.98c-.24-.17-.49-.32-.73-.42-.25-.11-.51-.16-.78-.16-.35 0-.6.07-.76.21-.17.15-.25.33-.25.54 0 .14.04.26.12.36s.18.18.31.26c.14.07.29.14.46.21l.54.19c.23.09.47.18.7.29s.44.24.64.4c.19.16.34.35.46.58.11.23.17.5.17.82 0 .3-.06.58-.17.83-.12.26-.29.48-.51.68-.23.19-.51.34-.84.45-.34.11-.72.17-1.15.17-.48 0-.95-.09-1.41-.27-.46-.19-.86-.41-1.2-.68z" fill="#535353"/></g></svg>" /></a></div><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Qiu, J., Wu, Q., Ding, G. <i>et al.</i> A survey of machine learning for big data processing.
                    <i>EURASIP J. Adv. Signal Process. </i> <b>2016, </b>67 (2016)  doi:10.1186/s13634-016-0355-x</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-category="article body" data-track-label="link" href="/article/10.1186/s13634-016-0355-x.ris?shared-article-renderer">Download citation<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><span class="u-h4">Received</span><p class="c-bibliographic-information__value"><time datetime="2015-08-31">31 August 2015</time></p></li><li class="c-bibliographic-information__list-item"><span class="u-h4">Accepted</span><p class="c-bibliographic-information__value"><time datetime="2016-04-22">22 April 2016</time></p></li><li class="c-bibliographic-information__list-item"><span class="u-h4">Published</span><p class="c-bibliographic-information__value"><time datetime="2016-05-28">28 May 2016</time></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--doi"><span class="u-h4"><abbr title="Digital Object Identifier">DOI</abbr></span><p class="c-bibliographic-information__value"><a href="https://doi.org/10.1186/s13634-016-0355-x" data-track="click" data-track-action="view doi" data-track-category="article body" data-track-label="link" itemprop="sameAs">https://doi.org/10.1186/s13634-016-0355-x</a></p></li></ul><div data-component="share-box"></div><h3 class="c-article__sub-heading u-h3">Keywords</h3><ul class="c-article-subject-list"><li class="c-article-subject-list__subject"><span itemprop="about">Machine learning</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Big data</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Data mining</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Signal processing techniques</span></li></ul><div data-component="article-info-list"></div></div></div></div></div></section>
                            <placeholder type='Article History'/>
                        </div>
                    </article>
                </main>

                <div class="c-page-layout__side u-text-sm u-hide-print" id="sidebar" data-container-type="reading-companion" data-track-component="reading companion">
                    <aside>
                        <div data-test="download-article-link-wrapper">
                            <div id="pdflink-container" class="download-article test-pdf-link">
    
    <div class="c-pdf-download u-clear-both">
        <a href="//link.springer.com/content/pdf/10.1186/s13634-016-0355-x.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-category="article body" data-track-label="link">
            <span>Download PDF</span>
            <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
        </a>
    </div>
    
</div>

                        </div>

                        <div class="u-hide" id="js-buy-button">
                            <a href="#access-options" class="c-article__button" data-test="buy-button" data-track="click" data-track-action="buy or subscribe" data-track-label="button">
                                <span>Buy or subscribe</span>
                            </a>
                        </div>

                        <div data-test="collections">
                            <aside><div class="c-article-associated-content__container"><h2 class="c-article-associated-content__title u-h3">Associated Content</h2><div class="c-article-associated-content__collection collection"><section><p class="c-article-associated-content__collection-label">Collection</p><h3 class="c-article-associated-content__collection-title u-h3" itemprop="name headline"><a href="/journal/13634/topicalCollection/AC_8011df92d430b18eb3fe9814719e61a3" data-track="click" data-track-action="view collection" data-track-label="link">Signal Processing for Big Data</a></h3></section></div></div></aside>
                        </div>

                        <div data-test="editorial-summary">
                            
                        </div>

                        <div class="c-reading-companion">
                            <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky">
                                <div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections">
                                    <div class="js-ad">
                                        <div class="c-ad c-ad--MPU1">
                                            <div class="c-ad c-ad__inner">
                                                <p class="c-ad__label">Advertisement</p>
                                                <div id="div-gpt-ad-MPU1" data-gpt-unitpath="/270604982/springerlink/13634/article" data-gpt-sizes="300x250" data-gpt-targeting="pos=MPU1;"></div>
                                            </div>
                                        </div>
                                    </div>
                                </div>
                                <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures"></div>
                                <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references"></div>
                            </div>
                        </div>
                    </aside>
                </div>
            </div>
        </div>

        <div class="c-page-layout__footer">
            <footer class="c-footer" role="contentinfo">
    <div class="c-footer__aside-wrapper u-hide-print">
        <div class="c-footer__container">
            <p class="c-footer__strapline">Over 10 million scientific documents at your fingertips</p>
            
                <div class="c-footer__edition" data-component="SV.EditionSwitcher">
                    <span class="u-visually-hidden" data-role="button-dropdown__title" data-btn-text="Switch between Academic & Corporate Edition">Switch Edition</span>
                    <ul class="c-footer-edition-list u-list-inline" data-role="button-dropdown__content" data-test="footer-edition-switcher-list">
                        <li class="selected">
                            <a data-test="footer-academic-link"
                               href="/siteEdition/link?id=siteedition-academic-link"
                               id="siteedition-academic-link">Academic Edition</a>
                        </li>
                        <li>
                            <a data-test="footer-corporate-link"
                               href="/siteEdition/rd?id=siteedition-corporate-link"
                               id="siteedition-corporate-link">Corporate Edition</a>
                        </li>
                    </ul>
                </div>
            
        </div>
    </div>
    <div class="c-footer__container">
        <ul class="c-footer__nav u-list-inline u-hide-print">
            <li><a href="/">Home</a></li>
            <li><a href="/impressum">Impressum</a></li>
            <li><a href="/termsandconditions">Legal information</a></li>
            <li><a href="/privacystatement">Privacy statement</a></li>
            <li><a href="/cookiepolicy">How we use cookies</a></li>
            <li><a href="/accessibility">Accessibility</a></li>
            <li><a id="contactus-footer-link" href="/contactus">Contact us</a></li>
        </ul>
        <div class="c-user-metadata">
    
        <p class="c-user-metadata__item">
            <span data-test="footer-user-login-status">Not logged in</span>
            <span data-test="footer-user-ip"> - 185.128.27.154</span>
        </p>
        <p class="c-user-metadata__item" data-test="footer-business-partners">
            Not affiliated
        </p>

        
    
</div>

        <a class="c-footer__parent-logo" target="_blank" rel="noopener" href="//www.springernature.com"  title="Go to Springer Nature">
            <span class="u-visually-hidden">Springer Nature</span>
            <svg width="125" height="12" focusable="false" aria-hidden="true">
                <image width="125" height="12" alt="Springer Nature logo"
                       src=/oscar-static/images/springerlink/png/springernature-60a72a849b.png
                       xmlns:xlink="http://www.w3.org/1999/xlink"
                       xlink:href=/oscar-static/images/springerlink/svg/springernature-ecf01c77dd.svg>
                </image>
            </svg>
        </a>

        <p class="c-footer__copyright">&copy; 2019 Springer Nature Switzerland AG. Part of <a target="_blank" rel="noopener" href="//www.springernature.com">Springer Nature</a>.</p>
        
    </div>

    
    <svg class="u-hide hide">
        <symbol id="global-icon-chevron-right" viewBox="0 0 16 16">
            <path d="M7.782 7L5.3 4.518c-.393-.392-.4-1.022-.02-1.403a1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L7.782 9l1.013-.998z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-download" viewBox="0 0 16 16">
            <path d="M2 14c0-.556.449-1 1.002-1h9.996a.999.999 0 110 2H3.002A1.006 1.006 0 012 14zM9 2v6.8l2.482-2.482c.392-.392 1.022-.4 1.403-.02a1.001 1.001 0 010 1.417l-4.177 4.177a1.001 1.001 0 01-1.416 0L3.115 7.715a.991.991 0 01-.016-1.4 1 1 0 011.42.003L7 8.8V2c0-.55.444-.996 1-.996.552 0 1 .445 1 .996z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-email" viewBox="0 0 18 18">
            <path d="M1.995 2h14.01A2 2 0 0118 4.006v9.988A2 2 0 0116.005 16H1.995A2 2 0 010 13.994V4.006A2 2 0 011.995 2zM1 13.994A1 1 0 001.995 15h14.01A1 1 0 0017 13.994V4.006A1 1 0 0016.005 3H1.995A1 1 0 001 4.006zM9 11L2 7V5.557l7 4 7-4V7z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-institution" viewBox="0 0 18 18">
            <path d="M14 8a1 1 0 011 1v6h1.5a.5.5 0 01.5.5v.5h.5a.5.5 0 01.5.5V18H0v-1.5a.5.5 0 01.5-.5H1v-.5a.5.5 0 01.5-.5H3V9a1 1 0 112 0v6h8V9a1 1 0 011-1zM6 8l2 1v4l-2 1zm6 0v6l-2-1V9zM9.573.401l7.036 4.925A.92.92 0 0116.081 7H1.92a.92.92 0 01-.528-1.674L8.427.401a1 1 0 011.146 0zM9 2.441L5.345 5h7.31z" fill-rule="evenodd"/>
        </symbol>
    </svg>

</footer>

        </div>
    </div>

    <script>
    window.config = {
        mustardcut: false
    };

    
    (function(H){H.className=H.className.replace(/\bno-js\b/,'js')})(document.documentElement)

        
    var mustardcutlink = document.getElementById('js-mustard');
    if (mustardcutlink && window.matchMedia && window.matchMedia(mustardcutlink.media).matches) {
        window.config.mustardcut = true;
    }
</script>

<script src=/oscar-static/js/app-springerlink-bundle-30d63d4d0b.js></script>

<script>
    if (window.config && window.config.mustardcut) {
        var globalArticle = document.createElement('script');

        globalArticle.src = '/oscar-static/js/global-article-bundle-3cdf92e2e9.js';
        
        window.Component = {};
        document.body.appendChild(globalArticle);
        
    }
</script>

    <div id="googleanalytics-container">
    
        
    
</div>
<div id="google-tag-manager-head-container">
    
        
            <!-- Google Tag Manager -->
            <script>
                (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-WCF9Z9');
            </script>
            <!-- End Google Tag Manager -->
        
    
</div>

<div id="google-tag-manager-body-container">
    
        
            <!-- Google Tag Manager (noscript) -->
            <noscript>
                <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WCF9Z9"
                        height="0" width="0" style="display:none;visibility:hidden"></iframe>
            </noscript>
            <!-- End Google Tag Manager (noscript) -->
        
    
</div>

</body>
</html>
