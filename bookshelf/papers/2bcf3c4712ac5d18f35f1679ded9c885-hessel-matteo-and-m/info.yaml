abstract: The deep reinforcement learning community has made several independent improvements
  to the DQN algorithm. However, it is unclear which of these extensions are complementary
  and can be fruitfully combined. This paper examines six extensions to the DQN algorithm
  and empirically studies their combination. Our experiments show that the combination
  provides state-of-the-art performance on the Atari 2600 benchmark, both in terms
  of data efficiency and final performance. We also provide results from a detailed
  ablation study that shows the contribution of each component to overall performance.
archiveprefix: arXiv
author: Hessel, Matteo and Modayil, Joseph and van Hasselt, Hado and Schaul, Tom and
  Ostrovski, Georg and Dabney, Will and Horgan, Dan and Piot, Bilal and Azar, Mohammad
  and Silver, David
author_list:
- family: Hessel
  given: Matteo
- family: Modayil
  given: Joseph
- family: van Hasselt
  given: Hado
- family: Schaul
  given: Tom
- family: Ostrovski
  given: Georg
- family: Dabney
  given: Will
- family: Horgan
  given: Dan
- family: Piot
  given: Bilal
- family: Azar
  given: Mohammad
- family: Silver
  given: David
eprint: 1710.02298v1
file: 1710.02298v1.pdf
files:
- hessel-matteo-and-modayil-joseph-and-van-hasselt-hado-and-schaul-tom-and-ostrovski-georg-and-dabney-will-and-horgan-dan-and-piot-bilal-and-aza.pdf
month: Oct
primaryclass: cs.AI
ref: 1710.02298v1
title: 'Rainbow: Combining Improvements in Deep Reinforcement Learning'
type: article
url: http://arxiv.org/abs/1710.02298v1
year: '2017'
