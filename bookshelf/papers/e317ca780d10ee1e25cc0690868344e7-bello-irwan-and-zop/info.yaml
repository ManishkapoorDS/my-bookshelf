abstract: We present an approach to automate the process of discovering optimization
  methods, with a focus on deep learning architectures. We train a Recurrent Neural
  Network controller to generate a string in a domain specific language that describes
  a mathematical update equation based on a list of primitive functions, such as the
  gradient, running average of the gradient, etc. The controller is trained with Reinforcement
  Learning to maximize the performance of a model after a few epochs. On CIFAR-10,
  our method discovers several update rules that are better than many commonly used
  optimizers, such as Adam, RMSProp, or SGD with and without Momentum on a ConvNet
  model. We introduce two new optimizers, named PowerSign and AddSign, which we show
  transfer well and improve training on a variety of different tasks and architectures,
  including ImageNet classification and Google's neural machine translation system.
archiveprefix: arXiv
author: Bello, Irwan and Zoph, Barret and Vasudevan, Vijay and Le, Quoc V.
author_list:
- family: Bello
  given: Irwan
- family: Zoph
  given: Barret
- family: Vasudevan
  given: Vijay
- family: Le
  given: Quoc V.
eprint: 1709.07417v2
file: 1709.07417v2.pdf
files:
- bello-irwan-and-zoph-barret-and-vasudevan-vijay-and-le-quoc-v.neural-optimizer-search-with-reinforcement-learning2017.pdf
month: Sep
primaryclass: cs.AI
ref: 1709.07417v2
time-added: 2020-06-05-18:16:08
title: Neural Optimizer Search with Reinforcement Learning
type: article
url: http://arxiv.org/abs/1709.07417v2
year: '2017'
