abstract: 'In this work, we introduce a novel probabilistic representation of deep
  learning, which provides an explicit explanation for the Deep Neural Networks (DNNs)
  in three aspects: (i) neurons define the energy of a Gibbs distribution; (ii) the
  hidden layers of DNNs formulate Gibbs distributions; and (iii) the whole architecture
  of DNNs can be interpreted as a Bayesian neural network. Based on the proposed probabilistic
  representation, we investigate two fundamental properties of deep learning: hierarchy
  and generalization. First, we explicitly formulate the hierarchy property from the
  Bayesian perspective, namely that some hidden layers formulate a prior distribution
  and the remaining layers formulate a likelihood distribution. Second, we demonstrate
  that DNNs have an explicit regularization by learning a prior distribution and the
  learning algorithm is one reason for decreasing the generalization ability of DNNs.
  Moreover, we clarify two empirical phenomena of DNNs that cannot be explained by
  traditional theories of generalization. Simulation results validate the proposed
  probabilistic representation and the insights into these properties of deep learning
  based on a synthetic dataset.'
archiveprefix: arXiv
author: Lan, Xinjie and Barner, Kenneth E.
author_list:
- family: Lan
  given: Xinjie
- family: Barner
  given: Kenneth E.
eprint: 1908.09772v1
file: 1908.09772v1.pdf
files:
- lan-xinjie-and-barner-kenneth-e.a-probabilistic-representation-of-deep-learning2019.pdf
month: Aug
primaryclass: cs.LG
ref: 1908.09772v1
time-added: 2020-06-24-12:02:54
title: A Probabilistic Representation of Deep Learning
type: article
url: http://arxiv.org/abs/1908.09772v1
year: '2019'
