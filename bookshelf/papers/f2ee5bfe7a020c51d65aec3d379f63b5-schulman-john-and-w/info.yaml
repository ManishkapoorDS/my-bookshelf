abstract: We propose a new family of policy gradient methods for reinforcement learning,
  which alternate between sampling data through interaction with the environment,
  and optimizing a "surrogate" objective function using stochastic gradient ascent.
  Whereas standard policy gradient methods perform one gradient update per data sample,
  we propose a novel objective function that enables multiple epochs of minibatch
  updates. The new methods, which we call proximal policy optimization (PPO), have
  some of the benefits of trust region policy optimization (TRPO), but they are much
  simpler to implement, more general, and have better sample complexity (empirically).
  Our experiments test PPO on a collection of benchmark tasks, including simulated
  robotic locomotion and Atari game playing, and we show that PPO outperforms other
  online policy gradient methods, and overall strikes a favorable balance between
  sample complexity, simplicity, and wall-time.
archiveprefix: arXiv
author: Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec
  and Klimov, Oleg
author_list:
- family: Schulman
  given: John
- family: Wolski
  given: Filip
- family: Dhariwal
  given: Prafulla
- family: Radford
  given: Alec
- family: Klimov
  given: Oleg
eprint: 1707.06347v2
file: 1707.06347v2.pdf
files:
- schulman-john-and-wolski-filip-and-dhariwal-prafulla-and-radford-alec-and-klimov-olegproximal-policy-optimization-algorithms2017.pdf
month: Jul
primaryclass: cs.LG
ref: 1707.06347v2
title: Proximal Policy Optimization Algorithms
type: article
url: http://arxiv.org/abs/1707.06347v2
year: '2017'
