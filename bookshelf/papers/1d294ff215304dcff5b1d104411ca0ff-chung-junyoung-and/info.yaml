abstract: In this paper we compare different types of recurrent units in recurrent
  neural networks (RNNs). Especially, we focus on more sophisticated units that implement
  a gating mechanism, such as a long short-term memory (LSTM) unit and a recently
  proposed gated recurrent unit (GRU). We evaluate these recurrent units on the tasks
  of polyphonic music modeling and speech signal modeling. Our experiments revealed
  that these advanced recurrent units are indeed better than more traditional recurrent
  units such as tanh units. Also, we found GRU to be comparable to LSTM.
archiveprefix: arXiv
author: Chung, Junyoung and Gulcehre, Caglar and Cho, KyungHyun and Bengio, Yoshua
author_list:
- family: Chung
  given: Junyoung
- family: Gulcehre
  given: Caglar
- family: Cho
  given: KyungHyun
- family: Bengio
  given: Yoshua
eprint: 1412.3555v1
file: 1412.3555v1.pdf
files:
- chung-junyoung-and-gulcehre-caglar-and-cho-kyunghyun-and-bengio-yoshuaempirical-evaluation-of-gated-recurrent-neural-networks-on-sequence-modeli.pdf
month: Dec
primaryclass: cs.NE
ref: 1412.3555v1
title: Empirical Evaluation of Gated Recurrent Neural Networks on Sequence   Modeling
type: article
url: http://arxiv.org/abs/1412.3555v1
year: '2014'
