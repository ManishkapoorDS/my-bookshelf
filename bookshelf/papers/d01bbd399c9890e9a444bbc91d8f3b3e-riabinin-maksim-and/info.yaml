abstract: 'Many recent breakthroughs in deep learning were achieved by training increasingly
  larger models on massive datasets. However, training such models can be prohibitively
  expensive. For instance, the cluster used to train GPT-3 costs over \$250 million.
  As a result, most researchers cannot afford to train state of the art models and
  contribute to their development. Hypothetically, a researcher could crowdsource
  the training of large neural networks with thousands of regular PCs provided by
  volunteers. The raw computing power of a hundred thousand \$2500 desktops dwarfs
  that of a \$250M server pod, but one cannot utilize that power efficiently with
  conventional distributed training methods. In this work, we propose Learning@home:
  a novel neural network training paradigm designed to handle large amounts of poorly
  connected participants. We analyze the performance, reliability, and architectural
  constraints of this paradigm and compare it against existing distributed training
  techniques.'
archiveprefix: arXiv
author: Riabinin, Maksim and Gusev, Anton
author_list:
- family: Riabinin
  given: Maksim
- family: Gusev
  given: Anton
eprint: 2002.04013v2
file: 2002.04013v2.pdf
files:
- riabinin-maksim-and-gusev-antonlearning-home-crowdsourced-training-of-large-neural-networks-using-decentralized-mixture-of-experts2020.pdf
month: Feb
primaryclass: cs.DC
ref: 2002.04013v2
time-added: 2020-09-04-15:29:09
title: 'Learning@home: Crowdsourced Training of Large Neural Networks using   Decentralized
  Mixture-of-Experts'
type: article
url: http://arxiv.org/abs/2002.04013v2
year: '2020'
