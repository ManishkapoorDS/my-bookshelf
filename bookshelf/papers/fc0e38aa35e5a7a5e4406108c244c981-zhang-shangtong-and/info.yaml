abstract: Recently experience replay is widely used in various deep reinforcement
  learning (RL) algorithms, in this paper we rethink the utility of experience replay.
  It introduces a new hyper-parameter, the memory buffer size, which needs carefully
  tuning. However unfortunately the importance of this new hyper-parameter has been
  underestimated in the community for a long time. In this paper we did a systematic
  empirical study of experience replay under various function representations. We
  showcase that a large replay buffer can significantly hurt the performance. Moreover,
  we propose a simple O(1) method to remedy the negative influence of a large replay
  buffer. We showcase its utility in both simple grid world and challenging domains
  like Atari games.
archiveprefix: arXiv
author: Zhang, Shangtong and Sutton, Richard S.
author_list:
- family: Zhang
  given: Shangtong
- family: Sutton
  given: Richard S.
eprint: 1712.01275v3
file: 1712.01275v3.pdf
files:
- zhang-shangtong-and-sutton-richard-s.a-deeper-look-at-experience-replay2017.pdf
month: Dec
primaryclass: cs.LG
ref: 1712.01275v3
time-added: 2020-06-13-17:34:55
title: A Deeper Look at Experience Replay
type: article
url: http://arxiv.org/abs/1712.01275v3
year: '2017'
