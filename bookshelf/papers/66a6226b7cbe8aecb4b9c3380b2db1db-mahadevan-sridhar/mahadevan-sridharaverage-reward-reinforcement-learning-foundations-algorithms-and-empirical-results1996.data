<!DOCTYPE html>
<html lang="en" class="no-js">
<head>
    <meta charset="UTF-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="access" content="Yes">

    

    <meta name="twitter:site" content="@SpringerLink"/>

    <meta name="twitter:card" content="summary"/>

    <meta name="twitter:image:alt" content="Content cover image"/>

    <meta name="journal_id" content="10994"/>

    <meta name="dc.title" content="Average reward reinforcement learning: Foundations, algorithms, and empirical results"/>

    <meta name="dc.source" content="Machine Learning 1996 22:1"/>

    <meta name="dc.format" content="text/html"/>

    <meta name="dc.publisher" content="Springer"/>

    <meta name="dc.type" content="OriginalPaper"/>

    <meta name="dc.language" content="En"/>

    <meta name="dc.copyright" content="1996 Kluwer Academic Publishers"/>

    <meta name="dc.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="dc.description" content="This paper presents a detailed study of average reward reinforcement learning, an undiscounted optimality framework that is more appropriate for cyclical tasks than the much better studied discounted framework. A wide spectrum of average reward algorithms are described, ranging from synchronous dynamic programming methods to several (provably convergent) asynchronous algorithms from optimal control and learning automata. A general sensitive discount optimality metric calledn-discount-optimality is introduced, and used to compare the various algorithms. The overview identifies a key similarity across several asynchronous algorithms that is crucial to their convergence, namely independent estimation of the average reward and the relative values. The overview also uncovers a surprising limitation shared by the different algorithms while several algorithms can provably generategain-optimal policies that maximize average reward, none of them can reliably filter these to producebias-optimal (orT-optimal) policies that also maximize the finite reward to absorbing goal states. This paper also presents a detailed empirical study of R-learning, an average reward reinforcement learning method, using two empirical testbeds: a stochastic grid world domain and a simulated robot environment. A detailed sensitivity analysis of R-learning is carried out to test its dependence on learning rates and exploration levels. The results suggest that R-learning is quite sensitive to exploration strategies and can fall into sub-optimal limit cycles. The performance of R-learning is also compared with that of Q-learning, the best studied discounted RL method. Here, the results suggest that R-learning can be fine-tuned to give better performance than Q-learning in both domains."/>

    <meta name="prism.issn" content="1573-0565"/>

    <meta name="prism.publicationName" content="Machine Learning"/>

    <meta name="prism.volume" content="22"/>

    <meta name="prism.number" content="1"/>

    <meta name="prism.section" content="OriginalPaper"/>

    <meta name="prism.startingPage" content="159"/>

    <meta name="prism.endingPage" content="195"/>

    <meta name="prism.copyright" content="1996 Kluwer Academic Publishers"/>

    <meta name="prism.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="prism.url" content="https://link.springer.com/article/10.1007/BF00114727"/>

    <meta name="prism.doi" content="doi:10.1007/BF00114727"/>

    <meta name="citation_pdf_url" content="https://link.springer.com/content/pdf/10.1007/BF00114727.pdf"/>

    <meta name="citation_fulltext_html_url" content="https://link.springer.com/article/10.1007/BF00114727"/>

    <meta name="citation_journal_title" content="Machine Learning"/>

    <meta name="citation_journal_abbrev" content="Mach Learn"/>

    <meta name="citation_publisher" content="Kluwer Academic Publishers"/>

    <meta name="citation_issn" content="1573-0565"/>

    <meta name="citation_title" content="Average reward reinforcement learning: Foundations, algorithms, and empirical results"/>

    <meta name="citation_volume" content="22"/>

    <meta name="citation_issue" content="1"/>

    <meta name="citation_publication_date" content="1996/03"/>

    <meta name="citation_firstpage" content="159"/>

    <meta name="citation_lastpage" content="195"/>

    <meta name="citation_article_type" content="Article"/>

    <meta name="citation_fulltext_world_readable" content=""/>

    <meta name="citation_language" content="en"/>

    <meta name="dc.identifier" content="doi:10.1007/BF00114727"/>

    <meta name="DOI" content="10.1007/BF00114727"/>

    <meta name="citation_doi" content="10.1007/BF00114727"/>

    <meta name="description" content="This paper presents a detailed study of average reward reinforcement learning, an undiscounted optimality framework that is more appropriate for cyclical t"/>

    <meta name="dc.creator" content="Sridhar Mahadevan"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Control, Robotics, Mechatronics"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Simulation and Modeling"/>

    <meta name="dc.subject" content="Natural Language Processing (NLP)"/>

    <meta name="citation_reference" content="Baird, L. Personal Communication."/>

    <meta name="citation_reference" content="Baird, L., (1995). Residual algorithms: Reinforcement learning with function approximation. InProceedings of the 12th International Conference on Machine Learning, pages 30&#8211;37. Morgan Kaufmann."/>

    <meta name="citation_reference" content="citation_journal_title=Artificial Intelligence; citation_title=Learning to act using real-time dynamic programming; citation_author=A. Barto, S. Bradtke, S. Singh; citation_volume=72; citation_issue=1; citation_publication_date=1995; citation_pages=81-138; citation_id=CR3"/>

    <meta name="citation_reference" content="Bertsekas, D., (1982). Distributed dynamic programming.IEEE Transactions on Automatic Control. AC-27(3)"/>

    <meta name="citation_reference" content="Bertsekas, D., (1987).Dynamic Programming: Deterministic and Stochastic Models. Prentice-Hall."/>

    <meta name="citation_reference" content="citation_journal_title=Annals of Mathematical Statistics; citation_title=Discrete dynamic programming; citation_author=D. Blackwell; citation_volume=33; citation_publication_date=1962; citation_pages=719-726; citation_id=CR6"/>

    <meta name="citation_reference" content="Boutilier, C. &amp; Puterman, M., (1995). Process-oriented planning and average-reward optimality. InProceedings of the Fourteenth JCAI, pages 1096&#8211;1103. Morgan Kaufmann."/>

    <meta name="citation_reference" content="Dayan, P. &amp; Hinton, G., (1992). Feudal reinforcement learning. InNeural Information Processing Systems (NIPS), pages 271&#8211;278."/>

    <meta name="citation_reference" content="citation_journal_title=Operations Research; citation_title=Computing a bias-optimal policy in a discrete-time Markov decision problem; citation_author=E. Denardo; citation_volume=18; citation_publication_date=1970; citation_pages=272-289; citation_id=CR9"/>

    <meta name="citation_reference" content="Dent, L., Boticario, J., McDermott, J., Mitchell, T. &amp; Zabowski, D., (1992). A personal learning apprentice InProceedings of the Tenth National Conference on Artificial Intelligence (AAAI), pages 96&#8211;103. MIT Press."/>

    <meta name="citation_reference" content="Engelberger, J., (1989).Robotics in Service. MIT Press."/>

    <meta name="citation_reference" content="citation_journal_title=Mathematics of Operations Research; citation_title=Successive approximation methods for solving nested functional equations in Markov decision problems; citation_author=A. Federgruen, P. Schweitzer; citation_volume=9; citation_publication_date=1984; citation_pages=319-344; citation_id=CR12"/>

    <meta name="citation_reference" content="citation_journal_title=Annals of Operations Research; citation_title=An improved algorithm for solving communicating average reward markov decision processes; citation_author=M. Haviv, M. Puterman; citation_volume=28; citation_publication_date=1991; citation_pages=229-242; citation_id=CR13"/>

    <meta name="citation_reference" content="citation_journal_title=Annals of Statistics; citation_title=A modified form of the iterative method of dynamic programming; citation_author=A. Hordijk, H. Tijms; citation_volume=3; citation_publication_date=1975; citation_pages=203-208; citation_id=CR14"/>

    <meta name="citation_reference" content="Howard, R., (1960).Dynamic Programming and Markov Processes. MIT Press."/>

    <meta name="citation_reference" content="Jalali, A. &amp; Ferguson, M., (1989). Computationally efficient adaptive control algorithms for Markov chains InProceedings of the 28th IEEE Conference on Decision and Control, pages 1283&#8211;1288."/>

    <meta name="citation_reference" content="Jalali, A. &amp; Ferguson, M., (1990). A distributed asynchronous algorithm for expected average cost dynamic programming. InProceedings of the 29th IEEE Conference on Dectsion and Control. pages 1394&#8211;1395."/>

    <meta name="citation_reference" content="Kaelbling, L., (1993a). Hierarchical learning in stochastic domains: Preliminary results. InProceedings of the Tenth International Conference on Machine Learning, pages 167&#8211;173 Morgan Kaufmann."/>

    <meta name="citation_reference" content="Kaelbling, L., (1993b)Learning in Embedded Systems. MIT Press."/>

    <meta name="citation_reference" content="Lin, L., (1993).Reinforcement Learning for Robots using Neural Networks. PhD thesis. Carnegie-Mellon Univ."/>

    <meta name="citation_reference" content="Mahadevan, S. A model-based bias-optimal reinforcement learning algorithm. In preparation."/>

    <meta name="citation_reference" content="Mahadevan, S., (1992). Enhancing transfer in reinforcement learning by building stochastic models of robot actions. InProceedings of the Seventh International Conference on Machine Learning, pages 290&#8211;299 Morgan Kaufmann."/>

    <meta name="citation_reference" content="Mahadevan, S., (1994). To discount or not to discount in reinforcement learning: A case study comparing R-learning and Q-learning. InProceedings of the Eleventh International Conference on Machine Learning. pages 164&#8211;172. Morgan Kaufmann."/>

    <meta name="citation_reference" content="Mahadevan, S. &amp; Baird, L. Value function approximation in average reward reinforcement learning. In preparation."/>

    <meta name="citation_reference" content="citation_journal_title=Artificial Intelligence; citation_title=Automatic programming of behavior-based robots using reinforcement learning; citation_author=S. Mahadevan, J. Connell; citation_volume=55; citation_publication_date=1992; citation_pages=311-365; citation_id=CR25"/>

    <meta name="citation_reference" content="Moore, A., (1991). Variable resolution dynamic programming: Efficiently learning action maps in multivariate real-valued state spaces. InProceedings of the Eighth International Workshop on Machine Learning, pages 333&#8211;337. Morgan Kaufmann."/>

    <meta name="citation_reference" content="Narendra, K. &amp; Thathachar, M., (1989).Learning Automata: An Introduction. Prentice Hall."/>

    <meta name="citation_reference" content="Puterman, M., (1994).Markov Decision Processes: Discrete Dynamic Stochastic Programming. John Wiley."/>

    <meta name="citation_reference" content="Ross, S., (1983).Introduction to Stochastic Dynamic Programming. Academic Press."/>

    <meta name="citation_reference" content="Salganicoff, M., (1993). Density-adaptive learning and forgetting. InProceedings of the Tenth International Conference on Machine Learning, pages 276&#8211;283. Morgan Kaufmann."/>

    <meta name="citation_reference" content="Schwartz, A., (1993). A reinforcement learning method for maximizing undiscounted rewards. InProceedings of the Tenth International Conference on Machine Learning, pages 298&#8211;305. Morgan Kaufmann."/>

    <meta name="citation_reference" content="Singh, S., (1994a).Learning to Solve Markovian Decision Processes. PhD thesis, Univ of Massachusetts, Amherst."/>

    <meta name="citation_reference" content="Singh, S., (1994b) Reinforcement learning algorithms for average-payoff Markovian decision processes. InProceedings of the 12th AAAI. MIT Press."/>

    <meta name="citation_reference" content="citation_journal_title=Machine Learning; citation_title=Learning to predict by the method of temporal differences; citation_author=R Sutton; citation_volume=3; citation_publication_date=1988; citation_pages=9-44; citation_id=CR34"/>

    <meta name="citation_reference" content="Sutton, R., (1990). Integrated architectures for learning, planning, and reacting based on approximating dynamic programming. InProceedings of the Seventh International Conference on Machine Learning, pages 216&#8211;224. Morgan Kaufmann."/>

    <meta name="citation_reference" content="Sutton, R., editor, (1992).Reinforcement Learning. Kluwer Academic Press. Special Issue of Machine Learning Journal Vol 8, Nos 3&#8211;4, May 1992."/>

    <meta name="citation_reference" content="Tadepall, P. Personal Communication."/>

    <meta name="citation_reference" content="Tadepalli, P. &amp; Ok, D., (1994). H learning: A reinforcement learning method to optimize undiscounted average reward. Technical Report 94-30-01, Oregon State Univ."/>

    <meta name="citation_reference" content="Tesauro, G., (1992). Practical issues in temporal difference learning. In R. Sutton, editor,Reinforcement Learning. Kluwer Academic Publishers."/>

    <meta name="citation_reference" content="Thrun, S. The role of exploration in learning control. In D. A. White and D. A. Sofge, editors,Handbook of Intelligent Control: Neural, Fuzzy, and Adaptive Approaches. Van Nostrand Reinhold."/>

    <meta name="citation_reference" content="citation_journal_title=Machine Learning; citation_title=Asynchronous stochastic approximation and Q-learning; citation_author=J. Tsitsiklis; citation_volume=16; citation_publication_date=1994; citation_pages=185-202; citation_id=CR41"/>

    <meta name="citation_reference" content="citation_journal_title=Annals of Mathematical Statistics; citation_title=Discrete dynamic programming with sensitive discount optimality criteria; citation_author=A. Veinott; citation_volume=40; citation_issue=5; citation_publication_date=1969; citation_pages=1635-1660; citation_id=CR42"/>

    <meta name="citation_reference" content="Watkins, C., (1989).Learning from Delayed Rewards. PhD thesis, King&#39;s College, Cambridge, England."/>

    <meta name="citation_reference" content="Wheeler, R. &amp; Narendra, K., (1986). Decentralized learning in finite Markov chains.IEEE Transactions on Automatic Control, AC-31(6)"/>

    <meta name="citation_reference" content="citation_journal_title=Journal of Mathematical Analysis and Applications; citation_title=Dynamic programming, markov chains, and the method of successive approximations; citation_author=D. White; citation_volume=6; citation_publication_date=1963; citation_pages=373-376; citation_id=CR45"/>

    <meta name="citation_reference" content="Whitehead, S., Karlsson, J. &amp; Tenenberg, J., (1993). Learning multiple goal behavior via task decomposition and dynamic policy merging. In J. Connell and S. Mahadevan, editors,Robot Learning. Kluwer Academic Publishers."/>

    <meta name="citation_author" content="Sridhar Mahadevan"/>

    <meta name="citation_author_email" content="mahadeva@samuel.csee.usf.edu"/>

    <meta name="citation_author_institution" content="Department of Computer Science and Engineering, University of South Florida, Tampa"/>

    <meta name="citation_springer_api_url" content="http://api.springer.com/metadata/pam?q=doi:10.1007/BF00114727&amp;api_key="/>

    <meta name="format-detection" content="telephone=no"/>

    <meta name="citation_cover_date" content="1996/03/01"/>


    
        <meta property="og:url" content="https://link.springer.com/article/10.1007/BF00114727"/>
        <meta property="og:type" content="article"/>
        <meta property="og:site_name" content="Machine Learning"/>
        <meta property="og:title" content="Average reward reinforcement learning: Foundations, algorithms, and empirical results"/>
        <meta property="og:description" content="This paper presents a detailed study of average reward reinforcement learning, an undiscounted optimality framework that is more appropriate for cyclical tasks than the much better studied discounted framework. A wide spectrum of average reward algorithms are described, ranging from synchronous dynamic programming methods to several (provably convergent) asynchronous algorithms from optimal control and learning automata. A general sensitive discount optimality metric calledn-discount-optimality is introduced, and used to compare the various algorithms. The overview identifies a key similarity across several asynchronous algorithms that is crucial to their convergence, namely independent estimation of the average reward and the relative values. The overview also uncovers a surprising limitation shared by the different algorithms while several algorithms can provably generategain-optimal policies that maximize average reward, none of them can reliably filter these to producebias-optimal (orT-optimal) policies that also maximize the finite reward to absorbing goal states. This paper also presents a detailed empirical study of R-learning, an average reward reinforcement learning method, using two empirical testbeds: a stochastic grid world domain and a simulated robot environment. A detailed sensitivity analysis of R-learning is carried out to test its dependence on learning rates and exploration levels. The results suggest that R-learning is quite sensitive to exploration strategies and can fall into sub-optimal limit cycles. The performance of R-learning is also compared with that of Q-learning, the best studied discounted RL method. Here, the results suggest that R-learning can be fine-tuned to give better performance than Q-learning in both domains."/>
        <meta property="og:image" content="https://media.springernature.com/w110/springer-static/cover/journal/10994.jpg"/>
    

    <title>Average reward reinforcement learning: Foundations, algorithms, and empirical results | SpringerLink</title>

    <link rel="shortcut icon" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16 32x32 48x48" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-16x16-8bd8c1c945.png />
<link rel="icon" sizes="32x32" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-32x32-61a52d80ab.png />
<link rel="icon" sizes="48x48" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-48x48-0ec46b6b10.png />
<link rel="apple-touch-icon" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />
<link rel="apple-touch-icon" sizes="72x72" href=/oscar-static/images/favicons/springerlink/ic_launcher_hdpi-f77cda7f65.png />
<link rel="apple-touch-icon" sizes="76x76" href=/oscar-static/images/favicons/springerlink/app-icon-ipad-c3fd26520d.png />
<link rel="apple-touch-icon" sizes="114x114" href=/oscar-static/images/favicons/springerlink/app-icon-114x114-3d7d4cf9f3.png />
<link rel="apple-touch-icon" sizes="120x120" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@2x-67b35150b3.png />
<link rel="apple-touch-icon" sizes="144x144" href=/oscar-static/images/favicons/springerlink/ic_launcher_xxhdpi-986442de7b.png />
<link rel="apple-touch-icon" sizes="152x152" href=/oscar-static/images/favicons/springerlink/app-icon-ipad@2x-677ba24d04.png />
<link rel="apple-touch-icon" sizes="180x180" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />


    
    <script>(function(H){H.className=H.className.replace(/\bno-js\b/,'js')})(document.documentElement)</script>

    <link rel="stylesheet" href=/oscar-static/app-springerlink/css/core-article-b0cd12fb00.css media="screen">
    <link rel="stylesheet" id="js-mustard" href=/oscar-static/app-springerlink/css/enhanced-article-fc7a197567.css media="only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)">
    

    
    <script type="text/javascript">
        window.dataLayer = [{"Country":"IT","doi":"10.1007-BF00114727","Journal Title":"Machine Learning","Journal Id":10994,"Keywords":"Reinforcement learning, Markov decision processes","kwrd":["Reinforcement_learning","Markov_decision_processes"],"Labs":"Y","ksg":"Krux.segments","kuid":"Krux.uid","Has Body":"N","Features":[],"Open Access":"N","hasAccess":"Y","bypassPaywall":"N","user":{"license":{"businessPartnerID":[],"businessPartnerIDString":""}},"Access Type":"permanently-free","Bpids":"","Bpnames":"","BPID":["1"],"VG Wort Identifier":"pw-vgzm.415900-10.1007-BF00114727","Full HTML":"N","Subject Codes":["SCI","SCI21000","SCT19000","SCI19000","SCI21040"],"pmc":["I","I21000","T19000","I19000","I21040"],"session":{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},"content":{"serial":{"eissn":"1573-0565","pissn":"0885-6125"},"type":"Article","category":{"pmc":{"primarySubject":"Computer Science","primarySubjectCode":"I","secondarySubjects":{"1":"Artificial Intelligence","2":"Control, Robotics, Mechatronics","3":"Artificial Intelligence","4":"Simulation and Modeling","5":"Natural Language Processing (NLP)"},"secondarySubjectCodes":{"1":"I21000","2":"T19000","3":"I21000","4":"I19000","5":"I21040"}},"sucode":"SC6"},"attributes":{"deliveryPlatform":"oscar"}},"Event Category":"Article","GA Key":"UA-26408784-1","DOI":"10.1007/BF00114727","Page":"article","page":{"attributes":{"environment":"live"}}}];
    </script>


    
    
        
            <script src=/oscar-static/js/jquery-220afd743d.js></script>
        
    

    
        <script>
    (function() {
        function deleteCookie (name, domain) {
            document.cookie = encodeURIComponent(name) +
                    '=' +
                    ';path=/' +
                    ';domain=' + domain +
                    ';expires=Thu, 01 Jan 1970 00:00:00 GMT';
        }

        var consentCookieParts = ('; ' + document.cookie).split('; OptanonConsent=');

        if (consentCookieParts.length > 1) {
            consentCookieParts.shift(); // remove redundant first part from the split array

            // onetrust can set the same cookie multiple times with different domain specificities
            for (let i=0; i<consentCookieParts.length; i++) {
                var otCookieGroups = consentCookieParts[i].split('&groups=').pop().split('&').shift();

                if (otCookieGroups.indexOf('C0001') === -1) {
                    // to be deleted when new onetrust implemented on www.springer.com pages
                    deleteCookie('OptanonConsent', '.springer.com');

                    deleteCookie('OptanonConsent', 'link.springer.com');
                    deleteCookie('OptanonAlertBoxClosed', 'link.springer.com');
                }
            }
        }
    })();
</script>
    

    <script data-test="onetrust-control">
        
            (function(w,d,t) {
                var assetPath = '/oscar-static/js/cookie-consent-es5-bundle-0ea0aa3601.js';
                function cc() {
                    var h = w.location.hostname,
                        e = d.createElement(t),
                        s = d.getElementsByTagName(t)[0];

                    if (h === "link.springer.com") {
                        e.src = "https://cdn.cookielaw.org/scripttemplates/otSDKStub.js";
                        e.setAttribute("data-domain-script", "4f53bc14-4ee3-45bd-9935-e3d2b6b2a543");
                    } else {
                        e.src = assetPath;
                        e.setAttribute("data-consent", h);
                    }
                    s.parentNode.insertBefore(e, s);
                }
                w.google_tag_manager ? cc() : window.addEventListener("gtm_loaded", cc);
            })(window,document,"script");
        
    </script>
    <script>
        function OptanonWrapper() {
            var elementInside = function(candidate, element) {
                if (candidate === element) {
                    return true;
                } else if (candidate.nodeName.toLowerCase() === 'body') {
                    return false;
                } else {
                    return elementInside(candidate.parentNode, element);
                }
            };

            var disclaimer = document.querySelector('.c-disclaimer[aria-hidden="false"]');
            window.dataLayer.push({event:'OneTrustGroupsUpdated'});
            if (disclaimer) {
                if (!elementInside(document.activeElement, disclaimer)) {
                    disclaimer.querySelector('button').focus();
                }
            } else {
                document.activeElement.blur();
            }
        }
    </script>

    <script>
    window.config = window.config || {};
    window.config.mustardcut = false;

    
    if (window.matchMedia && window.matchMedia('only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)').matches) {
        window.config.mustardcut = true;
    }
</script>

    <!--Polyfills CustomEvent constructor in IE. Allows us to use events to manage race conditions in client side js-->
<script>
    (function () {
        if ( typeof window.CustomEvent === "function" ) return false;
        function CustomEvent ( event, params ) {
            params = params || { bubbles: false, cancelable: false, detail: null };
            var evt = document.createEvent( 'CustomEvent' );
            evt.initCustomEvent( event, params.bubbles, params.cancelable, params.detail );
            return evt;
        }

        CustomEvent.prototype = window.Event.prototype;

        window.CustomEvent = CustomEvent;
    })();
</script>

    
    
        <!-- Google Tag Manager -->
        <script data-test="gtm-head">
            if (window.config.mustardcut) {
                (function (w, d, s, l, i) {
                    w[l] = w[l] || [];
                    w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                    var f = d.getElementsByTagName(s)[0],
                            j = d.createElement(s),
                            dl = l != 'dataLayer' ? '&l=' + l : '';
                    j.async = true;
                    j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                    
                    j.addEventListener('load', function() {
                        var _ge = new CustomEvent('gtm_loaded', { bubbles: true });
                        d.dispatchEvent(_ge);
                    });
                    f.parentNode.insertBefore(j, f);
                })(window, document, 'script', 'dataLayer', 'GTM-WCF9Z9');
            }
        </script>
        <!-- End Google Tag Manager -->
    


    <script class="js-entry">
    if (window.config.mustardcut) {
        (function(w, d) {
            
            
            
                window.Component = {};
            

            var currentScript = d.currentScript || d.head.querySelector('script.js-entry');

            
            function catchNoModuleSupport() {
                var scriptEl = d.createElement('script');
                return (!('noModule' in scriptEl) && 'onbeforeload' in scriptEl)
            }

            var headScripts = [
                {'src': '/oscar-static/js/polyfill-es5-bundle-974eb189f7.js', 'async': false},
                {'src': '/oscar-static/js/airbrake-es5-bundle-2c32f222f7.js', 'async': false},
            ];

            var bodyScripts = [
                {'src': '/oscar-static/js/app-es5-bundle-6bf1df7b77.js', 'async': false, 'module': false},
                {'src': '/oscar-static/js/app-es6-bundle-91efed6b7f.js', 'async': false, 'module': true}
                
                
                    , {'src': '/oscar-static/js/global-article-es5-bundle-fcbea0a133.js', 'async': false, 'module': false},
                    {'src': '/oscar-static/js/global-article-es6-bundle-13a7535994.js', 'async': false, 'module': true}
                
            ];

            function createScript(script) {
                var scriptEl = d.createElement('script');
                scriptEl.src = script.src;
                scriptEl.async = script.async;
                if (script.module === true) {
                    scriptEl.type = "module";
                    if (catchNoModuleSupport()) {
                        scriptEl.src = '';
                    }
                } else if (script.module === false) {
                    scriptEl.setAttribute('nomodule', true)
                }
                if (script.charset) {
                    scriptEl.setAttribute('charset', script.charset);
                }

                return scriptEl;
            }

            for (var i = 0; i < headScripts.length; ++i) {
                var scriptEl = createScript(headScripts[i]);
                currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);
            }

            d.addEventListener('DOMContentLoaded', function() {
                for (var i = 0; i < bodyScripts.length; ++i) {
                    var scriptEl = createScript(bodyScripts[i]);
                    d.body.appendChild(scriptEl);
                }
            });

            // Webfont repeat view
            var config = w.config;
            if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {
                d.documentElement.className += ' webfonts-loaded';
            }
        })(window, document);
    }
</script>

    
    
    <link rel="canonical" href="https://link.springer.com/article/10.1007/BF00114727"/>
    

</head>
<body class="shared-article-renderer">
    
    
    
        <!-- Google Tag Manager (noscript) -->
        <noscript data-test="gtm-body">
            <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WCF9Z9"
            height="0" width="0" style="display:none;visibility:hidden"></iframe>
        </noscript>
        <!-- End Google Tag Manager (noscript) -->
    


    <div class="u-vh-full">
        
    <aside class="c-ad c-ad--728x90" data-test="springer-doubleclick-ad">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-LB1" data-gpt-unitpath="/270604982/springerlink/10994/article" data-gpt-sizes="728x90" data-gpt-targeting="pos=LB1;articleid=BF00114727;"></div>
        </div>
    </aside>

<div class="u-position-relative">
    <header class="c-header u-mb-24" data-test="publisher-header">
        <div class="c-header__container">
            <div class="c-header__brand">
                
    <a id="logo" class="u-display-block" href="/" title="Go to homepage" data-test="springerlink-logo">
        <picture>
            <source type="image/svg+xml" srcset=/oscar-static/images/springerlink/svg/springerlink-253e23a83d.svg>
            <img src=/oscar-static/images/springerlink/png/springerlink-1db8a5b8b1.png alt="SpringerLink" width="148" height="30" data-test="header-academic">
        </picture>
        
        
    </a>


            </div>
            <div class="c-header__navigation">
                
    
        <button type="button"
                class="c-header__link u-button-reset u-mr-24"
                data-expander
                data-expander-target="#popup-search"
                data-expander-autofocus="firstTabbable"
                data-test="header-search-button">
            <span class="u-display-flex u-align-items-center">
                Search
                <svg class="c-icon u-ml-8" width="22" height="22" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </span>
        </button>
        <nav>
            <ul class="c-header__menu">
                
                <li class="c-header__item">
                    <a 
                        data-test="login-link" 
                        class="c-header__link" 
                        href="//link.springer.com/signup-login?previousUrl=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2FBF00114727"
                        data-track="click"
                        data-track-category="header"
                        data-track-action="login header"
                        data-track-label="link">Log in</a>
                </li>
                

                
            </ul>
        </nav>
    

    



            </div>
        </div>
    </header>

    
        <div id="popup-search" class="c-popup-search u-mb-16 js-header-search u-js-hide">
            <div class="c-popup-search__content">
                <div class="u-container">
                    <div class="c-popup-search__container" data-test="springerlink-popup-search">
                        <div class="app-search">
    <form role="search" method="GET" action="/search" >
        <label for="search" class="app-search__label">Search SpringerLink</label>
        <div class="app-search__content">
            <input id="search" class="app-search__input" data-search-input autocomplete="off" role="textbox" name="query" type="text" value="">
            <button class="app-search__button" type="submit">
                <span class="u-visually-hidden">Search</span>
                <svg class="c-icon" width="14" height="14" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </button>
            
                <input type="hidden" name="searchType" value="publisherSearch">
            
            
        </div>
    </form>
</div>

                    </div>
                </div>
            </div>
        </div>
    
</div>

        

    <div class="u-container u-mt-32 u-mb-32 u-clearfix" id="main-content" data-component="article-container">
        <main class="c-article-main-column u-float-left js-main-column" data-track-component="article body">
            
                
                <div class="c-context-bar u-hide" data-component="context-bar" aria-hidden="true">
                    <div class="c-context-bar__container u-container">
                        <div class="c-context-bar__title">
                            Average reward reinforcement learning: Foundations, algorithms, and empirical results
                        </div>
                        
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/BF00114727.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button" download>
            
                <span>Download PDF</span>
                <svg aria-hidden="true" focusable="false" width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                    </div>
                </div>
            

            <div class="c-pdf-button__container">
                
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/BF00114727.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button" download>
            
                <span>Download PDF</span>
                <svg aria-hidden="true" focusable="false" width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

            </div>

            <article itemscope itemtype="http://schema.org/ScholarlyArticle" lang="en">
                <div class="c-article-header">
                    <header>
                        <ul class="c-article-identifiers" data-test="article-identifier">
                            
    
    
    

                            <li class="c-article-identifiers__item"><a href="#article-info" data-track="click" data-track-action="publication date" data-track-label="link">Published: <time datetime="1996-03" itemprop="datePublished">March 1996</time></a></li>
                        </ul>

                        
                        <h1 class="c-article-title" data-test="article-title" data-article-title="" itemprop="name headline">Average reward reinforcement learning: Foundations, algorithms, and empirical results</h1>
                        <ul class="c-author-list js-etal-collapsed" data-etal="25" data-etal-small="3" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Sridhar-Mahadevan" data-author-popup="auth-Sridhar-Mahadevan">Sridhar Mahadevan</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="University of South Florida" /><meta itemprop="address" content="grid.170693.a, 000000012353285X, Department of Computer Science and Engineering, University of South Florida, 33620, Tampa, Florida" /></span></sup> </li></ul>
                        <p class="c-article-info-details" data-container-section="info">
                            
    <a data-test="journal-link" href="/journal/10994"><i data-test="journal-title">Machine Learning</i></a>

                            <b data-test="journal-volume"><span class="u-visually-hidden">volume</span> 22</b>, <span class="u-visually-hidden">pages</span><span itemprop="pageStart">159</span>–<span itemprop="pageEnd">195</span>(<span data-test="article-publication-year">1996</span>)<a href="#citeas" class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-label="link">Cite this article</a>
                        </p>
                        
    

                        <div data-test="article-metrics">
                            <div id="altmetric-container">
    
        <div class="c-article-metrics-bar__wrapper u-clear-both">
            <ul class="c-article-metrics-bar u-list-reset">
                
                    <li class=" c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">6330 <span class="c-article-metrics-bar__label">Accesses</span></p>
                    </li>
                
                
                    <li class="c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">155 <span class="c-article-metrics-bar__label">Citations</span></p>
                    </li>
                
                
                <li class="c-article-metrics-bar__item">
                    <p class="c-article-metrics-bar__details"><a href="/article/10.1007%2FBF00114727/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
                </li>
            </ul>
        </div>
    
</div>

                        </div>
                            
    
        

        

    

                    </header>
                </div>

                <div data-article-body="true" data-track-component="article body" class="c-article-body">
                    <section aria-labelledby="Abs1" data-title="Abstract" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><p>This paper presents a detailed study of average reward reinforcement learning, an undiscounted optimality framework that is more appropriate for cyclical tasks than the much better studied discounted framework. A wide spectrum of average reward algorithms are described, ranging from synchronous dynamic programming methods to several (provably convergent) asynchronous algorithms from optimal control and learning automata. A general sensitive discount optimality metric called<i>n-discount-optimality</i> is introduced, and used to compare the various algorithms. The overview identifies a key similarity across several asynchronous algorithms that is crucial to their convergence, namely independent estimation of the average reward and the relative values. The overview also uncovers a surprising limitation shared by the different algorithms while several algorithms can provably generate<i>gain-optimal</i> policies that maximize average reward, none of them can reliably filter these to produce<i>bias-optimal</i> (or<i>T-optimal</i>) policies that also maximize the finite reward to absorbing goal states. This paper also presents a detailed empirical study of R-learning, an average reward reinforcement learning method, using two empirical testbeds: a stochastic grid world domain and a simulated robot environment. A detailed sensitivity analysis of R-learning is carried out to test its dependence on learning rates and exploration levels. The results suggest that R-learning is quite sensitive to exploration strategies and can fall into sub-optimal limit cycles. The performance of R-learning is also compared with that of Q-learning, the best studied discounted RL method. Here, the results suggest that R-learning can be fine-tuned to give better performance than Q-learning in both domains.</p></div></div></section>
                    
    


                    

                    
    <div class="note test-pdf-link" id="cobranding-and-download-availability-text">
        <div class="c-article-access-provider" aria-hidden="true" data-component="provided-by-box">
            
            
                <p class="c-article-access-provider__text">
                    <a href="/content/pdf/10.1007/BF00114727.pdf" target="_blank" rel="noopener"
                    data-track="click" data-track-action="download pdf" data-track-label="inline link">Download</a> to read the full article text
                </p>
            
        </div>
    </div>


                    
                        
                            
                        
                    

                    <section aria-labelledby="Bib1" data-title="References"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references"><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Baird, L. Personal Communication." /><p class="c-article-references__text" id="ref-CR1">Baird, L. Personal Communication.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Baird, L., (1995). Residual algorithms: Reinforcement learning with function approximation. InProceedings of t" /><p class="c-article-references__text" id="ref-CR2">Baird, L., (1995). Residual algorithms: Reinforcement learning with function approximation. In<i>Proceedings of the 12th International Conference on Machine Learning</i>, pages 30–37. Morgan Kaufmann.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="A.. Barto, S.. Bradtke, S.. Singh, " /><meta itemprop="datePublished" content="1995" /><meta itemprop="headline" content="Barto, A., Bradtke, S. &amp; Singh, S., (1995). Learning to act using real-time dynamic programming.Artificial Int" /><p class="c-article-references__text" id="ref-CR3">Barto, A., Bradtke, S. &amp; Singh, S., (1995). Learning to act using real-time dynamic programming.<i>Artificial Intelligence</i>, 72(1):81–138.</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 3 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Learning%20to%20act%20using%20real-time%20dynamic%20programming&amp;journal=Artificial%20Intelligence&amp;volume=72&amp;issue=1&amp;pages=81-138&amp;publication_year=1995&amp;author=Barto%2CA.&amp;author=Bradtke%2CS.&amp;author=Singh%2CS.">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Bertsekas, D., (1982). Distributed dynamic programming.IEEE Transactions on Automatic Control. AC-27(3)" /><p class="c-article-references__text" id="ref-CR4">Bertsekas, D., (1982). Distributed dynamic programming.<i>IEEE Transactions on Automatic Control</i>. AC-27(3)</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Bertsekas, D., (1987).Dynamic Programming: Deterministic and Stochastic Models. Prentice-Hall." /><p class="c-article-references__text" id="ref-CR5">Bertsekas, D., (1987).<i>Dynamic Programming: Deterministic and Stochastic Models</i>. Prentice-Hall.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="D.. Blackwell, " /><meta itemprop="datePublished" content="1962" /><meta itemprop="headline" content="Blackwell, D., (1962). Discrete dynamic programming.Annals of Mathematical Statistics, 33 719–726." /><p class="c-article-references__text" id="ref-CR6">Blackwell, D., (1962). Discrete dynamic programming.<i>Annals of Mathematical Statistics</i>, 33 719–726.</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 6 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Discrete%20dynamic%20programming&amp;journal=Annals%20of%20Mathematical%20Statistics&amp;volume=33&amp;pages=719-726&amp;publication_year=1962&amp;author=Blackwell%2CD.">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Boutilier, C. &amp; Puterman, M., (1995). Process-oriented planning and average-reward optimality. InProceedings o" /><p class="c-article-references__text" id="ref-CR7">Boutilier, C. &amp; Puterman, M., (1995). Process-oriented planning and average-reward optimality. In<i>Proceedings of the Fourteenth JCAI</i>, pages 1096–1103. Morgan Kaufmann.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Dayan, P. &amp; Hinton, G., (1992). Feudal reinforcement learning. InNeural Information Processing Systems (NIPS)," /><p class="c-article-references__text" id="ref-CR8">Dayan, P. &amp; Hinton, G., (1992). Feudal reinforcement learning. In<i>Neural Information Processing Systems (NIPS)</i>, pages 271–278.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="E.. Denardo, " /><meta itemprop="datePublished" content="1970" /><meta itemprop="headline" content="Denardo, E., (1970). Computing a bias-optimal policy in a discrete-time Markov decision problem.Operations Res" /><p class="c-article-references__text" id="ref-CR9">Denardo, E., (1970). Computing a bias-optimal policy in a discrete-time Markov decision problem.<i>Operations Research</i>, 18:272–289.</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 9 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Computing%20a%20bias-optimal%20policy%20in%20a%20discrete-time%20Markov%20decision%20problem&amp;journal=Operations%20Research&amp;volume=18&amp;pages=272-289&amp;publication_year=1970&amp;author=Denardo%2CE.">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Dent, L., Boticario, J., McDermott, J., Mitchell, T. &amp; Zabowski, D., (1992). A personal learning apprentice In" /><p class="c-article-references__text" id="ref-CR10">Dent, L., Boticario, J., McDermott, J., Mitchell, T. &amp; Zabowski, D., (1992). A personal learning apprentice In<i>Proceedings of the Tenth National Conference on Artificial Intelligence (AAAI)</i>, pages 96–103. MIT Press.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Engelberger, J., (1989).Robotics in Service. MIT Press." /><p class="c-article-references__text" id="ref-CR11">Engelberger, J., (1989).<i>Robotics in Service</i>. MIT Press.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="A.. Federgruen, P.. Schweitzer, " /><meta itemprop="datePublished" content="1984" /><meta itemprop="headline" content="Federgruen, A. &amp; Schweitzer, P., (1984). Successive approximation methods for solving nested functional equati" /><p class="c-article-references__text" id="ref-CR12">Federgruen, A. &amp; Schweitzer, P., (1984). Successive approximation methods for solving nested functional equations in Markov decision problems.<i>Mathematics of Operations Research</i>, 9:319–344.</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 12 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Successive%20approximation%20methods%20for%20solving%20nested%20functional%20equations%20in%20Markov%20decision%20problems&amp;journal=Mathematics%20of%20Operations%20Research&amp;volume=9&amp;pages=319-344&amp;publication_year=1984&amp;author=Federgruen%2CA.&amp;author=Schweitzer%2CP.">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="M.. Haviv, M.. Puterman, " /><meta itemprop="datePublished" content="1991" /><meta itemprop="headline" content="Haviv, M. &amp; Puterman, M., (1991) An improved algorithm for solving communicating average reward markov decisio" /><p class="c-article-references__text" id="ref-CR13">Haviv, M. &amp; Puterman, M., (1991) An improved algorithm for solving communicating average reward markov decision processes.<i>Annals of Operations Research</i>, 28:229–242.</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 13 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=An%20improved%20algorithm%20for%20solving%20communicating%20average%20reward%20markov%20decision%20processes&amp;journal=Annals%20of%20Operations%20Research&amp;volume=28&amp;pages=229-242&amp;publication_year=1991&amp;author=Haviv%2CM.&amp;author=Puterman%2CM.">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="A.. Hordijk, H.. Tijms, " /><meta itemprop="datePublished" content="1975" /><meta itemprop="headline" content="Hordijk, A. &amp; Tijms, H., (1975). A modified form of the iterative method of dynamic programming.Annals of Stat" /><p class="c-article-references__text" id="ref-CR14">Hordijk, A. &amp; Tijms, H., (1975). A modified form of the iterative method of dynamic programming.<i>Annals of Statistics</i>, 3:203–208.</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 14 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20modified%20form%20of%20the%20iterative%20method%20of%20dynamic%20programming&amp;journal=Annals%20of%20Statistics&amp;volume=3&amp;pages=203-208&amp;publication_year=1975&amp;author=Hordijk%2CA.&amp;author=Tijms%2CH.">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Howard, R., (1960).Dynamic Programming and Markov Processes. MIT Press." /><p class="c-article-references__text" id="ref-CR15">Howard, R., (1960).<i>Dynamic Programming and Markov Processes</i>. MIT Press.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Jalali, A. &amp; Ferguson, M., (1989). Computationally efficient adaptive control algorithms for Markov chains InP" /><p class="c-article-references__text" id="ref-CR16">Jalali, A. &amp; Ferguson, M., (1989). Computationally efficient adaptive control algorithms for Markov chains In<i>Proceedings of the 28th IEEE Conference on Decision and Control</i>, pages 1283–1288.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Jalali, A. &amp; Ferguson, M., (1990). A distributed asynchronous algorithm for expected average cost dynamic prog" /><p class="c-article-references__text" id="ref-CR17">Jalali, A. &amp; Ferguson, M., (1990). A distributed asynchronous algorithm for expected average cost dynamic programming. In<i>Proceedings of the 29th IEEE Conference on Dectsion and Control</i>. pages 1394–1395.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Kaelbling, L., (1993a). Hierarchical learning in stochastic domains: Preliminary results. InProceedings of the" /><p class="c-article-references__text" id="ref-CR18">Kaelbling, L., (1993a). Hierarchical learning in stochastic domains: Preliminary results. In<i>Proceedings of the Tenth International Conference on Machine Learning</i>, pages 167–173 Morgan Kaufmann.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Kaelbling, L., (1993b)Learning in Embedded Systems. MIT Press." /><p class="c-article-references__text" id="ref-CR19">Kaelbling, L., (1993b)<i>Learning in Embedded Systems</i>. MIT Press.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Lin, L., (1993).Reinforcement Learning for Robots using Neural Networks. PhD thesis. Carnegie-Mellon Univ." /><p class="c-article-references__text" id="ref-CR20">Lin, L., (1993).<i>Reinforcement Learning for Robots using Neural Networks</i>. PhD thesis. Carnegie-Mellon Univ.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Mahadevan, S. A model-based bias-optimal reinforcement learning algorithm. In preparation." /><p class="c-article-references__text" id="ref-CR21">Mahadevan, S. A model-based bias-optimal reinforcement learning algorithm. In preparation.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Mahadevan, S., (1992). Enhancing transfer in reinforcement learning by building stochastic models of robot act" /><p class="c-article-references__text" id="ref-CR22">Mahadevan, S., (1992). Enhancing transfer in reinforcement learning by building stochastic models of robot actions. In<i>Proceedings of the Seventh International Conference on Machine Learning</i>, pages 290–299 Morgan Kaufmann.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Mahadevan, S., (1994). To discount or not to discount in reinforcement learning: A case study comparing R-lear" /><p class="c-article-references__text" id="ref-CR23">Mahadevan, S., (1994). To discount or not to discount in reinforcement learning: A case study comparing R-learning and Q-learning. In<i>Proceedings of the Eleventh International Conference on Machine Learning</i>. pages 164–172. Morgan Kaufmann.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Mahadevan, S. &amp; Baird, L. Value function approximation in average reward reinforcement learning. In preparatio" /><p class="c-article-references__text" id="ref-CR24">Mahadevan, S. &amp; Baird, L. Value function approximation in average reward reinforcement learning. In preparation.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="S.. Mahadevan, J.. Connell, " /><meta itemprop="datePublished" content="1992" /><meta itemprop="headline" content="Mahadevan, S. &amp; Connell, J., (1992). Automatic programming of behavior-based robots using reinforcement learni" /><p class="c-article-references__text" id="ref-CR25">Mahadevan, S. &amp; Connell, J., (1992). Automatic programming of behavior-based robots using reinforcement learning.<i>Artificial Intelligence</i>, 55:311–365. Appeared originally as IBM TR RC16359. Dec 1990.</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 25 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Automatic%20programming%20of%20behavior-based%20robots%20using%20reinforcement%20learning&amp;journal=Artificial%20Intelligence&amp;volume=55&amp;pages=311-365&amp;publication_year=1992&amp;author=Mahadevan%2CS.&amp;author=Connell%2CJ.">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Moore, A., (1991). Variable resolution dynamic programming: Efficiently learning action maps in multivariate r" /><p class="c-article-references__text" id="ref-CR26">Moore, A., (1991). Variable resolution dynamic programming: Efficiently learning action maps in multivariate real-valued state spaces. In<i>Proceedings of the Eighth International Workshop on Machine Learning</i>, pages 333–337. Morgan Kaufmann.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Narendra, K. &amp; Thathachar, M., (1989).Learning Automata: An Introduction. Prentice Hall." /><p class="c-article-references__text" id="ref-CR27">Narendra, K. &amp; Thathachar, M., (1989).<i>Learning Automata: An Introduction</i>. Prentice Hall.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Puterman, M., (1994).Markov Decision Processes: Discrete Dynamic Stochastic Programming. John Wiley." /><p class="c-article-references__text" id="ref-CR28">Puterman, M., (1994).<i>Markov Decision Processes: Discrete Dynamic Stochastic Programming</i>. John Wiley.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Ross, S., (1983).Introduction to Stochastic Dynamic Programming. Academic Press." /><p class="c-article-references__text" id="ref-CR29">Ross, S., (1983).<i>Introduction to Stochastic Dynamic Programming</i>. Academic Press.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Salganicoff, M., (1993). Density-adaptive learning and forgetting. InProceedings of the Tenth International Co" /><p class="c-article-references__text" id="ref-CR30">Salganicoff, M., (1993). Density-adaptive learning and forgetting. In<i>Proceedings of the Tenth International Conference on Machine Learning</i>, pages 276–283. Morgan Kaufmann.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Schwartz, A., (1993). A reinforcement learning method for maximizing undiscounted rewards. InProceedings of th" /><p class="c-article-references__text" id="ref-CR31">Schwartz, A., (1993). A reinforcement learning method for maximizing undiscounted rewards. In<i>Proceedings of the Tenth International Conference on Machine Learning</i>, pages 298–305. Morgan Kaufmann.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Singh, S., (1994a).Learning to Solve Markovian Decision Processes. PhD thesis, Univ of Massachusetts, Amherst." /><p class="c-article-references__text" id="ref-CR32">Singh, S., (1994a).<i>Learning to Solve Markovian Decision Processes</i>. PhD thesis, Univ of Massachusetts, Amherst.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Singh, S., (1994b) Reinforcement learning algorithms for average-payoff Markovian decision processes. InProcee" /><p class="c-article-references__text" id="ref-CR33">Singh, S., (1994b) Reinforcement learning algorithms for average-payoff Markovian decision processes. In<i>Proceedings of the 12th AAAI</i>. MIT Press.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="R. Sutton, " /><meta itemprop="datePublished" content="1988" /><meta itemprop="headline" content="Sutton, R., (1988). Learning to predict by the method of temporal differences.Machine Learning, 3:9–44." /><p class="c-article-references__text" id="ref-CR34">Sutton, R., (1988). Learning to predict by the method of temporal differences.<i>Machine Learning</i>, 3:9–44.</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 34 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Learning%20to%20predict%20by%20the%20method%20of%20temporal%20differences&amp;journal=Machine%20Learning&amp;volume=3&amp;pages=9-44&amp;publication_year=1988&amp;author=Sutton%2CR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Sutton, R., (1990). Integrated architectures for learning, planning, and reacting based on approximating dynam" /><p class="c-article-references__text" id="ref-CR35">Sutton, R., (1990). Integrated architectures for learning, planning, and reacting based on approximating dynamic programming. In<i>Proceedings of the Seventh International Conference on Machine Learning</i>, pages 216–224. Morgan Kaufmann.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Sutton, R., editor, (1992).Reinforcement Learning. Kluwer Academic Press. Special Issue of Machine Learning Jo" /><p class="c-article-references__text" id="ref-CR36">Sutton, R., editor, (1992).<i>Reinforcement Learning</i>. Kluwer Academic Press. Special Issue of Machine Learning Journal Vol 8, Nos 3–4, May 1992.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Tadepall, P. Personal Communication." /><p class="c-article-references__text" id="ref-CR37">Tadepall, P. Personal Communication.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Tadepalli, P. &amp; Ok, D., (1994). H learning: A reinforcement learning method to optimize undiscounted average r" /><p class="c-article-references__text" id="ref-CR38">Tadepalli, P. &amp; Ok, D., (1994). H learning: A reinforcement learning method to optimize undiscounted average reward. Technical Report 94-30-01, Oregon State Univ.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Tesauro, G., (1992). Practical issues in temporal difference learning. In R. Sutton, editor,Reinforcement Lear" /><p class="c-article-references__text" id="ref-CR39">Tesauro, G., (1992). Practical issues in temporal difference learning. In R. Sutton, editor,<i>Reinforcement Learning</i>. Kluwer Academic Publishers.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Thrun, S. The role of exploration in learning control. In D. A. White and D. A. Sofge, editors,Handbook of Int" /><p class="c-article-references__text" id="ref-CR40">Thrun, S. The role of exploration in learning control. In D. A. White and D. A. Sofge, editors,<i>Handbook of Intelligent Control: Neural, Fuzzy, and Adaptive Approaches</i>. Van Nostrand Reinhold.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="J.. Tsitsiklis, " /><meta itemprop="datePublished" content="1994" /><meta itemprop="headline" content="Tsitsiklis, J., (1994). Asynchronous stochastic approximation and Q-learning.Machine Learning, 16:185–202." /><p class="c-article-references__text" id="ref-CR41">Tsitsiklis, J., (1994). Asynchronous stochastic approximation and Q-learning.<i>Machine Learning</i>, 16:185–202.</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 41 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Asynchronous%20stochastic%20approximation%20and%20Q-learning&amp;journal=Machine%20Learning&amp;volume=16&amp;pages=185-202&amp;publication_year=1994&amp;author=Tsitsiklis%2CJ.">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="A.. Veinott, " /><meta itemprop="datePublished" content="1969" /><meta itemprop="headline" content="Veinott, A., (1969) Discrete dynamic programming with sensitive discount optimality criteria.Annals of Mathema" /><p class="c-article-references__text" id="ref-CR42">Veinott, A., (1969) Discrete dynamic programming with sensitive discount optimality criteria.<i>Annals of Mathematical Statistics</i>, 40(5):1635–1660.</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 42 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Discrete%20dynamic%20programming%20with%20sensitive%20discount%20optimality%20criteria&amp;journal=Annals%20of%20Mathematical%20Statistics&amp;volume=40&amp;issue=5&amp;pages=1635-1660&amp;publication_year=1969&amp;author=Veinott%2CA.">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Watkins, C., (1989).Learning from Delayed Rewards. PhD thesis, King's College, Cambridge, England." /><p class="c-article-references__text" id="ref-CR43">Watkins, C., (1989).<i>Learning from Delayed Rewards</i>. PhD thesis, King's College, Cambridge, England.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Wheeler, R. &amp; Narendra, K., (1986). Decentralized learning in finite Markov chains.IEEE Transactions on Automa" /><p class="c-article-references__text" id="ref-CR44">Wheeler, R. &amp; Narendra, K., (1986). Decentralized learning in finite Markov chains.<i>IEEE Transactions on Automatic Control</i>, AC-31(6)</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="D.. White, " /><meta itemprop="datePublished" content="1963" /><meta itemprop="headline" content="White, D., (1963). Dynamic programming, markov chains, and the method of successive approximationsJournal of M" /><p class="c-article-references__text" id="ref-CR45">White, D., (1963). Dynamic programming, markov chains, and the method of successive approximations<i>Journal of Mathematical Analysis and Applications</i>, 6:373–376.</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 45 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Dynamic%20programming%2C%20markov%20chains%2C%20and%20the%20method%20of%20successive%20approximations&amp;journal=Journal%20of%20Mathematical%20Analysis%20and%20Applications&amp;volume=6&amp;pages=373-376&amp;publication_year=1963&amp;author=White%2CD.">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Whitehead, S., Karlsson, J. &amp; Tenenberg, J., (1993). Learning multiple goal behavior via task decomposition an" /><p class="c-article-references__text" id="ref-CR46">Whitehead, S., Karlsson, J. &amp; Tenenberg, J., (1993). Learning multiple goal behavior via task decomposition and dynamic policy merging. In J. Connell and S. Mahadevan, editors,<i>Robot Learning</i>. Kluwer Academic Publishers.</p></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" href="/article/10.1007/BF00114727-references.ris">Download references<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p></div></div></div></section><section aria-labelledby="Ack1"><div class="c-article-section" id="Ack1-section"><div class="c-article-section__content" id="Ack1-content"></div></div></section><section aria-labelledby="author-information" data-title="Author information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><h3 class="c-article__sub-heading" id="affiliations">Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">Department of Computer Science and Engineering, University of South Florida, 33620, Tampa, Florida</p><p class="c-article-author-affiliation__authors-list">Sridhar Mahadevan</p></li></ol><div class="js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-Sridhar-Mahadevan"><span class="c-article-authors-search__title u-h3 js-search-name">Sridhar Mahadevan</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Sridhar+Mahadevan&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Sridhar+Mahadevan" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Sridhar+Mahadevan%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li></ol></div></div></div></section><section aria-labelledby="rightslink" data-title="Rights and permissions"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content"><p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=Average%20reward%20reinforcement%20learning%3A%20Foundations%2C%20algorithms%2C%20and%20empirical%20results&amp;author=Sridhar%20Mahadevan&amp;contentID=10.1007%2FBF00114727&amp;copyright=Kluwer%20Academic%20Publishers&amp;publication=0885-6125&amp;publicationDate=1996-03&amp;publisherName=SpringerNature&amp;orderBeanReset=true">Reprints and Permissions</a></p></div></div></section><section aria-labelledby="article-info" data-title="About this article"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Mahadevan, S. Average reward reinforcement learning: Foundations, algorithms, and empirical results.
                    <i>Mach Learn</i> <b>22, </b>159–195 (1996). https://doi.org/10.1007/BF00114727</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" href="/article/10.1007/BF00114727.ris">Download citation<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Received<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="1994-11-15">15 November 1994</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Accepted<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="1995-02-24">24 February 1995</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Issue Date<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="1996-03">March 1996</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--doi"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value"><a href="https://doi.org/10.1007/BF00114727" data-track="click" data-track-action="view doi" data-track-label="link" itemprop="sameAs">https://doi.org/10.1007/BF00114727</a></span></p></li></ul><div data-component="share-box"></div><h3 class="c-article__sub-heading">Keywords</h3><ul class="c-article-subject-list"><li class="c-article-subject-list__subject"><span itemprop="about">Reinforcement learning</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Markov decision processes</span></li></ul><div data-component="article-info-list"></div></div></div></div></div></section>
                </div>
            </article>
        </main>

        <div class="c-article-extras u-text-sm u-hide-print" id="sidebar" data-container-type="reading-companion" data-track-component="reading companion">
            <aside>
                <div data-test="download-article-link-wrapper">
                    
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/BF00114727.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button" download>
            
                <span>Download PDF</span>
                <svg aria-hidden="true" focusable="false" width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                </div>

                <div data-test="collections">
                    <div id="SpringerLinkArticleCollections">
    
</div>

                </div>

                <div data-test="editorial-summary">
                    
                </div>

                <div class="c-reading-companion">
                    <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky">
                        

                        <div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections">
                            <div class="js-ad">
    <aside class="c-ad c-ad--300x250">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-MPU1" data-gpt-unitpath="/270604982/springerlink/10994/article" data-gpt-sizes="300x250" data-gpt-targeting="pos=MPU1;articleid=BF00114727;"></div>
        </div>
    </aside>
</div>

                        </div>
                        <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures"></div>
                        <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references"></div>
                    </div>
                </div>
            </aside>
        </div>
    </div>


        
    <footer class="app-footer" role="contentinfo">
        <div class="app-footer__aside-wrapper u-hide-print">
            <div class="app-footer__container">
                <p class="app-footer__strapline">Over 10 million scientific documents at your fingertips</p>
                
                    <div class="app-footer__edition" data-component="SV.EditionSwitcher">
                        <span class="u-visually-hidden" data-role="button-dropdown__title" data-btn-text="Switch between Academic & Corporate Edition">Switch Edition</span>
                        <ul class="app-footer-edition-list" data-role="button-dropdown__content" data-test="footer-edition-switcher-list">
                            <li class="selected">
                                <a data-test="footer-academic-link"
                                   href="/siteEdition/link"
                                   id="siteedition-academic-link">Academic Edition</a>
                            </li>
                            <li>
                                <a data-test="footer-corporate-link"
                                   href="/siteEdition/rd"
                                   id="siteedition-corporate-link">Corporate Edition</a>
                            </li>
                        </ul>
                    </div>
                
            </div>
        </div>
        <div class="app-footer__container">
            <ul class="app-footer__nav u-hide-print">
                <li><a href="/">Home</a></li>
                <li><a href="/impressum">Impressum</a></li>
                <li><a href="/termsandconditions">Legal information</a></li>
                <li><a href="/privacystatement">Privacy statement</a></li>
                <li><a href="https://www.springernature.com/ccpa">California Privacy Statement</a></li>
                <li><a href="/cookiepolicy">How we use cookies</a></li>
                
                <li><a class="optanon-toggle-display" href="javascript:void(0);">Manage cookies/Do not sell my data</a></li>
                
                <li><a href="/accessibility">Accessibility</a></li>
                <li><a id="contactus-footer-link" href="/contactus">Contact us</a></li>
            </ul>
            <div class="c-user-metadata">
    
        <p class="c-user-metadata__item">
            <span data-test="footer-user-login-status">Not logged in</span>
            <span data-test="footer-user-ip"> - 79.55.238.234</span>
        </p>
        <p class="c-user-metadata__item" data-test="footer-business-partners">
            Not affiliated
        </p>

        
    
</div>

            <a class="app-footer__parent-logo" target="_blank" rel="noopener" href="//www.springernature.com"  title="Go to Springer Nature">
                <span class="u-visually-hidden">Springer Nature</span>
                <svg width="125" height="12" focusable="false" aria-hidden="true">
                    <image width="125" height="12" alt="Springer Nature logo"
                           src=/oscar-static/images/springerlink/png/springernature-60a72a849b.png
                           xmlns:xlink="http://www.w3.org/1999/xlink"
                           xlink:href=/oscar-static/images/springerlink/svg/springernature-ecf01c77dd.svg>
                    </image>
                </svg>
            </a>
            <p class="app-footer__copyright">&copy; 2020 Springer Nature Switzerland AG. Part of <a target="_blank" rel="noopener" href="//www.springernature.com">Springer Nature</a>.</p>
            
        </div>
        
    <svg class="u-hide hide">
        <symbol id="global-icon-chevron-right" viewBox="0 0 16 16">
            <path d="M7.782 7L5.3 4.518c-.393-.392-.4-1.022-.02-1.403a1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L7.782 9l1.013-.998z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-download" viewBox="0 0 16 16">
            <path d="M2 14c0-.556.449-1 1.002-1h9.996a.999.999 0 110 2H3.002A1.006 1.006 0 012 14zM9 2v6.8l2.482-2.482c.392-.392 1.022-.4 1.403-.02a1.001 1.001 0 010 1.417l-4.177 4.177a1.001 1.001 0 01-1.416 0L3.115 7.715a.991.991 0 01-.016-1.4 1 1 0 011.42.003L7 8.8V2c0-.55.444-.996 1-.996.552 0 1 .445 1 .996z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-email" viewBox="0 0 18 18">
            <path d="M1.995 2h14.01A2 2 0 0118 4.006v9.988A2 2 0 0116.005 16H1.995A2 2 0 010 13.994V4.006A2 2 0 011.995 2zM1 13.994A1 1 0 001.995 15h14.01A1 1 0 0017 13.994V4.006A1 1 0 0016.005 3H1.995A1 1 0 001 4.006zM9 11L2 7V5.557l7 4 7-4V7z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-institution" viewBox="0 0 18 18">
            <path d="M14 8a1 1 0 011 1v6h1.5a.5.5 0 01.5.5v.5h.5a.5.5 0 01.5.5V18H0v-1.5a.5.5 0 01.5-.5H1v-.5a.5.5 0 01.5-.5H3V9a1 1 0 112 0v6h8V9a1 1 0 011-1zM6 8l2 1v4l-2 1zm6 0v6l-2-1V9zM9.573.401l7.036 4.925A.92.92 0 0116.081 7H1.92a.92.92 0 01-.528-1.674L8.427.401a1 1 0 011.146 0zM9 2.441L5.345 5h7.31z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-search" viewBox="0 0 22 22">
            <path fill-rule="evenodd" d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z"/>
        </symbol>
        <symbol id="global-icon-info" viewBox="0 0 18 18">
            <path d="m9 0c4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9zm0 7h-1.5l-.11662113.00672773c-.49733868.05776511-.88337887.48043643-.88337887.99327227 0 .47338693.32893365.86994729.77070917.97358929l.1126697.01968298.11662113.00672773h.5v3h-.5l-.11662113.0067277c-.42082504.0488782-.76196299.3590206-.85696816.7639815l-.01968298.1126697-.00672773.1166211.00672773.1166211c.04887817.4208251.35902055.761963.76398144.8569682l.1126697.019683.11662113.0067277h3l.1166211-.0067277c.4973387-.0577651.8833789-.4804365.8833789-.9932723 0-.4733869-.3289337-.8699473-.7707092-.9735893l-.1126697-.019683-.1166211-.0067277h-.5v-4l-.00672773-.11662113c-.04887817-.42082504-.35902055-.76196299-.76398144-.85696816l-.1126697-.01968298zm0-3.25c-.69035594 0-1.25.55964406-1.25 1.25s.55964406 1.25 1.25 1.25 1.25-.55964406 1.25-1.25-.55964406-1.25-1.25-1.25z" fill-rule="evenodd"/>
        </symbol>
    </svg>

    </footer>



    </div>
    
    
</body>
</html>

