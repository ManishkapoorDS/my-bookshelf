abstract: Reservoir computers (RCs) and recurrent neural networks (RNNs) can mimic
  any finite-state automaton in theory, and some workers demonstrated that this can
  hold in practice. We test the capability of generalized linear models, RCs, and
  Long Short-Term Memory (LSTM) RNN architectures to predict the stochastic processes
  generated by a large suite of probabilistic deterministic finite-state automata
  (PDFA). PDFAs provide an excellent performance benchmark in that they can be systematically
  enumerated, the randomness and correlation structure of their generated processes
  are exactly known, and their optimal memory-limited predictors are easily computed.
  Unsurprisingly, LSTMs outperform RCs, which outperform generalized linear models.
  Surprisingly, each of these methods can fall short of the maximal predictive accuracy
  by as much as 50% after training and, when optimized, tend to fall short of the
  maximal predictive accuracy by ~5%, even though previously available methods achieve
  maximal predictive accuracy with orders-of-magnitude less data. Thus, despite the
  representational universality of RCs and RNNs, using them can engender a surprising
  predictive gap for simple stimuli. One concludes that there is an important and
  underappreciated role for methods that infer "causal states" or "predictive state
  representations".
archiveprefix: arXiv
author: Marzen, S. E. and Crutchfield, J. P.
author_list:
- family: Marzen
  given: S. E.
- family: Crutchfield
  given: J. P.
eprint: 1910.07663v1
file: 1910.07663v1.pdf
files:
- marzen-s.-e.-and-crutchfield-j.-p.probabilistic-deterministic-finite-automata-and-recurrent-networks-revisited2019.pdf
month: Oct
primaryclass: cs.LG
ref: 1910.07663v1
time-added: 2020-10-10-18:35:23
title: Probabilistic Deterministic Finite Automata and Recurrent Networks,   Revisited
type: article
url: http://arxiv.org/abs/1910.07663v1
year: '2019'
