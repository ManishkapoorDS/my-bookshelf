<!DOCTYPE html>
<html lang="en" class="grade-c">
<head>
    <title>Reinforcement learning in artificial and biological systems | Nature Machine Intelligence</title>
    
        
<link rel="preload" href=/static/fonts/Lora-Regular.8861b0072d.woff2 as="font" type="font/woff2" crossorigin>


<link rel="preconnect" href="https://code.jquery.com" crossorigin>

<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="viewport" content="width=device-width,initial-scale=1.0,maximum-scale=2.5,user-scalable=yes"/>


<script>
    (function(e){var t=e.documentElement,n=e.implementation;t.className+=' js';if(n&&n.hasFeature('http://www.w3.org/TR/SVG11/feature#Image','1.1')){t.className+=' svg'}})(document)
</script>

<script data-test="dataLayer">
    dataLayer = [{"content":{"category":{"contentType":"review article","legacy":{"webtrendsPrimaryArticleType":"reviews","webtrendsSubjectTerms":"computational-models;computational-neuroscience;neurology","webtrendsContentCategory":null,"webtrendsContentCollection":"One year anniversary collection;The brain","webtrendsContentGroup":"Nature Machine Intelligence","webtrendsContentGroupType":null,"webtrendsContentSubGroup":"Review Article"}},"article":{"doi":"10.1038/s42256-019-0025-4"},"attributes":{"cms":null,"deliveryPlatform":"oscar","copyright":{"open":false,"legacy":{"webtrendsLicenceType":null}}},"contentInfo":{"authors":["Emre O. Neftci","Bruno B. Averbeck"],"publishedAt":1551657600,"publishedAtString":"2019-03-04","title":"Reinforcement learning in artificial and biological systems","legacy":null,"publishedAtTime":null,"documentType":"aplusplus"},"journal":{"pcode":"natmachintell","title":"nature machine intelligence","volume":"1","issue":"3"},"authorization":{"status":true},"features":[{"name":"furtherReadingSection","present":true}],"collection":{"id":"adjbhaagce;jigfghaeje"}},"page":{"category":{"pageType":"article"},"attributes":{"template":"mosaic","featureFlags":[{"name":"ab_test_pandemic_bypass_paywall","active":false},{"name":"ab_use_nature_150_split_header","active":true}]},"search":null},"privacy":{},"version":"1.0.0","product":null,"session":null,"user":null,"backHalfContent":true}];
</script>




<script src="//cdn.cookielaw.org/consent/792a5a21-3d99-4432-8f70-957d389bf2b9.js" async=""></script>


<script id="js-position0">
    window.abTestSharedArticleRenderer = true;
    window.idpVerifyPrefix = 'https://verify.nature.com';
    window.ra21Host = 'https://wayf.springernature.com';
</script>


    <script>
        (function() {
            window.onerror = function(message, url, line, column, error) {
                if (window.Raven) {
                    window.Raven.captureException(error);
                } else {
                    var s = document.createElement('script');
                    s.src = 'https://cdn.ravenjs.com/3.27.0/raven.min.js';
                    s.crossorigin = 'anonymous';
                    s.defer = 'defer';
                    s.onload = function() {
                        window.Raven.config('https://bddad754e8264fc7b4d73878b697b599@sentry.i-ris.io/39', {
                            whitelistUrls: [
                                'www.nature.com/'
                            ]
                        }).install();
                        window.Raven.captureException(error);
                    };
                    document.body.appendChild(s);
                }
            };

            if (window.matchMedia && window.matchMedia('only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)').matches) {
                var index = 0;
                var scripts = [
                    {src: '/static/js/nature.2093dd0acc.js', test: 'nature-js'},
                    {src: '/static/js/nature-es6-bundle.bcfec66745.js', test: 'nature-es6-js'},
                    {src: '/static/js/shared-es6-bundle.076f9d1226.js', test: 'shared-es6-js'}
                    
                ];
                var initOnceWhenReady = function() {
                    if (window.initEs5 && window.initEs6 && !window.inited) {
                        setTimeout(window.initEs5, 0);
                        setTimeout(window.initEs6, 0);
                        window.inited = true;
                    }
                };
                var createScript = function(src, test, beforeLoad) {
                    var s = document.createElement('script');
                    s.id = 'js-position' + (index + 1);
                    s.setAttribute('data-test', test);
                    if (beforeLoad) {
                        s.defer = 'defer';
                        s.onload = function() {
                            if (document.readyState === 'complete' || (document.readyState !== 'loading' && !document.documentElement.doScroll)) {
                                initOnceWhenReady();
                            }
                        };
                    } else {
                        s.async = 'async';
                    }
                    s.src = src;
                    return s;
                };
                var insert = function(s) {
                    document.getElementById('js-position' + index).insertAdjacentElement('afterend', s);
                    ++index;
                };

                scripts.forEach(function (script) {
                    insert(createScript(script.src, script.test, true));
                });

                document.addEventListener('DOMContentLoaded', function() {
                    initOnceWhenReady();

                    var conditionalScripts = [
                        {match: 'div[data-pan-container]', src: '/static/js/pan-zoom.3afd0786ef.js', test: 'pan-zoom-js'},
                        {match: 'math,span.mathjax-tex', src: '/static/js/math.fbd3babae9.js', test: 'math-js'}
                    ];
                    conditionalScripts.filter(function(script) {
                        return !!document.querySelector(script.match);
                    }).forEach(function(script) {
                        insert(createScript(script.src, script.test));
                    });
                }, false);
            }
        })();
    </script>




<link rel="apple-touch-icon" sizes="180x180" href=/static/images/favicons/nature/apple-touch-icon.f39cb19454.png>
<link rel="icon" type="image/png" sizes="32x32" href=/static/images/favicons/nature/favicon-32x32.3fe59ece92.png>
<link rel="icon" type="image/png" sizes="16x16" href=/static/images/favicons/nature/favicon-16x16.951651ab72.png>
<link rel="manifest" href=/static/manifest.1a481c42b1.json>
<link rel="mask-icon" href=/static/images/favicons/nature/safari-pinned-tab.69bff48fe6.svg color="#000000">
<link rel="shortcut icon" href=/static/images/favicons/nature/favicon.62367f778b.ico>
<meta name="msapplication-TileColor" content="#000000">
<meta name="msapplication-config" content=/static/browserconfig.e35b3b052c.xml>
<meta name="theme-color" content="#000000">
<meta name="application-name" content="Nature">




    
     
        

    <style>html{font-family:sans-serif;line-height:1.15;height:100%;overflow-y:scroll;font-size:62.5%}article,aside,header,nav,section{display:block}h1{font-size:30px}a,sub,sup{vertical-align:baseline}a{background-color:transparent;text-decoration:none;color:#069}b{font-weight:bolder}sub,sup{font-size:75%;line-height:0;position:relative}sub{bottom:-.25em}sup{top:-.5em}img{border:0;max-width:100%;height:auto;vertical-align:middle}svg:not(:root){overflow:hidden}button,input{font-family:sans-serif;font-size:100%;line-height:1.15;overflow:visible}button{text-transform:none}[type=submit],button,html [type=button]{-webkit-appearance:button}[type=checkbox]{box-sizing:border-box;padding:0}[type=search]{-webkit-appearance:textfield;outline-offset:-2px}button{cursor:pointer;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;border-top-left-radius:0;border-top-right-radius:0;border-bottom-right-radius:0;border-bottom-left-radius:0}h2{font-size:24px}h3{font-size:17px}h4{font-size:14px}nav ul{list-style:none none}.c-article-identifiers,body{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif}body{min-height:100%;background-color:#eee;line-height:1.76;color:#222;background-position:initial initial;background-repeat:initial initial}.c-article-header{margin-bottom:40px}.c-article-identifiers{list-style:none;font-size:1.6rem;line-height:1.3;display:-webkit-flex;-webkit-flex-wrap:wrap;color:#6f6f6f;padding:0;margin:0 0 8px}.c-article-identifiers__item{border-right:1px solid #6f6f6f;margin-right:8px;padding-right:8px;list-style:none}.c-article-identifiers__item a{color:#069;text-decoration:none}.c-article-identifiers__item:last-child{margin-right:0;padding-right:0;border-right-width:0}.c-article-identifiers__open{color:#b74616}.c-article-title{font-size:2.4rem;line-height:1.25;margin-bottom:16px}@media only screen and (min-width:768px){.c-article-title{font-size:3rem;line-height:1.2}}.c-author-list{font-size:1.6rem;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;list-style:none;margin-bottom:0;padding:0;width:100%}.c-author-list__item{margin-left:0}.c-author-list__item,.c-author-list li{display:inline;padding-right:0}.c-author-list__item svg,.c-reading-companion__figure-full-link svg{margin-left:4px}.c-author-list__show-less{margin-left:8px}.c-author-list__show-more{margin-right:4px}.c-article-info-details{font-size:1.6rem;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;margin-bottom:8px;margin-top:16px}.c-article-info-details__cite-as{border-left:1px solid #6f6f6f;margin-left:8px;padding-left:8px}.c-article-metrics-bar{display:-webkit-flex;-webkit-flex-wrap:wrap;line-height:1.3;font-size:1.6rem;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif}.c-article-metrics-bar__item{-webkit-box-align:baseline;-webkit-align-items:baseline;border-right:1px solid #6f6f6f;margin-right:8px}.c-article-metrics-bar__item:last-child{border-right-width:0}.c-article-metrics-bar__count{font-weight:700;margin:0}.c-article-metrics-bar__label{color:#626262;font-weight:400;font-style:normal;margin:0 10px 0 5px}.c-article-metrics-bar__details{margin:0}.c-article-main-column{width:60.2%;margin-right:8.6%}@media only screen and (max-width:1023px){.c-article-main-column{width:100%;margin-right:0}}.c-article-section{font-family:Lora,Palatino,Times,Times New Roman,serif;clear:both}.c-article-section__title{border-bottom:2px solid #d5d5d5;font-size:2rem;line-height:1.3;padding-bottom:8px}@media only screen and (min-width:768px){.c-article-section__title{font-size:2.4rem;line-height:1.24}}.c-article-section__content{font-family:Lora,Palatino,Times,Times New Roman,serif;margin-bottom:40px}@media only screen and (max-width:1023px){.c-article-section__content{padding-left:0}}.c-article-authors-search{margin-top:0;margin-bottom:24px}.c-article-authors-search__item,.c-article-authors-search__title{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif}.c-article-authors-search__title{color:#626262;font-weight:700;margin:0;padding:0;font-size:1.7rem}.c-article-authors-search__item{font-size:1.6rem}.c-article-authors-search__text{margin:0}.c-article-share-box__no-sharelink-info{font-size:1.3rem;font-weight:700;padding-top:4px;margin-bottom:24px}.c-article-share-box__only-read-input{border:1px solid #d5d5d5;display:inline-block;margin-bottom:8px;font-size:1.4rem;font-weight:700;padding:8px 10px}.c-article-share-box__button--link-like{background-color:transparent;border:0;color:#069;cursor:pointer;font-size:1.4rem;margin-bottom:8px;margin-left:10px}.c-article-body{clear:both}.c-article-body a,.c-article-body p{word-wrap:break-word}.c-article-body a{word-break:break-word}.c-pdf-download{display:-webkit-flex;margin-bottom:24px;max-height:48px}@media only screen and (min-width:540px){.c-pdf-download{max-height:none}}@media only screen and (min-width:1024px){.c-pdf-download{max-height:48px}}.c-article-extras{width:31.2%;float:left}@media only screen and (max-width:1023px){.c-article-extras{display:none}}.c-pdf-download__link{display:-webkit-flex;-webkit-box-pack:justify;-webkit-justify-content:space-between;color:#fff;background-color:#069;border:1px solid #069;border-top-left-radius:2px;border-top-right-radius:2px;border-bottom-right-radius:2px;border-bottom-left-radius:2px;text-decoration:none;font-size:1.6rem;line-height:1.3;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;-webkit-box-flex:1;-webkit-flex:1 1 0px;padding:13px 24px;background-position:initial initial;background-repeat:initial initial}.c-reading-companion{clear:both}.c-reading-companion__sticky{max-width:582px}.c-reading-companion__scroll-pane{overflow-x:hidden;overflow-y:auto;margin:0 0 16px}.c-reading-companion__tabs{font-size:1.6rem;list-style:none;display:-webkit-flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-webkit-flex-flow:row nowrap;margin:0 0 8px;padding:0}.c-reading-companion__tabs>li{-webkit-box-flex:1;-webkit-flex-grow:1}.c-reading-companion__tab{color:#069;border:1px solid #d5d5d5;border-left-width:0;background-color:#eee;padding:8px 8px 8px 15px;text-align:left;font-size:1.6rem;width:100%}.c-reading-companion__tabs li:first-child .c-reading-companion__tab{border-left-width:1px}.c-reading-companion__tab--active{color:#222;background-color:#fff;border-bottom:1px solid #fff;font-weight:700}.c-reading-companion__figures-list,.c-reading-companion__references-list,.c-reading-companion__sections-list{list-style:none;padding:0}.c-reading-companion__references-list--numeric{list-style:decimal;padding:0 0 0 30px}.c-reading-companion__sections-list{margin:0 0 8px;min-height:50px}.c-reading-companion__section-item{font-size:1.6rem;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;padding:0}.c-reading-companion__section-item a{display:block;padding:8px 0 8px 16px;line-height:1em;overflow:hidden;white-space:nowrap;text-overflow:ellipsis;text-decoration:none}.c-reading-companion__figure-item{padding:16px 8px 16px 0;border-top:1px solid #d5d5d5;font-size:1.6rem}.c-reading-companion__figure-item:first-child{border-top-style:none;padding-top:8px}.c-reading-companion__reference-item{padding:8px 8px 8px 0;border-top:1px solid #d5d5d5;font-size:1.6rem}.c-reading-companion__reference-item:first-child{border-top-style:none}.c-reading-companion__reference-citation{margin:0;font-family:Lora,Palatino,Times,Times New Roman,serif}.c-reading-companion__reference-links{list-style:none;text-align:right;margin:8px 0 0;padding:0;font-weight:700;font-size:1.3rem}.c-reading-companion__reference-links>a{display:inline-block;padding-left:8px}.c-reading-companion__reference-links>a:first-child{display:inline-block;padding-left:0}.c-reading-companion__figure-title{font-weight:700;font-family:Lora,Palatino,Times,Times New Roman,serif;display:block;margin:0 0 8px}.c-reading-companion__figure-links{margin:8px 0 0;display:-webkit-flex;-webkit-box-pack:justify;-webkit-justify-content:space-between}.c-reading-companion__figure-links>a{display:inline-block}.c-reading-companion__panel{display:none;border-top:1px solid #d5d5d5;margin-top:-9px;padding-top:9px}.c-reading-companion__panel--active{display:block}.c-ad{display:none;padding:8px;text-align:center}@media only screen and (min-width:768px){.js .c-ad{display:block}}.c-ad--728x90{background-color:#ccc}.c-ad--728x90 .c-ad__inner{min-height:calc(1.5em + 94px)}.c-ad__label{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-weight:400;margin-bottom:4px;color:#333;line-height:1.5;font-size:1.4rem}.u-display-none{display:none}.js .u-js-hide,.u-hide{display:none;visibility:hidden}.u-visually-hidden{border:0;clip:rect(0 0 0 0);height:1px;margin:-100%;overflow:hidden;padding:0;width:1px;position:absolute!important}@media print{.u-hide-print{display:none}}.u-icon{fill:currentColor;-webkit-transform:translate(0);display:inline-block;vertical-align:text-top}.u-float-left{float:left}.u-list-reset{list-style:none;margin:0;padding:0}.u-h3{font-size:1.7rem}.c-article-section__title,.c-article-title{font-weight:700}@media only screen and (min-width:1024px){.c-pdf-button__container{display:none}}@media screen and (min-width:876px){.background-brand-secondary-pdf{display:none}}body,button,div,form,input{margin:0;padding:0}h1,h2,h3,h4,p{padding:0}body{font-size:1.8em}p{margin:0 0 28px}ol,ul{margin-top:0;margin-bottom:28px}h1{font-size:3rem;margin:0 0 24px;font-weight:400}h1,h2,h3{font-family:Lora,Palatino,Times,Times New Roman,serif}h2{font-size:2.4rem}h2,h3,h4{margin:0 0 8px;font-weight:400}h3{font-size:1.7rem}h4{font-size:1.4rem;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif}.box-sizing{box-sizing:border-box}.grid,.grid-ng{float:left;padding-left:0!important;padding-right:0!important}.container{margin:0}.pin-right{float:right}.pin-left{float:left}.position-absolute{position:absolute}.position-relative{position:relative}.position-top{top:0}.position-right{right:0}.block{display:block}.inline-block{display:inline-block}.inline-list>li{display:inline}.inline-group-top{vertical-align:top}.cleared:after,.cleared:before{content:" ";display:table}.clear,.cleared:after{clear:both}.hide,.js .js-hide{display:none;visibility:hidden}.visually-hidden{width:1px;height:1px;clip:rect(1px 1px 1px 1px);position:absolute!important}.hide-text{display:block;text-indent:-9999em;direction:ltr}.hide-overflow,.hide-text{overflow:hidden}.text-center{text-align:center}.lower{text-transform:lowercase}.nowrap{white-space:nowrap}.overflow-ellipsis{text-overflow:ellipsis}.unstyled{font-weight:400;font-style:normal}.search-btn{width:4rem;min-width:40px;height:4rem;min-height:40px;font-size:1.2rem;background-image:url("data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D'http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg'%20width%3D'16'%20height%3D'16'%20viewBox%3D'0%200%2016%2016'%3E%3Cpath%20fill%3D'%23fff'%20d%3D'M10.6%201.3c2.3%200%204.2%201.9%204.2%204.2s-1.9%204.2-4.2%204.2-4.2-2-4.2-4.3%201.9-4.1%204.2-4.1m0-1.3c-3%200-5.4%202.4-5.4%205.4s2.4%205.4%205.4%205.4%205.4-2.4%205.4-5.4-2.4-5.4-5.4-5.4zm-3.5%2011.1l-4.9%204.9-2.2-2.2%204.9-4.8%202.2%202.1z'%2F%3E%3C%2Fsvg%3E"),none;background-size:40%;background-color:#069;border:1px solid #069;color:transparent}@media not all{.search-btn{background-size:16px}}.content{max-width:1000px;margin:0 auto}.article-page{background-color:#fff;background-position:initial initial;background-repeat:initial initial}.z-index-100{z-index:100}.z-index-50{z-index:50}.z-index-1{z-index:1}.composite-layer{-webkit-transform:translateZ(0)}.grid{margin-right:3.2%}.grid-8{width:65.6%}.grid-12{width:100%}.grid-left-2{margin-left:17.2%}.grid-1of4{width:25%}.grid-1of2{width:50%}.standard-space-below{margin-bottom:28px}.serif{font-family:Lora,Palatino,Times,Times New Roman,serif}.sans-serif{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif}.strong{font-weight:700;-webkit-font-smoothing:antialiased}.contrast-text,.contrast-text a{color:#fff}.text-gray{color:#222}.text-orange{color:#cc4b14}.equalize-line-height{line-height:1em}.tighten-line-height{line-height:1.4}.text11{font-size:1.1rem}.text14{font-size:1.4rem}.border-gray,.border-gray-medium{border:0 solid #999}.border-gray-medium{border-color:#dadada}.border-all-1{border-width:1px}.border-bottom-1{border-bottom-width:1px}.border-bottom-2{border-bottom-width:2px}.kill-border{border-width:0}.background-white{background-color:#fff}.background-gray-dark{background-color:#333}.background-bluegray{background-color:#5a5a62}.ma0{margin:0}.mt1{margin-top:1px}.mt4{margin-top:4px}.mt6{margin-top:6px}.mr1{margin-right:1px}.mr20{margin-right:20px}.mb0{margin-bottom:0}.mb4{margin-bottom:4px}.mb6{margin-bottom:6px}.mb20{margin-bottom:20px}.mb40{margin-bottom:40px}.ml6{margin-left:6px}.ml20{margin-left:20px}.pa6{padding:6px}.pa10{padding:10px}.pa20{padding:20px}.pt10{padding-top:10px}.pt20{padding-top:20px}.pt30{padding-top:30px}.pr10{padding-right:10px}.pr15{padding-right:15px}.pr20{padding-right:20px}.pr40{padding-right:40px}.pb10{padding-bottom:10px}.pl15{padding-left:15px}.pl20{padding-left:20px}@media only screen and (max-width:75em){.mq1200-padded{padding-left:20px;padding-right:20px}}@media only screen and (max-width:54.688em){.mq875-ml0{margin-left:0}.mq875-mt20{margin-top:20px}.mq875-pa0{padding:0}}@media only screen and (max-width:40em){.mq640-pr5{padding-right:5px}.mq640-pb30{padding-bottom:30px}}.js .js-author-etal{display:none;visibility:hidden}@media only screen and (max-width:40em){.mq640-hide{display:none;visibility:hidden}}@media only screen and (max-width:30em){.js .js-mq480-show-inline{display:inline;visibility:visible}.js .js-smaller-author-etal{display:none;visibility:hidden}}.icon{background-repeat:no-repeat}.icon-right{background-position:100% 50%}.icon-above{padding-top:25px;min-width:25px;background-position:50% 0}.icon-center{background-position:50% 50%}.icon-rotate:after{content:"";position:absolute;top:50%;-webkit-transform:translateY(-50%);transition:.2s;-webkit-transition:.2s;background-position:0 0;background-repeat:no-repeat;right:0}.icon-login-25x25-white,.icon-search-25x25-white{background-image:url("data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D'http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg'%20width%3D'25'%20height%3D'25'%20viewBox%3D'138.481%2015.385%2025%2025'%20enable-background%3D'new%20138.481%2015.385%2025%2025'%3E%3Cpath%20fill%3D'%23fff'%20d%3D'M150.98%2024.757c2.604%200%204.687-2.083%204.687-4.686s-2.083-4.685-4.686-4.685c-2.603%200-4.685%202.083-4.685%204.686s2.083%204.687%204.686%204.687zm4.79%201.562c-2.498%200-2.394%204.998-4.79%204.998-2.395%200-2.29-5-4.79-5s-6.143%203.958-6.143%209.27c0%205.102%205.832%204.79%2010.934%204.79s10.935.416%2010.935-4.687c0-5.31-3.644-9.37-6.144-9.37z'%2F%3E%3C%2Fsvg%3E"),none;background-size:25px}.icon-search-25x25-white{background-image:url("data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D'http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg'%20width%3D'25'%20height%3D'25'%20viewBox%3D'235%2015%2025%2025'%20enable-background%3D'new%20235%2015%2025%2025'%3E%3Cg%20fill%3D'%23fff'%3E%3Cpath%20d%3D'M251.583%2017c3.583%200%206.583%203%206.583%206.583s-3%206.583-6.583%206.583-6.583-3-6.583-6.583S248%2017%20251.583%2017m0-2c-4.667%200-8.417%203.75-8.417%208.417s3.75%208.417%208.417%208.417S260%2028.167%20260%2023.417%20256.25%2015%20251.583%2015zM246.083%2032.333L238.417%2040%20235%2036.583l7.667-7.666z'%2F%3E%3C%2Fg%3E%3C%2Fsvg%3E"),none}.icon-ealert-25x25-white,.icon-submit-25x25-white{background-image:url("data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D'http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg'%20width%3D'25'%20height%3D'25'%20viewBox%3D'0%200%2025%2025'%3E%3Cpath%20fill%3D'%23fff'%20d%3D'M15.6%201.3v5.3h5.3v17h-17v-22.3h11.7m1.4-1.3h-14.3v25h19.6v-19.6h-5.4l.1-5.4zm-.6%201.3l4.6%204.6v17.7h-17v-22.3h12.4m.6-1.3h-14.3v25h19.6v-19.6l-5.3-5.4zM16.6%2013.5l-3.1-3.1-.8-.8c-.1-.1-.3-.1-.4%200l-.8.8-3.1%203.1-.7.7c-.1.1-.2.2-.2.3s.1.2.2.2h2.7c.1%200%20.2.1.2.2v3.3c0%20.2.2.4.4.4h3c.2%200%20.4-.2.4-.4v-3.3c0-.1.1-.2.2-.2h2.7c.1%200%20.2-.1.2-.2s-.1-.3-.2-.4c-.1-.1-.4-.3-.7-.6z'%2F%3E%3C%2Fsvg%3E"),none;background-size:25px}.icon-ealert-25x25-white{background-image:url("data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D'http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg'%20width%3D'25'%20height%3D'25'%20viewBox%3D'0%200%2025%2025'%3E%3Cpath%20fill%3D'%23fff'%20d%3D'M25%203.1h-25v18.8h25v-18.8zm-17.8%2012.8l-3.9%204.4c-.2.2-.5.3-.6.3-.2%200-.5-.2-.6-.3-.3-.3-.3-.9%200-1.4l3.9-4.4c.3-.3.9-.3%201.3%200%20.2.5.2%201.1-.1%201.4zm5.3-.9c-.2%200-.3%200-.5-.2l-9.8-8.9c-.5-.3-.5-.7-.2-1.1.3-.5.8-.5%201.1-.2l9.4%208.4%209.4-8.4c.3-.3.8-.3%201.1.2.3.3.3.9-.2%201.1l-9.7%208.9c-.1.2-.4.2-.6.2zm10.5%205.3c-.2.2-.5.3-.6.3s-.5-.2-.6-.3l-4.1-4.4c-.3-.3-.3-.9%200-1.4.3-.3.9-.3%201.3%200l4.1%204.4c.2.5.2%201.1-.1%201.4z'%2F%3E%3C%2Fsvg%3E"),none}.icon-arrow-right-6x10-white{background-image:url("data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D'http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg'%20width%3D'6'%20height%3D'10'%20viewBox%3D'-0.064%20-1.375%206%2010'%20enable-background%3D'new%20-0.064%20-1.375%206%2010'%3E%3Cpath%20fill%3D'%23fff'%20d%3D'M-.064.625l3.686%203-3.686%203v2l6-5-6-5v2z'%2F%3E%3C%2Fsvg%3E"),none;background-size:7px}.icon-rotate.menu-button-icon:after{margin-top:-1px}.header-tools-link{height:29px}.clean-list{padding:0;list-style:none}@media only screen and (max-width:54.688em){.mq875-grid-12{width:100%}.mq875-grid-4{width:31.2%}}@media only screen and (max-width:40em){.mq640-grid-12{width:100%}}.skiplink{width:1px;height:1px;display:block;overflow:hidden;position:absolute}.breadcrumbs{padding:0 15px 5px}.breadcrumbs ol{padding:10px 5px 5px;overflow:hidden;line-height:1.1em;white-space:nowrap;text-overflow:ellipsis}.small-header-icons{padding-left:7px;padding-right:7px;margin-top:14px;width:65px}@media only screen and (max-width:75em){.breadcrumbs{padding:0 15px 5px 0}}.header-logo-container{height:50px;padding:15px 20px}.nature-research-logo{width:319px;background-position:50% 50%;background-repeat:no-repeat}.small-header-side{right:0}@media only screen and (max-width:54.688em){.small-header-hide{position:absolute;width:1px;left:-9999em}}.header-logo,.header-logo-container{overflow:hidden}.header-logo img{max-height:50px;margin:0;padding:0}.header-logo-primary{display:inline;visibility:visible}.header-logo-secondary{display:none;visibility:hidden}.menu-button{position:absolute;top:15px;right:100%;padding:6px 10px;border:1px solid rgba(34,34,34,.74902);text-transform:uppercase;font-size:1.4rem;color:#222}.menu-button-icon{display:block;position:relative;padding-right:15px;text-align:right;background-position:100% 50%;background-repeat:no-repeat}.menu-button-icon:after{width:12px;height:7px;background-image:url("data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D'http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg'%20width%3D'12'%20height%3D'7'%20viewBox%3D'0%200%2012%207'%3E%3Cpath%20fill%3D'%23222'%20d%3D'M9.7%200L6%204.3%202.3%200H0l6%207%206-7H9.7z'%2F%3E%3C%2Fsvg%3E"),none;background-size:12px}.menu-button-label{padding-right:4px}.menu-button-icon,.menu-button-label{height:11px;line-height:11px}.menu-button-icon{min-width:45px}@media only screen and (max-width:75em){.small-header-side{right:20px}.menu-button{display:inline-block;float:none;vertical-align:top;position:static;margin:1px 20px 1px 0}.menu-button-icon{min-width:0;padding-right:12px}.menu-button-label{position:absolute;left:-9999em;width:1px}.header-logo-container{padding-left:0}.banner{padding-left:20px;padding-right:0}}@media only screen and (max-width:54.688em){.menu-button{margin:1px 20px 1px 0}.header-logo-container{padding:7px 0;height:25px}.header-logo{height:25px}.header-logo img{max-height:25px}.header-logo-primary{display:none;visibility:hidden}.header-logo-secondary{display:inline;visibility:visible}.small-header-icons{border-left:1px solid #8d8d78;margin-top:0;margin-right:0;padding:7px 16px;width:auto;background-color:#5a5a62}.small-header-icons:first-of-type{border-left-width:0}.small-header-main,.small-header-side{width:auto;margin:0}.small-header-side{float:right;position:static}.nature-research-logo{width:231px;margin-right:-20px}}@media only screen and (max-width:40em){.menu-button{margin-right:4px}.header-logo-primary{display:inline;visibility:visible}.header-logo-secondary{display:none;visibility:hidden}}.js .menu{position:absolute;overflow-y:auto;left:0;right:0;display:none;margin:0}@media only screen and (max-width:75em) and (min-width:54.74em){.banner{padding-right:20px}}.grade-c-hide{display:block}.grade-c-show{display:none;visibility:hidden}.grade-c-invisible{visibility:visible}.menu-button{float:none}.header-logo-container h1{color:#000;padding:0;line-height:0;background-position:0 0;background-repeat:initial initial}.container{max-width:none}</style>



        
        
            <link id="mustard" rel="stylesheet" href="/static/css/article.fe8dc0b343.css" media="print" onload="this.media='only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)';this.onload=null">
            <noscript>
                <link id="mustard" rel="stylesheet" type="text/css" href="/static/css/article.fe8dc0b343.css" media="only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)">
            </noscript>
        
        <link rel="stylesheet" type="text/css" href="/static/css/article-print.553b5f0277.css" media="print">
    



    
        
            <style>
                .header-logo img {
                    max-width: 280px;
                }

                 .background-brand-primary {
                      background-color: #819096;
                      color: #fff;
                 }

                 .background-brand-secondary {
                      background-color: #9aa9af;
                      color: #fff;
                 }

                 .background-brand-secondary-pdf {
                      background-color: #036599;
                      background-size: 22px auto;
                      background-position: 80% 50%;
                      color: #fff;
                 }

                 .background-brand-secondary-pdf a {
                      color: #fff;
                 }

                 .background-brand-tertiary {
                      background-color: #cbd6db;
                      color: #222;
                 }

                 .background-gradient-brand-primary {
                      background-image: linear-gradient(to right, #819096, #819096 50%, transparent 50%);
                 }

                 .background-gradient-brand-secondary-tertiary {
                      background-image: linear-gradient(to right, #9aa9af, #9aa9af 50%, #cbd6db 50%);
                 }

                 .background-gradient-brand-gray {
                      background-image: linear-gradient(to right, #e0e0e0, #e0e0e0 50%, transparent 50%);
                 }

                 .background-gradient-brand-gray-light {
                      background-image: linear-gradient(to right, #eee, #eee 50%, transparent 50%);
                 }

                 
                      .background-brand-primary a {
                          color: #fff;
                      }
                 

                 
                      .background-brand-secondary a {
                          color: #fff;
                      }
                 

                 

                 .nav-border {
                      border-bottom: 4px solid #9aa9af;
                 }

                 /* less than or equal to 875px */
                 @media only screen and (max-width: 54.688em) {
                      .banner {
                          background-color: #819096;
                      }

                      .nav-border .menu-cell li {
                          border-bottom: 1px solid #9aa9af;
                      }
                 }

                 .sticky-header .inner-banner,
                 .header-primary-color {
                      background-color: #819096;
                 }

                 
                     .menu-button,
                     .menu-button-clone,
                     .header-submit-button {
                         border: 1px solid #fff;
                         border: 1px solid rgba(255, 255, 255, 0.75);
                         color: #fff;
                     }
                     .header-top-bar {
                         border-bottom: 1px solid #fff;
                         border-bottom: 1px solid rgba(255, 255, 255, 0.75);
                     }

                     .icon-rotate.tools-menu-button-icon:after,
                     .menu-button-icon:after {
                          background-image: url("/static/images/icons/icon-arrow-down-12x7-white.e10aedb54f.svg"), none;
                     }

                     .nature-research-logo {
                          background-image: url("/static/images/product-logos/nature-research-logo-white.a9028dbd21.svg"), none;
                     }
                     .header-search-button {
                          padding-right: 25px;
                          background-image: url("/static/images/icons/icon-search-24x24-white.7b3ba09d04.svg"), none;
                     }
                 
             </style>
         
    


<script>(function(w,d){if(w.matchMedia && w.matchMedia(d.getElementById('mustard').media).matches){d.documentElement.className=d.documentElement.className.replace(/\s*grade-c/, "")}})(window,document)</script>


<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
        new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-NWDMT9Q');</script>




<meta name="robots" content="noarchive">
<meta name="access" content="Yes">
<meta name="WT.cg_s" content="Article"/>
<meta name="WT.z_bandiera_abtest" content="a"/>
<meta name="WT.page_categorisation" content="Article_HTML"/>

    <meta name="WT.template" content="oscar"/>
    <meta name="WT.z_cg_type" content="Nature Research Journals"/>
    <meta name="WT.cg_n" content="Nature Machine Intelligence"/>
    <meta name="dc.rights" content="©2020 Macmillan Publishers Limited. All Rights Reserved."/>
    <meta name="prism.issn" content="2522-5839"/>


<link rel="search" href="http://www.nature.com/search">
<link rel="search" href="http://www.nature.com/opensearch/opensearch.xml" type="application/opensearchdescription+xml" title="nature.com">
<link rel="search" href="http://www.nature.com/opensearch/request" type="application/sru+xml" title="nature.com">


    
    

    <meta name="journal_id" content="42256"/>

    <meta name="dc.title" content="Reinforcement learning in artificial and biological systems"/>

    <meta name="dc.source" content="Nature Machine Intelligence 2019 1:3"/>

    <meta name="dc.format" content="text/html"/>

    <meta name="dc.publisher" content="Nature Publishing Group"/>

    <meta name="dc.date" content="2019-03-04"/>

    <meta name="dc.type" content="ReviewPaper"/>

    <meta name="dc.language" content="En"/>

    <meta name="dc.copyright" content="2019 This is a U.S. government work and not under copyright protection in the U.S.; foreign copyright protection may apply"/>

    <meta name="dc.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="dc.description" content="There is and has been a fruitful flow of concepts and ideas between studies of learning in biological and artificial systems. Much early work that led to the development of reinforcement learning (RL) algorithms for artificial systems was inspired by learning rules first developed in biology by Bush and Mosteller, and Rescorla and Wagner. More recently, temporal-difference RL, developed for learning in artificial agents, has provided a foundational framework for interpreting the activity of dopamine neurons. In this Review, we describe state-of-the-art work on RL in biological and artificial agents. We focus on points of contact between these disciplines and identify areas where future research can benefit from information flow between these fields. Most work in biological systems has focused on simple learning problems, often embedded in dynamic environments where flexibility and ongoing learning are important, similar to real-world learning problems faced by biological systems. In contrast, most work in artificial agents has focused on learning a single complex problem in a static environment. Moving forward, work in each field will benefit from a flow of ideas that represent the strengths within each discipline. Research on reinforcement learning in artificial agents focuses on a single complex problem within a static environment. In biological agents, research focuses on simple learning problems embedded in flexible, dynamic environments. The authors review the literature on these topics and suggest areas of synergy between them."/>

    <meta name="prism.issn" content="2522-5839"/>

    <meta name="prism.publicationName" content="Nature Machine Intelligence"/>

    <meta name="prism.publicationDate" content="2019-03-04"/>

    <meta name="prism.volume" content="1"/>

    <meta name="prism.number" content="3"/>

    <meta name="prism.section" content="ReviewPaper"/>

    <meta name="prism.startingPage" content="133"/>

    <meta name="prism.endingPage" content="143"/>

    <meta name="prism.copyright" content="2019 This is a U.S. government work and not under copyright protection in the U.S.; foreign copyright protection may apply"/>

    <meta name="prism.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="prism.url" content="https://www.nature.com/articles/s42256-019-0025-4"/>

    <meta name="prism.doi" content="doi:10.1038/s42256-019-0025-4"/>

    <meta name="citation_pdf_url" content="https://www.nature.com/articles/s42256-019-0025-4.pdf"/>

    <meta name="citation_fulltext_html_url" content="https://www.nature.com/articles/s42256-019-0025-4"/>

    <meta name="citation_journal_title" content="Nature Machine Intelligence"/>

    <meta name="citation_journal_abbrev" content="Nat Mach Intell"/>

    <meta name="citation_publisher" content="Nature Publishing Group"/>

    <meta name="citation_issn" content="2522-5839"/>

    <meta name="citation_title" content="Reinforcement learning in artificial and biological systems"/>

    <meta name="citation_volume" content="1"/>

    <meta name="citation_issue" content="3"/>

    <meta name="citation_publication_date" content="2019/03"/>

    <meta name="citation_online_date" content="2019/03/04"/>

    <meta name="citation_firstpage" content="133"/>

    <meta name="citation_lastpage" content="143"/>

    <meta name="citation_article_type" content="Review Article"/>

    <meta name="citation_language" content="en"/>

    <meta name="dc.identifier" content="doi:10.1038/s42256-019-0025-4"/>

    <meta name="DOI" content="10.1038/s42256-019-0025-4"/>

    <meta name="citation_doi" content="10.1038/s42256-019-0025-4"/>

    <meta name="description" content="There is and has been a fruitful flow of concepts and ideas between studies of learning in biological and artificial systems. Much early work that led to the development of reinforcement learning (RL) algorithms for artificial systems was inspired by learning rules first developed in biology by Bush and Mosteller, and Rescorla and Wagner. More recently, temporal-difference RL, developed for learning in artificial agents, has provided a foundational framework for interpreting the activity of dopamine neurons. In this Review, we describe state-of-the-art work on RL in biological and artificial agents. We focus on points of contact between these disciplines and identify areas where future research can benefit from information flow between these fields. Most work in biological systems has focused on simple learning problems, often embedded in dynamic environments where flexibility and ongoing learning are important, similar to real-world learning problems faced by biological systems. In contrast, most work in artificial agents has focused on learning a single complex problem in a static environment. Moving forward, work in each field will benefit from a flow of ideas that represent the strengths within each discipline. Research on reinforcement learning in artificial agents focuses on a single complex problem within a static environment. In biological agents, research focuses on simple learning problems embedded in flexible, dynamic environments. The authors review the literature on these topics and suggest areas of synergy between them."/>

    <meta name="dc.creator" content="Emre O. Neftci"/>

    <meta name="dc.creator" content="Bruno B. Averbeck"/>

    <meta name="dc.subject" content="Computational models"/>

    <meta name="dc.subject" content="Computational neuroscience"/>

    <meta name="dc.subject" content="Neurology"/>

    <meta name="citation_reference" content="Sutton, R. S. &amp; Barto, A. G. Reinforcement Learning: An Introduction (MIT Press, Cambridge, 1998)."/>

    <meta name="citation_reference" content="citation_journal_title=Annu. Rev. Psychol.; citation_title=A review of theory in physiological psychology; citation_author=KH Pribram; citation_volume=11; citation_publication_date=1960; citation_pages=1-40; citation_doi=10.1146/annurev.ps.11.020160.000245; citation_id=CR2"/>

    <meta name="citation_reference" content="citation_journal_title=Nature; citation_title=From circuits to behaviour in the amygdala; citation_author=PH Janak, KM Tye; citation_volume=517; citation_publication_date=2015; citation_pages=284-292; citation_doi=10.1038/nature14188; citation_id=CR3"/>

    <meta name="citation_reference" content="citation_journal_title=Nature; citation_title=A circuit mechanism for differentiating positive and negative associations; citation_author=P Namburi; citation_volume=520; citation_publication_date=2015; citation_pages=675-678; citation_doi=10.1038/nature14366; citation_id=CR4"/>

    <meta name="citation_reference" content="citation_journal_title=Nature; citation_title=The primate amygdala represents the positive and negative value of visual stimuli during learning; citation_author=JJ Paton, MA Belova, SE Morrison, CD Salzman; citation_volume=439; citation_publication_date=2006; citation_pages=865-870; citation_doi=10.1038/nature04490; citation_id=CR5"/>

    <meta name="citation_reference" content="citation_journal_title=Nat. Neurosci.; citation_title=Mesolimbic dopamine signals the value of work; citation_author=AA Hamid; citation_volume=19; citation_publication_date=2016; citation_pages=117-126; citation_doi=10.1038/nn.4173; citation_id=CR6"/>

    <meta name="citation_reference" content="citation_journal_title=Neuron; citation_title=Amygdala and ventral striatum make distinct contributions to reinforcement learning; citation_author=VD Costa, O Dal Monte, DR Lucas, EA Murray, BB Averbeck; citation_volume=92; citation_publication_date=2016; citation_pages=505-517; citation_doi=10.1016/j.neuron.2016.09.025; citation_id=CR7"/>

    <meta name="citation_reference" content="Puterman, M. L. Markov Decision Processes: Discrete Stochastic Dynamic Programming (Wiley, New York, 1994)."/>

    <meta name="citation_reference" content="Bertsekas, D. P. Dynamic Programming and Optimal Control (Athena Scientific, Belmont, 1995)."/>

    <meta name="citation_reference" content="Vapnik, V. The Nature of Statistical Learning Theory (Springer, New York, 2013)."/>

    <meta name="citation_reference" content="Hessel, M. et al. Multi-task deep reinforcement learning with PopArt. Preprint at 
                    https://arxiv.org/abs/1809.04474
                    
                   (2018)."/>

    <meta name="citation_reference" content="citation_journal_title=Proc. Natl Acad. Sci. USA; citation_title=Overcoming catastrophic forgetting in neural networks; citation_author=J Kirkpatrick; citation_volume=114; citation_publication_date=2017; citation_pages=3521-3526; citation_doi=10.1073/pnas.1611835114; citation_id=CR12"/>

    <meta name="citation_reference" content="citation_journal_title=Nature; citation_title=Vector-based navigation using grid-like representations in artificial agents; citation_author=A Banino; citation_volume=557; citation_publication_date=2018; citation_pages=429-433; citation_doi=10.1038/s41586-018-0102-6; citation_id=CR13"/>

    <meta name="citation_reference" content="citation_journal_title=Nat. Neurosci.; citation_title=Prioritized memory access explains planning and hippocampal replay; citation_author=MG Mattar, ND Daw; citation_volume=21; citation_publication_date=2018; citation_pages=1609-1617; citation_doi=10.1038/s41593-018-0232-z; citation_id=CR14"/>

    <meta name="citation_reference" content="citation_journal_title=Psychol. Rev.; citation_title=The perceptron: a probabilistic model for information-storage and organization in the brain; citation_author=F Rosenblatt; citation_volume=65; citation_publication_date=1958; citation_pages=386-408; citation_doi=10.1037/h0042519; citation_id=CR15"/>

    <meta name="citation_reference" content="citation_journal_title=Science; citation_title=The &#8220;wake-sleep&#8221; algorithm for unsupervised neural networks; citation_author=GE Hinton, P Dayan, BJ Frey, RM Neal; citation_volume=268; citation_publication_date=1995; citation_pages=1158-1161; citation_doi=10.1126/science.7761831; citation_id=CR16"/>

    <meta name="citation_reference" content="Rescorla, R. A. &amp; Wagner, A. R. in Classical Conditioning II: Current Research and Theory (eds Black, A. H. &amp; Prokasy, W. F.) 64&#8211;99 (Appleton-Century-Crofts, New York, 1972)."/>

    <meta name="citation_reference" content="citation_journal_title=Science; citation_title=A neural substrate of prediction and reward; citation_author=W Schultz, P Dayan, PR Montague; citation_volume=275; citation_publication_date=1997; citation_pages=1593-1599; citation_doi=10.1126/science.275.5306.1593; citation_id=CR18"/>

    <meta name="citation_reference" content="citation_journal_title=J. Neurosci.; citation_title=A framework for mesencephalic dopamine systems based on predictive Hebbian learning; citation_author=PR Montague, P Dayan, TJ Sejnowski; citation_volume=16; citation_publication_date=1996; citation_pages=1936-1947; citation_doi=10.1523/JNEUROSCI.16-05-01936.1996; citation_id=CR19"/>

    <meta name="citation_reference" content="Houk, J. C., Adamas, J. L. &amp; Barto, A. G. in Models of Information Processing in the Basal Ganglia (eds Houk, J. C., Davis, J. L. &amp; Beiser, D. G.) 249&#8211;274 (MIT Press, Cambridge, 1995)."/>

    <meta name="citation_reference" content="citation_journal_title=J. Cogn. Neurosci.; citation_title=Dynamic dopamine modulation in the basal ganglia: a neurocomputational account of cognitive deficits in medicated and nonmedicated Parkinsonism; citation_author=MJ Frank; citation_volume=17; citation_publication_date=2005; citation_pages=51-72; citation_doi=10.1162/0898929052880093; citation_id=CR21"/>

    <meta name="citation_reference" content="citation_journal_title=J. Neurosci.; citation_title=Reward-related cortical inputs define a large striatal region in primates that interface with associative cortical connections, providing a substrate for incentive-based learning; citation_author=SN Haber, KS Kim, P Mailly, R Calzavara; citation_volume=26; citation_publication_date=2006; citation_pages=8368-8376; citation_doi=10.1523/JNEUROSCI.0271-06.2006; citation_id=CR22"/>

    <meta name="citation_reference" content="citation_journal_title=Prog. Neurobiol.; citation_title=The basal ganglia: focused selection and inhibition of competing motor programs; citation_author=JW Mink; citation_volume=50; citation_publication_date=1996; citation_pages=381-425; citation_doi=10.1016/S0301-0082(96)00042-1; citation_id=CR23"/>

    <meta name="citation_reference" content="citation_journal_title=Neuron; citation_title=Value representations in the primate striatum during matching behavior; citation_author=B Lau, PW Glimcher; citation_volume=58; citation_publication_date=2008; citation_pages=451-463; citation_doi=10.1016/j.neuron.2008.02.021; citation_id=CR24"/>

    <meta name="citation_reference" content="citation_journal_title=Science; citation_title=Dissociable roles of ventral and dorsal striatum in instrumental conditioning; citation_author=J O&#8217;Doherty; citation_volume=304; citation_publication_date=2004; citation_pages=452-454; citation_doi=10.1126/science.1094285; citation_id=CR25"/>

    <meta name="citation_reference" content="citation_journal_title=Nat. Neurosci.; citation_title=Motivational neural circuits underlying reinforcement learning; citation_author=BB Averbeck, VD Costa; citation_volume=20; citation_publication_date=2017; citation_pages=505-512; citation_doi=10.1038/nn.4506; citation_id=CR26"/>

    <meta name="citation_reference" content="citation_journal_title=Mach. Learn.; citation_title=Learning to predict by the methods of temporal differences; citation_author=RS Sutton; citation_volume=3; citation_publication_date=1988; citation_pages=9-44; citation_id=CR27"/>

    <meta name="citation_reference" content="Schultz, W. Dopamine reward prediction error coding. Dialog. Clin. Neurosci. 18, 23&#8211;32 (2016)."/>

    <meta name="citation_reference" content="citation_journal_title=Nat. Neurosci.; citation_title=A causal link between prediction errors, dopamine neurons and learning; citation_author=EE Steinberg; citation_volume=16; citation_publication_date=2013; citation_pages=966-973; citation_doi=10.1038/nn.3413; citation_id=CR29"/>

    <meta name="citation_reference" content="citation_journal_title=Nat. Neurosci.; citation_title=Dopamine neurons create Pavlovian conditioned stimuli with circuit-defined motivational properties; citation_author=BT Saunders, JM Richard, EB Margolis, PH Janak; citation_volume=21; citation_publication_date=2018; citation_pages=1072-1083; citation_doi=10.1038/s41593-018-0191-4; citation_id=CR30"/>

    <meta name="citation_reference" content="citation_journal_title=Nat. Neurosci.; citation_title=Dopamine transients are sufficient and necessary for acquisition of model-based associations; citation_author=MJ Sharpe; citation_volume=20; citation_publication_date=2017; citation_pages=735-742; citation_doi=10.1038/nn.4538; citation_id=CR31"/>

    <meta name="citation_reference" content="citation_journal_title=Nat. Neurosci.; citation_title=Activity in prefrontal cortex during dynamic selection of action sequences; citation_author=BB Averbeck, JW Sohn, D Lee; citation_volume=9; citation_publication_date=2006; citation_pages=276-282; citation_doi=10.1038/nn1634; citation_id=CR32"/>

    <meta name="citation_reference" content="citation_journal_title=Neuron; citation_title=Action selection and action value in frontal-striatal circuits; citation_author=M Seo, E Lee, BB Averbeck; citation_volume=74; citation_publication_date=2012; citation_pages=947-960; citation_doi=10.1016/j.neuron.2012.03.037; citation_id=CR33"/>

    <meta name="citation_reference" content="citation_journal_title=J. Neurosci.; citation_title=Injection of a dopamine type 2 receptor antagonist into the dorsal striatum disrupts choices driven by previous outcomes, but not perceptual inference; citation_author=E Lee, M Seo, O Dal Monte, BB Averbeck; citation_volume=35; citation_publication_date=2015; citation_pages=6298-6306; citation_doi=10.1523/JNEUROSCI.4561-14.2015; citation_id=CR34"/>

    <meta name="citation_reference" content="citation_journal_title=J. Neurosci.; citation_title=Estimates of projection overlap and zones of convergence within frontal-striatal circuits; citation_author=BB Averbeck, J Lehman, M Jacobson, SN Haber; citation_volume=34; citation_publication_date=2014; citation_pages=9497-9505; citation_doi=10.1523/JNEUROSCI.5806-12.2014; citation_id=CR35"/>

    <meta name="citation_reference" content="citation_journal_title=J. Neurosci.; citation_title=Effects of ventral striatum lesions on stimulus versus action based reinforcement learning; citation_author=KM Rothenhoefer; citation_volume=37; citation_publication_date=2017; citation_pages=6902-6914; citation_doi=10.1523/JNEUROSCI.0631-17.2017; citation_id=CR36"/>

    <meta name="citation_reference" content="citation_journal_title=J. Comp. Neurol.; citation_title=Comparison of hippocampal, amygdala, and perirhinal projections to the nucleus accumbens: combined anterograde and retrograde tracing study in the Macaque brain; citation_author=DP Friedman, JP Aggleton, RC Saunders; citation_volume=450; citation_publication_date=2002; citation_pages=345-365; citation_doi=10.1002/cne.10336; citation_id=CR37"/>

    <meta name="citation_reference" content="citation_journal_title=Annu. Rev. Neurosci.; citation_title=Parallel organization of functionally segregated circuits linking basal ganglia and cortex; citation_author=GE Alexander, MR DeLong, PL Strick; citation_volume=9; citation_publication_date=1986; citation_pages=357-381; citation_doi=10.1146/annurev.ne.09.030186.002041; citation_id=CR38"/>

    <meta name="citation_reference" content="Averbeck, B. B. Amygdala and ventral striatum population codes implement multiple learning rates for reinforcement learning. In IEEE Symposium Series on Computational Intelligence (IEEE, 2017)."/>

    <meta name="citation_reference" content="citation_journal_title=Neural Comput.; citation_title=Adaptive mixtures of local experts; citation_author=RA Jacobs, MI Jordan, SJ Nowlan, GE Hinton; citation_volume=3; citation_publication_date=1991; citation_pages=79-87; citation_doi=10.1162/neco.1991.3.1.79; citation_id=CR40"/>

    <meta name="citation_reference" content="citation_journal_title=Neural Comput.; citation_title=Optimal spike-timing-dependent plasticity for precise action potential firing in supervised learning; citation_author=JP Pfister, T Toyoizumi, D Barber, W Gerstner; citation_volume=18; citation_publication_date=2006; citation_pages=1318-1348; citation_doi=10.1162/neco.2006.18.6.1318; citation_id=CR41"/>

    <meta name="citation_reference" content="Benna, M. K. &amp; Fusi, S. Computational principles of biological memory. Preprint at 
                    https://arxiv.org/abs/1507.07580
                    
                   (2015)."/>

    <meta name="citation_reference" content="Lahiri, S. &amp; Ganguli, S. A memory frontier for complex synapses. In Advances in Neural Information Processing Systems Vol. 26 (eds Burges, C. J. C., Bottou, L., Welling, M., Ghahramani, Z. &amp; Weinberger, K. Q.) 1034&#8211;1042 (NIPS, 2013)."/>

    <meta name="citation_reference" content="Koutnik, J., Greff, K., Gomez, F. &amp; Schmidhuber, J. A clockwork RNN. Preprint at 
                    https://arxiv.org/abs/1402.3511
                    
                   (2014)."/>

    <meta name="citation_reference" content="Neil, D., M., P. &amp; Liu, S.-C. Phased LSTM: accelerating recurrent network training for long or event-based sequences. In Advances in Neural Information Processing Systems Vol. 29 (eds Lee, D. D., Sugiyama, M., Luxburg, U. V., Guyon, I. &amp; Garnett, R.) 3882&#8211;3890 (NIPS, 2016)."/>

    <meta name="citation_reference" content="citation_journal_title=Neural Comput.; citation_title=Long short-term memory; citation_author=S Hochreiter, J Schmidhuber; citation_volume=9; citation_publication_date=1997; citation_pages=1735-1780; citation_doi=10.1162/neco.1997.9.8.1735; citation_id=CR46"/>

    <meta name="citation_reference" content="citation_journal_title=Neural Comput.; citation_title=Making working memory work: a computational model of learning in the prefrontal cortex and basal ganglia; citation_author=RC O&#8217;Reilly, MJ Frank; citation_volume=18; citation_publication_date=2006; citation_pages=283-328; citation_doi=10.1162/089976606775093909; citation_id=CR47"/>

    <meta name="citation_reference" content="Bishop, C. M. Pattern Recognition and Machine Learning (Springer, New York, 2006)."/>

    <meta name="citation_reference" content="Bottou, L. &amp; LeCun, Y. Large scale online learning. In Advances in Neural Information Processing Systems Vol. 16 (eds Thrun, S., Saul, L. K. &amp; Sch&#246;lkopf, B.) (NIPS, 2004)."/>

    <meta name="citation_reference" content="McCloskey, M. &amp; Cohen, N. J. in Psychology of Learning and Motivation
                           : Advances in Research and Theory Vol. 24 (ed. Bower, G. H.) 109&#8211;165 (1989)."/>

    <meta name="citation_reference" content="citation_journal_title=Psychol. Rev.; citation_title=Why there are complementary learning systems in the hippocampus and neocortex: insights from the successes and failures of connectionist models of learning and memory; citation_author=JL McClelland, BL McNaughton, RC O&#8217;Reilly; citation_volume=102; citation_publication_date=1995; citation_pages=419-457; citation_doi=10.1037/0033-295X.102.3.419; citation_id=CR51"/>

    <meta name="citation_reference" content="citation_journal_title=Trends Cogn. Sci.; citation_title=What learning systems do intelligent agents need? Complementary learning systems theory updated; citation_author=D Kumaran, D Hassabis, JL McClelland; citation_volume=20; citation_publication_date=2016; citation_pages=512-534; citation_doi=10.1016/j.tics.2016.05.004; citation_id=CR52"/>

    <meta name="citation_reference" content="citation_journal_title=Nature; citation_title=Human-level control through deep reinforcement learning; citation_author=V Mnih; citation_volume=518; citation_publication_date=2015; citation_pages=529-533; citation_doi=10.1038/nature14236; citation_id=CR53"/>

    <meta name="citation_reference" content="citation_journal_title=Mach. Learn.; citation_title=Self-improving reactive agents based on reinforcement learning, planning and teaching; citation_author=LJ Lin; citation_volume=8; citation_publication_date=1992; citation_pages=293-321; citation_id=CR54"/>

    <meta name="citation_reference" content="Zenke, F., Poole, B. &amp; Ganguli, S. Continual learning through synaptic intelligence. Preprint at 
                    https://arxiv.org/abs/1703.04200
                    
                   (2017)."/>

    <meta name="citation_reference" content="Aljundi, R., Babiloni, F., Elhoseiny, M., Rohrbach, M. &amp; Tuytelaars, T. Memory aware synapses: learning what (not) to forget. Preprint at 
                    https://arxiv.org/abs/1711.09601
                    
                   (2017)."/>

    <meta name="citation_reference" content="citation_journal_title=Nat. Neurosci.; citation_title=Uncertainty-based competition between prefrontal and dorsolateral striatal systems for behavioral control; citation_author=ND Daw, Y Niv, P Dayan; citation_volume=8; citation_publication_date=2005; citation_pages=1704-1711; citation_doi=10.1038/nn1560; citation_id=CR57"/>

    <meta name="citation_reference" content="Costa, V. D., Tran, V. L., Turchi, J. &amp; Averbeck, B. B. Reversal learning and dopamine: a Bayesian perspective. J. Neurosci. 35, 2407&#8211;2416 (2015)."/>

    <meta name="citation_reference" content="citation_journal_title=Neuron; citation_title=Model-based influences on humans&#8217; choices and striatal prediction errors; citation_author=ND Daw, SJ Gershman, B Seymour, P Dayan, RJ Dolan; citation_volume=69; citation_publication_date=2011; citation_pages=1204-1215; citation_doi=10.1016/j.neuron.2011.02.027; citation_id=CR59"/>

    <meta name="citation_reference" content="citation_journal_title=Neuron; citation_title=States versus rewards: dissociable neural prediction error signals underlying model-based and model-free reinforcement learning; citation_author=J Glascher, N Daw, P Dayan, JP O&#8217;Doherty; citation_volume=66; citation_publication_date=2010; citation_pages=585-595; citation_doi=10.1016/j.neuron.2010.04.016; citation_id=CR60"/>

    <meta name="citation_reference" content="citation_journal_title=Curr. Opin. Neurobiol.; citation_title=The ubiquity of model-based reinforcement learning; citation_author=BB Doll, DA Simon, ND Daw; citation_volume=22; citation_publication_date=2012; citation_pages=1075-1081; citation_doi=10.1016/j.conb.2012.08.003; citation_id=CR61"/>

    <meta name="citation_reference" content="citation_journal_title=Neuron; citation_title=Dopamine enhances model-based over model-free choice behavior; citation_author=K Wunderlich, P Smittenaar, RJ Dolan; citation_volume=75; citation_publication_date=2012; citation_pages=418-424; citation_doi=10.1016/j.neuron.2012.03.042; citation_id=CR62"/>

    <meta name="citation_reference" content="citation_journal_title=Nat. Rev. Neurosci.; citation_title=The prefrontal cortex and cognitive control; citation_author=EK Miller; citation_volume=1; citation_publication_date=2000; citation_pages=59-65; citation_doi=10.1038/35036228; citation_id=CR63"/>

    <meta name="citation_reference" content="citation_journal_title=Neuropharmacology; citation_title=Goal-directed instrumental action: contingency and incentive learning and their cortical substrates; citation_author=BW Balleine, A Dickinson; citation_volume=37; citation_publication_date=1998; citation_pages=407-419; citation_doi=10.1016/S0028-3908(98)00033-1; citation_id=CR64"/>

    <meta name="citation_reference" content="citation_journal_title=Proc. Natl Acad. Sci. USA; citation_title=Ventral striatal dopamine reflects behavioral and neural signatures of model-based control during sequential decision making; citation_author=L Deserno; citation_volume=112; citation_publication_date=2015; citation_pages=1595-1600; citation_doi=10.1073/pnas.1417219112; citation_id=CR65"/>

    <meta name="citation_reference" content="citation_journal_title=Psychol. Rev.; citation_title=The formation of learning sets; citation_author=HF Harlow; citation_volume=56; citation_publication_date=1949; citation_pages=51-65; citation_doi=10.1037/h0062474; citation_id=CR66"/>

    <meta name="citation_reference" content="citation_journal_title=Exp. Brain Res.; citation_title=Perseverative interference in monkeys following selective lesions of the inferior prefrontal convexity; citation_author=SD Iversen, M Mishkin; citation_volume=11; citation_publication_date=1970; citation_pages=376-386; citation_doi=10.1007/BF00237911; citation_id=CR67"/>

    <meta name="citation_reference" content="citation_journal_title=J. Neurosci.; citation_title=The role of frontal cortical and medial-temporal lobe brain areas in learning a Bayesian prior belief on reversals; citation_author=AI Jang; citation_volume=35; citation_publication_date=2015; citation_pages=11751-11760; citation_doi=10.1523/JNEUROSCI.1594-15.2015; citation_id=CR68"/>

    <meta name="citation_reference" content="citation_journal_title=Nat. Neurosci.; citation_title=Prefrontal cortex as a meta-reinforcement learning system; citation_author=JX Wang; citation_volume=21; citation_publication_date=2018; citation_pages=860-868; citation_doi=10.1038/s41593-018-0147-8; citation_id=CR69"/>

    <meta name="citation_reference" content="citation_journal_title=Neuron; citation_title=Orbitofrontal cortex as a cognitive map of task space; citation_author=RC Wilson, YK Takahashi, G Schoenbaum, Y Niv; citation_volume=81; citation_publication_date=2014; citation_pages=267-279; citation_doi=10.1016/j.neuron.2013.11.005; citation_id=CR70"/>

    <meta name="citation_reference" content="citation_journal_title=Neuron; citation_title=Human orbitofrontal cortex represents a cognitive map of state space; citation_author=NW Schuck, MB Cai, RC Wilson, Y Niv; citation_volume=91; citation_publication_date=2016; citation_pages=1402-1412; citation_doi=10.1016/j.neuron.2016.08.019; citation_id=CR71"/>

    <meta name="citation_reference" content="DeGroot, M. H. Optimal Statistical Decisions (Wiley, Hoboken, 1970)."/>

    <meta name="citation_reference" content="citation_journal_title=Nat. Neurosci.; citation_title=Dopamine reward prediction errors reflect hidden-state inference across time; citation_author=CK Starkweather, BM Babayan, N Uchida, SJ Gershman; citation_volume=20; citation_publication_date=2017; citation_pages=581-589; citation_doi=10.1038/nn.4520; citation_id=CR73"/>

    <meta name="citation_reference" content="citation_journal_title=Neuron; citation_title=The medial prefrontal cortex shapes dopamine reward prediction errors under state uncertainty; citation_author=CK Starkweather, SJ Gershman, N Uchida; citation_volume=98; citation_publication_date=2018; citation_pages=616-629; citation_doi=10.1016/j.neuron.2018.03.036; citation_id=CR74"/>

    <meta name="citation_reference" content="citation_journal_title=J. Neurosci.; citation_title=Synaptic basis of cortical persistent activity: the importance of NMDA receptors to working memory; citation_author=XJ Wang; citation_volume=19; citation_publication_date=1999; citation_pages=9587-9603; citation_doi=10.1523/JNEUROSCI.19-21-09587.1999; citation_id=CR75"/>

    <meta name="citation_reference" content="Sch&#246;ner, G. in The Cambridge Handbook of Computational Psychology (ed. Sun, R.) 101&#8211;126 (Cambridge Univ. Press, Cambridge, 2008)."/>

    <meta name="citation_reference" content="Averbeck, B. B. Theory of choice in bandit, information sampling and foraging tasks. PLoS Comput. Biol. 11, e1004164 (2015)."/>

    <meta name="citation_reference" content="citation_journal_title=Proc. Natl Acad. Sci. USA; citation_title=Working-memory capacity protects model-based learning from stress; citation_author=AR Otto, CM Raio, A Chiang, EA Phelps, ND Daw; citation_volume=110; citation_publication_date=2013; citation_pages=20941-20946; citation_doi=10.1073/pnas.1312011110; citation_id=CR78"/>

    <meta name="citation_reference" content="citation_journal_title=PLoS. Comput. Biol.; citation_title=Simple plans or sophisticated habits? State, transition and learning interactions in the two-step task; citation_author=T Akam, R Costa, P Dayan; citation_volume=11; citation_publication_date=2015; citation_pages=e1004648; citation_doi=10.1371/journal.pcbi.1004648; citation_id=CR79"/>

    <meta name="citation_reference" content="citation_journal_title=Nat. Neurosci.; citation_title=Hierarchical models of object recognition in cortex; citation_author=M Riesenhuber, T Poggio; citation_volume=2; citation_publication_date=1999; citation_pages=1019-1025; citation_doi=10.1038/14819; citation_id=CR80"/>

    <meta name="citation_reference" content="citation_journal_title=Mach. Learn.; citation_title=Simple statistical gradient-following algorithms for connectionist reinforcement learning; citation_author=RJ Williams; citation_volume=8; citation_publication_date=1992; citation_pages=229-256; citation_id=CR81"/>

    <meta name="citation_reference" content="citation_journal_title=Commun. ACM; citation_title=Temporal difference learning and TD-Gammon; citation_author=G Tesauro; citation_volume=38; citation_publication_date=1995; citation_pages=58-68; citation_doi=10.1145/203330.203343; citation_id=CR82"/>

    <meta name="citation_reference" content="Pomerleau, D. A. ALVINN: An autonomous land vehicle in a neural network. In Advances in Neural Information Processing Systems Vol. 1 (ed. Touretzky, D. S.) (NIPS, 1988)."/>

    <meta name="citation_reference" content="citation_journal_title=J. Mach. Learn. Res.; citation_title=End-to-end training of deep visuomotor policies; citation_author=S Levine, C Finn, T Darrell, P Abbeel; citation_volume=17; citation_publication_date=2016; citation_pages=1334-1373; citation_id=CR84"/>

    <meta name="citation_reference" content="Gu, S., Lillicrap, T., Sutskever, I. &amp; Levine, S. Continuous deep Q-learning with model-based acceleration. In Proc. 33rd International Conference on Machine Learning Vol. 48 2829&#8211;2838 (PMLR, 2016)."/>

    <meta name="citation_reference" content="Burda, Y., Edwards, H., Storkey, A. &amp; Klimov, O. Exploration by random network distillation. Preprint at 
                    https://arxiv.org/abs/1810.12894
                    
                   (2018)."/>

    <meta name="citation_reference" content="Vezhnevets, A. S. et al. Feudal networks for hierarchical reinforcement learning. Preprint at 
                    https://arxiv.org/abs/1703.01161
                    
                   (2017)."/>

    <meta name="citation_reference" content="citation_journal_title=iScience; citation_title=Data and power efficient intelligence with neuromorphic learning machines; citation_author=EO Neftci; citation_volume=5; citation_publication_date=2018; citation_pages=52-68; citation_doi=10.1016/j.isci.2018.06.010; citation_id=CR88"/>

    <meta name="citation_reference" content="Kaiser, J., Mostafa, H. &amp; Neftci, E. O. Synaptic plasticity dynamics for deep continuous local learning. Preprint at 
                    https://arxiv.org/abs/1811.10766
                    
                   (2018)."/>

    <meta name="citation_reference" content="citation_journal_title=Front. Neurosci.; citation_title=Event-driven random back-propagation: enabling neuromorphic deep learning machines; citation_author=EO Neftci, C Augustine, S Paul, G Detorakis; citation_volume=11; citation_publication_date=2017; citation_pages=324; citation_doi=10.3389/fnins.2017.00324; citation_id=CR90"/>

    <meta name="citation_reference" content="citation_journal_title=Nat. Commun.; citation_title=Random synaptic feedback weights support error backpropagation for deep learning; citation_author=TP Lillicrap, D Cownden, DB Tweed, CJ Akerman; citation_volume=7; citation_publication_date=2016; citation_doi=10.1038/ncomms13276; citation_id=CR91"/>

    <meta name="citation_reference" content="Levine, S., Pastor, P., Krizhevsky, A., Ibarz, J. &amp; Quillen, D. Learning hand-eye coordination for robotic grasping with deep learning and large-scale data collection. Int. J. Robot. Res. 37, 421&#8211;436 (2018)."/>

    <meta name="citation_reference" content="Blundell, C. et al. Model-free episodic control. Preprint at 
                    https://arxiv.org/abs/1606.04460
                    
                   (2016)."/>

    <meta name="citation_reference" content="citation_journal_title=Ann. Rev. Psychol.; citation_title=Reinforcement learning and episodic memory in humans and animals: an integrative framework; citation_author=SJ Gershman, ND Daw; citation_volume=68; citation_publication_date=2017; citation_pages=101-128; citation_doi=10.1146/annurev-psych-122414-033625; citation_id=CR94"/>

    <meta name="citation_reference" content="Finn, C., Abbeel, P. &amp; Levine, S. Model-agnostic meta-learning for fast adaptation of deep networks. Preprint at 
                    https://arxiv.org/abs/1703.03400
                    
                   (2017)."/>

    <meta name="citation_reference" content="Ha, D. &amp; Schmidhuber, J. World models. Preprint at 
                    https://arxiv.org/abs/1803.10122
                    
                   (2018)."/>

    <meta name="citation_reference" content="Zambaldi, V. et al. Relational deep reinforcement learning. Preprint at 
                    https://arxiv.org/abs/1806.01830
                    
                   (2018)."/>

    <meta name="citation_reference" content="Daw, N. D., O&#8217;Doherty, J. P., Dayan, P., Seymour, B. &amp; Dolan, R. J. Cortical substrates for exploratory decisions in humans. Nature 441, 876&#8211;879 (2006)."/>

    <meta name="citation_reference" content="Kriegeskorte, N. Deep neural networks: a new framework for modeling biological vision and brain information processing. Annu. Rev. Vision Sci. 1, 417&#8211;446 (2015)."/>

    <meta name="citation_reference" content="Bernacchia, A., Seo, H., Lee, D. &amp; Wang, X. J. A reservoir of time constants for memory traces in cortical neurons. Nat. Neurosci. 14, 366&#8211;372 (2011)."/>

    <meta name="citation_reference" content="Walton, M. E., Behrens, T. E., Buckley, M. J., Rudebeck, P. H. &amp; Rushworth, M. F. Separable learning systems in the macaque brain and the role of orbitofrontal cortex in contingent learning. Neuron 65, 927&#8211;939 (2010)."/>

    <meta name="citation_reference" content="citation_journal_title=Neuron; citation_title=Hierarchical prediction errors in midbrain and basal forebrain during sensory learning; citation_author=S Iglesias; citation_volume=80; citation_publication_date=2013; citation_pages=519-530; citation_doi=10.1016/j.neuron.2013.09.009; citation_id=CR102"/>

    <meta name="citation_reference" content="citation_journal_title=Cereb. Cortex; citation_title=Mechanisms of hierarchical reinforcement learning in cortico-striatal circuits 2: evidence from fMRI; citation_author=D Badre, MJ Frank; citation_volume=22; citation_publication_date=2012; citation_pages=527-536; citation_doi=10.1093/cercor/bhr117; citation_id=CR103"/>

    <meta name="citation_reference" content="citation_journal_title=Cereb. Cortex; citation_title=Mechanisms of hierarchical reinforcement learning in corticostriatal circuits 1: computational analysis; citation_author=MJ Frank, D Badre; citation_volume=22; citation_publication_date=2012; citation_pages=509-526; citation_doi=10.1093/cercor/bhr114; citation_id=CR104"/>

    <meta name="citation_reference" content="citation_journal_title=Trends Cogn. Sci.; citation_title=Hierarchical models of behavior and prefrontal function; citation_author=MM Botvinick; citation_volume=12; citation_publication_date=2008; citation_pages=201-208; citation_doi=10.1016/j.tics.2008.02.009; citation_id=CR105"/>

    <meta name="citation_reference" content="citation_journal_title=Cognition; citation_title=Hierarchically organized behavior and its neural foundations: a reinforcement learning perspective; citation_author=MM Botvinick, Y Niv, AC Barto; citation_volume=113; citation_publication_date=2009; citation_pages=262-280; citation_doi=10.1016/j.cognition.2008.08.011; citation_id=CR106"/>

    <meta name="citation_reference" content="citation_journal_title=Neuron; citation_title=A neural signature of hierarchical reinforcement learning; citation_author=JJ Ribas-Fernandes; citation_volume=71; citation_publication_date=2011; citation_pages=370-379; citation_doi=10.1016/j.neuron.2011.05.042; citation_id=CR107"/>

    <meta name="citation_reference" content="citation_journal_title=Curr. Opin. Neurobiol.; citation_title=Hierarchical reinforcement learning and decision making; citation_author=MM Botvinick; citation_volume=22; citation_publication_date=2012; citation_pages=956-962; citation_doi=10.1016/j.conb.2012.05.008; citation_id=CR108"/>

    <meta name="citation_reference" content="citation_journal_title=Philos. Trans. R. Soc. Lond. B; citation_title=Model-based hierarchical reinforcement learning and human action control; citation_author=M Botvinick, A Weinstein; citation_volume=369; citation_publication_date=2014; citation_pages=20130480; citation_doi=10.1098/rstb.2013.0480; citation_id=CR109"/>

    <meta name="citation_reference" content="Dayan, P. &amp; Hinton, G. E. Feudal reinforcement learning. In Advances in Neural Information Processing Systems Vol. 5 (eds Hanson, S. J., Cowan, J. D. &amp; Giles, C. L.) 271&#8211;278 (NIPS, 1992)."/>

    <meta name="citation_reference" content="citation_journal_title=Artif. Intell.; citation_title=Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning; citation_author=RS Sutton, D Precup, S Singh; citation_volume=112; citation_publication_date=1999; citation_pages=181-211; citation_doi=10.1016/S0004-3702(99)00052-1; citation_id=CR111"/>

    <meta name="citation_reference" content="Bacon, P. L., Harb, J. &amp; Precup, D. The option-critic architecture. Proc. Thirty-First AAAI Conference on Artificial Intelligence 1726&#8211;1734 (AAAI, 2017)."/>

    <meta name="citation_reference" content="Jaderberg, M. et al. Reinforcement learning with unsupervised auxiliary tasks. Preprint at 
                    https://arxiv.org/abs/1611.05397
                    
                   (2016)."/>

    <meta name="citation_reference" content="Friston, K. The free-energy principle: a unified brain theory? Nat. Rev. Neurosci. 11, 127&#8211;138 (2010)."/>

    <meta name="citation_reference" content="Ross, S., Gordon, G. J. &amp; Bagnell, J. A. A reduction of imitation learning and structured prediction to no-regret online learning. Preprint at 
                    https://arxiv.org/abs/1011.0686
                    
                   (2010)."/>

    <meta name="citation_reference" content="Le, H. M. et al. Hierarchical imitation and reinforcement learning. Preprint at 
                    https://arxiv.org/abs/1803.00590
                    
                   (2018)."/>

    <meta name="citation_reference" content="citation_journal_title=Science; citation_title=The architecture of cognitive control in the human prefrontal cortex; citation_author=E Koechlin, C Ody, F Kouneiher; citation_volume=302; citation_publication_date=2003; citation_pages=1181-1185; citation_doi=10.1126/science.1088545; citation_id=CR117"/>

    <meta name="citation_reference" content="citation_journal_title=J. Cogn. Neurosci.; citation_title=Functional magnetic resonance imaging evidence for a hierarchical organization of the prefrontal cortex; citation_author=D Badre, M D&#8217;Esposito; citation_volume=19; citation_publication_date=2007; citation_pages=2082-2099; citation_doi=10.1162/jocn.2007.19.12.2082; citation_id=CR118"/>

    <meta name="citation_reference" content="citation_journal_title=J. Neurophysiol.; citation_title=Dissociable roles of preSMA in motor sequence chunking and hand switching&#8212;a TMS study; citation_author=D Muessgens, N Thirugnanasambandam, H Shitara, T Popa, M Hallett; citation_volume=116; citation_publication_date=2016; citation_pages=2637-2646; citation_doi=10.1152/jn.00565.2016; citation_id=CR119"/>

    <meta name="citation_reference" content="Sabour, S., Frosst, N. &amp; Hinton, G. E. Dynamic routing between capsules. In Advances in Neural Information Processing Systems Vol. 30 (eds Guyon, I. et al.) 3856&#8211;3866 (2017)."/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Micro; citation_title=Loihi: a neuromorphic manycore processor with on-chip learning; citation_author=M Davies; citation_volume=38; citation_publication_date=2018; citation_pages=82-99; citation_doi=10.1109/MM.2018.112130359; citation_id=CR121"/>

    <meta name="citation_reference" content="Friedmann, S. &amp; Schemmel, J. Demonstrating hybrid learning in a flexible neuromorphic hardware system. Preprint at 
                    https://arxiv.org/abs/1604.05080
                    
                   (2016)."/>

    <meta name="citation_reference" content="Qiao, N. et al. A reconfigurable on-line learning spiking neuromorphic processor comprising 256 neurons and 128K synapses. Front. Neurosci. 9, 141 (2015)."/>

    <meta name="citation_reference" content="citation_journal_title=Proc. Natl Acad. Sci. USA; citation_title=Synthesizing cognition in neuromorphic electronic systems; citation_author=E Neftci; citation_volume=110; citation_publication_date=2013; citation_pages=3468-3476; citation_doi=10.1073/pnas.1212083110; citation_id=CR124"/>

    <meta name="citation_reference" content="citation_journal_title=Front. Neurosci.; citation_title=Reward-based learning under hardware constraints-using a RISC processor embedded in a neuromorphic substrate; citation_author=S Friedmann, N Fremaux, J Schemmel, W Gerstner, K Meier; citation_volume=7; citation_publication_date=2013; citation_pages=160; citation_doi=10.3389/fnins.2013.00160; citation_id=CR125"/>

    <meta name="citation_reference" content="citation_journal_title=Neuron; citation_title=Neuroscience-inspired artificial intelligence; citation_author=D Hassabis, D Kumaran, C Summerfield, M Botvinick; citation_volume=95; citation_publication_date=2017; citation_pages=245-258; citation_doi=10.1016/j.neuron.2017.06.011; citation_id=CR126"/>

    <meta name="citation_reference" content="Courbariaux, M., Bengio, Y. &amp; David, J.-P. Training deep neural networks with low precision multiplications. Preprint at 
                    https://arxiv.org/abs/1412.7024
                    
                   (2014)."/>

    <meta name="citation_reference" content="citation_journal_title=Front. Neurosci.; citation_title=Neural and synaptic array transceiver: a brain-inspired computing framework for embedded learning; citation_author=G Detorakis; citation_volume=12; citation_publication_date=2018; citation_pages=583; citation_doi=10.3389/fnins.2018.00583; citation_id=CR128"/>

    <meta name="citation_reference" content="citation_journal_title=Curr. Opin. Neurobiol.; citation_title=Neuromorphic sensory systems; citation_author=SC Liu, T Delbruck; citation_volume=20; citation_publication_date=2010; citation_pages=288-295; citation_doi=10.1016/j.conb.2010.03.007; citation_id=CR129"/>

    <meta name="citation_author" content="Emre O. Neftci"/>

    <meta name="citation_author_institution" content="Department of Cognitive Sciences, Department of Computer Science, University of California Irvine, Irvine, USA"/>

    <meta name="citation_author" content="Bruno B. Averbeck"/>

    <meta name="citation_author_institution" content="Laboratory of Neuropsychology, National Institute of Mental Health, National Institutes of Health, Bethesda, USA"/>

    <meta name="access_endpoint" content="https://www.nature.com/platform/readcube-access"/>

    <meta name="twitter:card" content="summary"/>

    <meta name="twitter:title" content="Reinforcement learning in artificial and biological systems"/>

    <meta name="twitter:site" content="@NatMachIntell"/>

    <meta name="twitter:description" content="Research on reinforcement learning in artificial agents focuses on a single complex problem within a static environment. In biological agents, research focuses on simple learning problems embedded..."/>

    <meta name="twitter:image" content="https://media.springernature.com/full/springer-static/image/art%3A10.1038%2Fs42256-019-0025-4/MediaObjects/42256_2019_25_Fig1_HTML.png"/>

    <meta name="twitter:image:alt" content="Content cover image"/>

    <meta name="WT.z_cc_license_type" content=""/>

    <meta name="WT.z_primary_atype" content="Reviews"/>

    <meta name="WT.z_subject_term" content="Computational models;Computational neuroscience;Neurology"/>

    <meta name="WT.z_subject_term_id" content="computational-models;computational-neuroscience;neurology"/>


    
        <meta property="og:url" content="https://www.nature.com/articles/s42256-019-0025-4"/>
        <meta property="og:type" content="article"/>
        <meta property="og:site_name" content="Nature Machine Intelligence"/>
        <meta property="og:title" content="Reinforcement learning in artificial and biological systems"/>
        <meta property="og:description" content="Research on reinforcement learning in artificial agents focuses on a single complex problem within a static environment. In biological agents, research focuses on simple learning problems embedded in flexible, dynamic environments. The authors review the literature on these topics and suggest areas of synergy between them."/>
        <meta property="og:image" content="https://media.springernature.com/m685/springer-static/image/art%3A10.1038%2Fs42256-019-0025-4/MediaObjects/42256_2019_25_Fig1_HTML.png"/>
    
    <script>
        window.eligibleForRa21 = 'true'; // required by js files for displaying the cobranding box (entitlement-box.js)
    </script>
</head>
<!--[if IE 9]><body class="ie9 article-page"><![endif]-->
<!--[if gt IE 9]><!--><body class="article-page"><!--<![endif]-->

<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-NWDMT9Q"
                  height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>


<div role="banner" class="position-relative cleared z-index-50" data-test="top-containers">
    
    
        <div class="hide-print container background-gray-dark">
            <div class="content mq1200-padded">
                <a href="#content" id="skiplink" class="skiplink contrast-text js-no-scroll" data-track="click"
                   data-track-action="skip to content" data-track-category="header" data-track-label="link">Skip to main
                    content</a>
            </div>
        </div>
    

    
        

        

        <aside class="c-ad c-ad--728x90">
            <div class="c-ad__inner" data-container-type="banner-advert">
                <p class="c-ad__label">Advertisement</p>
                    
            
            
                <div id="article-doubleclickad-container">
    <div id="div-gpt-ad-top-1"
         class="div-gpt-ad advert leaderboard js-ad text-center hide-print grade-c-hide"
         data-ad-type="top"
         data-gpt-unitpath="/285/natmachintell.nature.com/article"
         data-gpt-sizes="728x90"
         data-gpt-targeting="type=article;pos=top;artid=s42256-019-0025-4;doi=10.1038/s42256-019-0025-4;subjmeta=114,116,2397,617,631,692;kwrd=Computational models,Computational neuroscience,Neurology">
        <noscript>
            <a href="//pubads.g.doubleclick.net/gampad/jump?iu=/285/natmachintell.nature.com/article&amp;sz=728x90&amp;c=-1726515280&amp;t=pos%3Dtop%26type%3Darticle%26artid%3Ds42256-019-0025-4%26doi%3D10.1038/s42256-019-0025-4%26subjmeta%3D114,116,2397,617,631,692%26kwrd%3DComputational models,Computational neuroscience,Neurology">
                <img data-test="gpt-advert-fallback-img"
                     src="//pubads.g.doubleclick.net/gampad/ad?iu=/285/natmachintell.nature.com/article&amp;sz=728x90&amp;c=-1726515280&amp;t=pos%3Dtop%26type%3Darticle%26artid%3Ds42256-019-0025-4%26doi%3D10.1038/s42256-019-0025-4%26subjmeta%3D114,116,2397,617,631,692%26kwrd%3DComputational models,Computational neuroscience,Neurology"
                     alt="Advertisement"
                     width="728"
                     height="90"></a>
        </noscript>
    </div>
</div>

            
            
        
            </div>
        </aside>
    

    

    
        <div class="hide-print text-orange content grade-c-show">
            
        <p>Thank you for visiting nature.com. You are using a browser version with limited support for CSS. To obtain
            the best experience, we recommend you use a more up to date browser (or turn off compatibility mode in
            Internet Explorer). In the meantime, to ensure continued support, we are displaying the site without styles
            and JavaScript.</p>
    
        </div>
    


    
        
    <div class="hide-print container background-bluegray contrast-text text14 strong lower mq640-hide position-relative z-index-100"
         data-test="header-breadcrumbs">
        <div class="content cleared mq1200-padded">
            
            <div class="breadcrumbs pin-left">
                <ol class="ma0 cleared clean-list inline-list"><li id="breadcrumb0" itemscope="itemscope" itemtype="http://data-vocabulary.org/Breadcrumb"
                                     itemref="breadcrumb1"><a href="/" itemprop="url" class="icon icon-right icon-arrow-right-6x10-white pr15 "
                                                                                                                 data-track="click" data-track-action="breadcrumb" data-track-category="header" data-track-label="link:nature"><span itemprop="title">nature</span></a></li><li id="breadcrumb1" itemscope="itemscope" itemtype="http://data-vocabulary.org/Breadcrumb"
                                     itemref="breadcrumb2" class="ml6"><a href="/natmachintell" itemprop="url" class="icon icon-right icon-arrow-right-6x10-white pr15 "
                                                                                                                 data-track="click" data-track-action="breadcrumb" data-track-category="header" data-track-label="link:nature machine intelligence"><span itemprop="title">nature machine intelligence</span></a></li><li id="breadcrumb2" itemscope="itemscope" itemtype="http://data-vocabulary.org/Breadcrumb"
                                     itemref="breadcrumb3" class="ml6"><a href="/natmachintell/articles?type&#x3D;review-article" itemprop="url" class="icon icon-right icon-arrow-right-6x10-white pr15 "
                                                                                                                 data-track="click" data-track-action="breadcrumb" data-track-category="header" data-track-label="link:review articles"><span itemprop="title">review articles</span></a></li><li id="breadcrumb3" itemscope="itemscope" itemtype="http://data-vocabulary.org/Breadcrumb"><a href="/articles/s42256-019-0025-4" itemprop="url" data-track="click" data-track-action="breadcrumb" data-track-category="header" data-track-label="link:article" class="ml6 text-gray"><span itemprop="title">article</span></a></li></ol>
            </div>
            
            
                <p class="pin-right box-sizing header-primary-color nature-research-logo ma0 pa6 hide-text grade-c-hide">A Nature Research Journal</p>
            
        </div>
    </div>


    

    <div class="u-mb-16">
        
    
        <div class="hide-print container js-header-container mb20 position-relative z-index-50" data-ui="header-container"
             data-container-type="header">
            <div class="cleared clear hide-print position-relative background-gradient-brand-primary js-header default-header z-index-100 composite-layer tighten-line-height">
                <div class="banner js-banner content mq1200-padded position-relative">
                    <div class="inner-banner cleared">
                        <div class="main-column small-header-main pin-left">
                            <div class="header-logo-container header-primary-color">
                                <a href="#menu" id="menu-button" class="js-header-menu-button menu-button js-no-scroll"
                                   data-test="menu-button">
                                    <span class="menu-button-icon icon-rotate">
                                        <span class="menu-button-label">Menu</span>
                                    </span>
                                </a>
                                <h1 class="inline-block">
                                    <a href="/natmachintell"
                                       class="header-logo inline-block"
                                       data-track="click" data-track-action="home" data-track-category="header" data-track-label="image">
                                        
                                            <img alt="Nature Machine Intelligence"
                                                 src="//media.springernature.com/full/nature-cms/uploads/product/natmachintell/header-1e4bd948a119b1c00cd1d536d70cbdf3.svg"
                                                 class="grade-c-invisible header-logo-primary js-svg"
                                                 data-png="//media.springernature.com/full/nature-cms/uploads/product/natmachintell/header-a7b2209dee476d2854c6b306e857138d.png"><img alt="Nature Machine Intelligence"
                                                 src="//media.springernature.com/full/nature-cms/uploads/product/natmachintell/header-1e4bd948a119b1c00cd1d536d70cbdf3.svg"
                                                 class="grade-c-invisible header-logo-secondary js-svg"
                                                 data-png="//media.springernature.com/full/nature-cms/uploads/product/natmachintell/header-ec79cdd9f252a8528049dfd9b9081d63.png">
                                        
                                    </a>
                                </h1>
                            </div>
                        </div>
                        <div class="position-absolute position-right small-header-side">
                            <div class="pin-right">
                                
                                    <a href="#search-menu" data-component="tray-button"
                                       class="pin-left inline-group-top pa10 pr15 pl15 small-header-icons background-bluegray contrast-text text11 mr1"
                                       data-test="search-link" data-track="click" data-track-category="header"
                                       data-track-action="open tray" data-track-label="button">
                                            <span
                                                    class="icon icon-above icon-search-25x25-white icon-search-25x25-gray block text-center">
                                                <span class="block mt6 small-header-hide">Search</span>
                                            </span>
                                    </a>
                                    
                                        <a href="https://www.nature.com/my-account/alerts/subscribe-journal?list-id&#x3D;393"
                                           class="mq640-hide pin-left inline-group-top pa10 small-header-icons background-bluegray contrast-text text11 mr1"
                                           data-test="ealert-link" data-track="click" data-track-action="ealert"
                                           data-track-category="header" data-track-label="link">
                                            <span
                                                    class="icon icon-above icon-ealert-25x25-white icon-ealert-25x25-gray block text-center">
                                                <span class="block mt6 small-header-hide">E-alert</span>
                                            </span>
                                        </a>
                                    
                                    
                                        <a href="http://mts-natmachintell.nature.com/"
                                           class="mq640-hide inline-block pin-left inline-group-top pa10 pr15 pl15 small-header-icons background-bluegray contrast-text text11 mr1"
                                           data-test="submit-link" data-track="click" data-track-action="manuscript submission"
                                           data-track-category="header" data-track-label="link">
                                            <span
                                                    class="icon icon-above icon-submit-25x25-white icon-submit-25x25-gray block text-center">
                                                <span class="block mt6 small-header-hide">Submit</span>
                                            </span>
                                        </a>
                                    
                                    
    <a href="/nams/svc/myaccount"
       id="my-account"
       class="placeholder inline-block pin-left inline-group-top pa10 pr15 pl15 small-header-icons background-bluegray contrast-text text11"
       data-test="login-link" data-track="click" data-track-action="my account" data-track-category="header" data-track-label="link">
        <span class="icon icon-above icon-login-25x25-white icon-login-25x25-gray block text-center">
            <span class="block mt6 small-header-hide">My Account</span>
        </span>
    </a>
    <a href="https://idp.nature.com/authorize/natureuser?client_id&#x3D;grover&amp;redirect_uri&#x3D;https%3A%2F%2Fwww.nature.com%2Farticles%2Fs42256-019-0025-4"
       id="login-button"
       style="display: none;"
       class="placeholder inline-block pin-left inline-group-top pa10 pr15 pl15 small-header-icons background-bluegray contrast-text text11"
       data-test="login-link" data-track="click" data-track-action="login" data-track-category="header" data-track-label="link">
        <span class="icon icon-above icon-login-25x25-white icon-login-25x25-gray block text-center">
            <span class="block mt6 small-header-hide">Login</span>
        </span>
    </a>




                                
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    

        
    </div>

    
    

</div>




<div id="content">
    <div class="container cleared container-type-article" data-container-type="article">
        <div class="content position-relative cleared clear mq1200-padded" data-component="article-container">
            <article itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle" lang="en">
                <div class="c-article-main-column u-float-left js-main-column composite-layer" role="main" data-track-component="article body">
                    <div class="c-article-header">
                        <header>
                            <ul class="c-article-identifiers" data-test="article-identifier">
                                
    
        <li class="c-article-identifiers__item" data-test="article-category">Review Article</li>
    
    
    

                                <li class="c-article-identifiers__item"><a href="#article-info" data-track="click" data-track-action="publication date" data-track-label="link">Published: <time datetime="2019-03-04" itemprop="datePublished">04 March 2019</time></a></li>
                            </ul>

                            
                            <h1 class="c-article-title" data-test="article-title" data-article-title="" itemprop="name headline">Reinforcement learning in artificial and biological systems</h1>
                            <ul class="c-author-list js-etal-collapsed" data-etal="25" data-etal-small="3" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-1" data-author-popup="auth-1">Emre O. Neftci</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="University of California Irvine" /><meta itemprop="address" content="0000 0001 0668 7243, grid.266093.8, Department of Cognitive Sciences, Department of Computer Science, University of California Irvine, Irvine, CA, USA" /></span></sup><sup class="u-js-hide"> <a href="#na1">na1</a></sup> &amp; </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-2" data-author-popup="auth-2" data-corresp-id="c1">Bruno B. Averbeck<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-email"></use></svg></a></span><sup class="u-js-hide"><a href="#Aff2">2</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="National Institute of Mental Health, National Institutes of Health" /><meta itemprop="address" content="0000 0004 0464 0574, grid.416868.5, Laboratory of Neuropsychology, National Institute of Mental Health, National Institutes of Health, Bethesda, MD, USA" /></span></sup><sup class="u-js-hide"> <a href="#na1">na1</a></sup> </li></ul>

                            

                            <p class="c-article-info-details" data-container-section="info">
                                
    <a data-test="journal-link" href="/natmachintell"><i data-test="journal-title">Nature Machine Intelligence</i></a>

                                <b data-test="journal-volume"><span class="u-visually-hidden">volume</span> 1</b>, <span class="u-visually-hidden">pages</span><span itemprop="pageStart">133</span>–<span itemprop="pageEnd">143</span>(<span data-test="article-publication-year">2019</span>)<a href="#citeas" class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-label="link">Cite this article</a>
                            </p>
                            
    
        <div class="c-article-metrics-bar__wrapper u-clear-both">
            <ul class="c-article-metrics-bar u-list-reset">
                
                    <li class=" c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">26k <span class="c-article-metrics-bar__label">Accesses</span></p>
                    </li>
                
                
                    <li class="c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">20 <span class="c-article-metrics-bar__label">Citations</span></p>
                    </li>
                
                
                    
                        <li class="c-article-metrics-bar__item">
                            <p class="c-article-metrics-bar__count">66 <span class="c-article-metrics-bar__label">Altmetric</span></p>
                        </li>
                    
                
                <li class="c-article-metrics-bar__item">
                    <p class="c-article-metrics-bar__details"><a href="/articles/s42256-019-0025-4/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
                </li>
            </ul>
        </div>
    



                            
                        </header>

                        
    <div class="js-hide" data-component="article-subject-links">
        
            <h3 class="c-article__sub-heading">Subjects</h3>
            <ul class="c-article-subject-list">
                <li class="c-article-subject-list__subject"><a href="/subjects/computational-models" data-track="click" data-track-action="view subject" data-track-label="link" itemprop="about">Computational models</a></li><li class="c-article-subject-list__subject"><a href="/subjects/computational-neuroscience" data-track="click" data-track-action="view subject" data-track-label="link" itemprop="about">Computational neuroscience</a></li><li class="c-article-subject-list__subject"><a href="/subjects/neurology" data-track="click" data-track-action="view subject" data-track-label="link" itemprop="about">Neurology</a></li>
            </ul>
        
    </div>

                        
    

                        
                    </div>

                    <div class="c-article-body">
                        <section aria-labelledby="Abs1" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><p>There is and has been a fruitful flow of concepts and ideas between studies of learning in biological and artificial systems. Much early work that led to the development of reinforcement learning (RL) algorithms for artificial systems was inspired by learning rules first developed in biology by Bush and Mosteller, and Rescorla and Wagner. More recently, temporal-difference RL, developed for learning in artificial agents, has provided a foundational framework for interpreting the activity of dopamine neurons. In this Review, we describe state-of-the-art work on RL in biological and artificial agents. We focus on points of contact between these disciplines and identify areas where future research can benefit from information flow between these fields. Most work in biological systems has focused on simple learning problems, often embedded in dynamic environments where flexibility and ongoing learning are important, similar to real-world learning problems faced by biological systems. In contrast, most work in artificial agents has focused on learning a single complex problem in a static environment. Moving forward, work in each field will benefit from a flow of ideas that represent the strengths within each discipline.</p></div></div></section>
                        <noscript>
                            
                                
<div class="c-nature-box c-nature-box--side c-nature-box--mobile" data-component="entitlement-box">
    
        <div class="js-access-button">
            <a href="https://wayf.springernature.com?redirect_uri&#x3D;https%3A%2F%2Fwww.nature.com%2Farticles%2Fs42256-019-0025-4" class="c-article__button" data-test="ra21" data-track="click" data-track-action="institution access" data-track-label="button">
                <svg class="u-icon" width="18" height="18"><use href="#global-icon-institution"></use></svg>
                <span class="c-article__button-text">Access through your institution</span>
            </a>
        </div>
         <div class="js-buy-button">
            <a href="#access-options" class="c-article__button c-article__button--inverted" data-test="ra21" data-track="click" data-track-action="buy or subscribe" data-track-label="button">
                <span>Buy or subscribe</span>
            </a>
        </div>
    
</div>

                            
                        </noscript>
                        
                            <div class="c-nature-box c-nature-box--side u-display-none" aria-hidden="true" data-component="entitlement-box" id=entitlement-box-mobile>
    
        <p class="c-nature-box__text js-text u-display-none" aria-hidden="true"></p>
        
    
        
        <div class="c-pdf-download u-clear-both">
            <a href="/articles/s42256-019-0025-4.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="download-pdf" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="link">
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            </a>
        </div>
        
    

    
</div>

                        
                        
                            <section aria-labelledby="Sec1"><div class="c-article-section" id="Sec1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1">Main</h2><div class="c-article-section__content" id="Sec1-content"><p>Biological and artificial agents must achieve goals to survive and be useful. This goal-directed or hedonistic behaviour is the foundation of reinforcement learning (RL)<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1" title="Sutton, R. S. &amp; Barto, A. G. Reinforcement Learning: An Introduction (MIT Press, Cambridge, 1998)." href="/articles/s42256-019-0025-4#ref-CR1" id="ref-link-section-d105468e345">1</a></sup>, which is learning to choose actions that maximize rewards and minimize punishments or losses. Reinforcement learning is based on interactions between an agent and its environment (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s42256-019-0025-4#Fig1">1a,b</a>). The agent must choose actions based on sensory inputs, where the sensory inputs define the states of the environment. It is the outcomes of these actions over time, either rewards or punishments, that the agent tries to optimize. This formulation is natural for behaviour in biological systems, but it has also proven highly useful for artificial agents.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1"><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 1: Overview of approaches to learning in biological and artificial agents.</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/articles/s42256-019-0025-4/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs42256-019-0025-4/MediaObjects/42256_2019_25_Fig1_HTML.png?as=webp"></source><img aria-describedby="figure-1-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs42256-019-0025-4/MediaObjects/42256_2019_25_Fig1_HTML.png" alt="figure1" loading="lazy" width="685" height="684" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p><b>a</b>, RL is based on the interaction between an agent and its environment. Agents choose actions, <i>a</i><sub><i>t</i></sub>, which lead to changes of the state, <i>s</i><sub><i>t</i></sub>, and rewards, <i>r</i><sub><i>t</i></sub>, which are returned by the environment. The agent’s goal is to maximize rewards over a defined time horizon. Action values, <i>Q</i>(<i>s</i><sub><i>t</i></sub>, <i>a</i><sub><i>t</i></sub>) in experiments used to study RL in biology are often simple functions of the frequencies of choices and rewards (that is, number of rewards <i>R</i> divided by the number of choices <i>C</i> for a small number of choices in bandit tasks). <b>b</b>, The same agent–environment distinction is important in artificial systems. In state-of-the-art artificial RL systems, action values are estimated by training deep networks. They are often complex functions of sensory inputs. <b>c</b>, Biological agents (for example, the brain) employ multiple learning systems that learn at different rates. The amygdala and striatum are two nuclei in the brain that can support RL learning. The amygdala (also see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s42256-019-0025-4#Fig3">3</a>) learns at a fast rate, and therefore can track rapid changes in the environment, but at the expense of sensitivity to noise. The striatum, on the other hand, learns more slowly. While it cannot track rapid changes in environmental values, it is more robust to noise. <b>d</b>, Artificial agents are often trained on complex, statistically stationary problems. The number of training trials is huge, and therefore these systems cannot adapt rapidly to changes in the environment. Artificial agents are often trained on a single task and fail to learn in sequential multitask settings. Hierarchical RL, structural plasticity and consolidation can enable artificial agents to learn on multiple timescales. <b>e</b>, Biological agents interact with the environment in an ‘on-behaviour’ fashion—that is, learning is online and there is a single copy of the environment. <b>f</b>, While many RL approaches for artificial agents follow these principles, the most recent and successful strategies include a form of agent parallelism, where the agents learn on copies of the environment to stabilize learning (see, for example, A3C and IMPALA). Experience replays inspired by the hippocampus or more complementary learning systems can provide the necessary properties for on-behaviour agents, and thus form a point of contact between artificial and biological RL. Credit: Sebastian Kaulitzki/Alamy Stock Photo (brain image); Moritz Wolf/imageBROKER/Alamy Stock Photo (slot machine image).</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/articles/s42256-019-0025-4/figures/1" data-track-dest="link:Figure1 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>Biological systems must find food, avoid harm and reproduce<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2" title="Pribram, K. H. A review of theory in physiological psychology. Annu. Rev. Psychol. 11, 1–40 (1960)." href="/articles/s42256-019-0025-4#ref-CR2" id="ref-link-section-d105468e436">2</a></sup>. The environments in which they live are dynamic and key processes unfold on multiple timescales (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s42256-019-0025-4#Fig1">1c</a>). While some of these changes can be slow and persistent (for example, seasonal), others can be sudden and ephemeral (for example, the appearance of a predator) and even fast and persistent (for example, destruction of a habitat). To deal with these changes, biological systems have to continuously adapt and learn on multiple timescales. Studies of biological systems have often focused on understanding how organisms deal with learning problems where the associations between choices and rewards are immediate, but dynamic<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Janak, P. H. &amp; Tye, K. M. From circuits to behaviour in the amygdala. Nature 517, 284–292 (2015)." href="#ref-CR3" id="ref-link-section-d105468e443">3</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Namburi, P. et al. A circuit mechanism for differentiating positive and negative associations. Nature 520, 675–678 (2015)." href="#ref-CR4" id="ref-link-section-d105468e443_1">4</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 5" title="Paton, J. J., Belova, M. A., Morrison, S. E. &amp; Salzman, C. D. The primate amygdala represents the positive and negative value of visual stimuli during learning. Nature 439, 865–870 (2006)." href="/articles/s42256-019-0025-4#ref-CR5" id="ref-link-section-d105468e446">5</a></sup>. These are similar to ecological problems like learning which food to eat and whether conspecifics are friendly. The values assigned to choices in these cases can be updated rapidly with experience because the credit assignment problem—the link between the choice and the outcome—is straightforward. More concretely, in two-armed bandit paradigms often used to study learning in animals, the rewards associated with choice options can be learned rapidly, and updated when they change<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 6" title="Hamid, A. A. et al. Mesolimbic dopamine signals the value of work. Nat. Neurosci. 19, 117–126 (2016)." href="/articles/s42256-019-0025-4#ref-CR6" id="ref-link-section-d105468e450">6</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 7" title="Costa, V. D., Dal Monte, O., Lucas, D. R., Murray, E. A. &amp; Averbeck, B. B. Amygdala and ventral striatum make distinct contributions to reinforcement learning. Neuron 92, 505–517 (2016)." href="/articles/s42256-019-0025-4#ref-CR7" id="ref-link-section-d105468e453">7</a></sup>.</p><p>On the other hand, artificial agents are constructed from mathematical models and typically trained to solve a single problem in a static environment<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 8" title="Puterman, M. L. Markov Decision Processes: Discrete Stochastic Dynamic Programming (Wiley, New York, 1994)." href="/articles/s42256-019-0025-4#ref-CR8" id="ref-link-section-d105468e460">8</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 9" title="Bertsekas, D. P. Dynamic Programming and Optimal Control (Athena Scientific, Belmont, 1995)." href="/articles/s42256-019-0025-4#ref-CR9" id="ref-link-section-d105468e463">9</a></sup>, meaning that the reward contingencies and environmental responses are statistically fixed. In recent years, the most successful artificial systems, including neural networks, are generally trained in a data-driven fashion through statistical optimization<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 10" title="Vapnik, V. The Nature of Statistical Learning Theory (Springer, New York, 2013)." href="/articles/s42256-019-0025-4#ref-CR10" id="ref-link-section-d105468e467">10</a></sup>. Training on these problems takes an enormous number of trials (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s42256-019-0025-4#Fig1">1d</a>). Due to specific requirements for optimization, the training phase is generally separated from the performance phase (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s42256-019-0025-4#Fig1">1f</a>). The separation of training and performance prevents artificial agents from benefiting from ongoing experience or adapting to changes in the environment. As we discuss later, joining these two phases to form one single ‘lifelong learner’ can lead to instabilities that challenge the assumptions made in statistical learning. Researchers are now attempting to address these issues (for example, DARPA’s Life Learning Machines (L2M) programme and DeepMind), using approaches like multitask reinforcement learning<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 11" title="Hessel, M. et al. Multi-task deep reinforcement learning with PopArt. Preprint at &#xA;                    https://arxiv.org/abs/1809.04474&#xA;                    &#xA;                   (2018)." href="/articles/s42256-019-0025-4#ref-CR11" id="ref-link-section-d105468e477">11</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 12" title="Kirkpatrick, J. et al. Overcoming catastrophic forgetting in neural networks. Proc. Natl Acad. Sci. USA 114, 3521–3526 (2017)." href="/articles/s42256-019-0025-4#ref-CR12" id="ref-link-section-d105468e480">12</a></sup>. However, achieving the data efficiency and adaptability of biological agents in dynamical environments remains a major challenge.</p><p>Despite differences between work on learning in biological and artificial agents, or perhaps due to these differences, there is much room for the flow of ideas between these fields. Systems neuroscience has used many theoretical concepts from the study of RL in artificial agents to frame questions about biological systems. Theoretical RL algorithms, both model-free and model-based, are providing novel insights into reward-based learning processes in biology<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 13" title="Banino, A. et al. Vector-based navigation using grid-like representations in artificial agents. Nature 557, 429–433 (2018)." href="/articles/s42256-019-0025-4#ref-CR13" id="ref-link-section-d105468e487">13</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 14" title="Mattar, M. G. &amp; Daw, N. D. Prioritized memory access explains planning and hippocampal replay. Nat. Neurosci. 21, 1609–1617 (2018)." href="/articles/s42256-019-0025-4#ref-CR14" id="ref-link-section-d105468e490">14</a></sup>. Moving from biology to theory, much of the work on learning in artificial neural networks was driven by ideas from learning in biology, including the perceptron<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 15" title="Rosenblatt, F. The perceptron: a probabilistic model for information-storage and organization in the brain. Psychol. Rev. 65, 386–408 (1958)." href="/articles/s42256-019-0025-4#ref-CR15" id="ref-link-section-d105468e494">15</a></sup> and the wake–sleep algorithm<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 16" title="Hinton, G. E., Dayan, P., Frey, B. J. &amp; Neal, R. M. The “wake-sleep” algorithm for unsupervised neural networks. Science 268, 1158–1161 (1995)." href="/articles/s42256-019-0025-4#ref-CR16" id="ref-link-section-d105468e498">16</a></sup>, which laid the foundations for efficient training of deep networks today.</p><p>A growing body of work explores the intersection of learning in artificial and biological systems. This work attempts on the one side to build an artificial brain and on the other to understand biological brains. In this Review, we focus on describing areas where the flow of ideas from the study of learning in artificial systems has led to increased understanding of learning in biological systems, and vice versa. We also point to areas where this flow of ideas may be exploited in the future, to better understand biological learning systems and to build artificial agents capable of solving increasingly complex real-world problems. Finally, we consider how these bridges underlie recent advances in engineering brain-inspired, neuromorphic technologies.</p><p>The paper is organized by first considering RL in biological systems. We start by discussing computationally simple model-free learning problems, where much is known about both the neural circuitry and behaviour, and ideas from learning in artificial agents have had a deep influence. We then move on to more complex, model-based RL, where ideas from learning in artificial agents have provided tools for understanding behaviour, and work on neural systems is just beginning. The second half of the paper is focused on learning in artificial systems. We start with an overview of the recent successes in training artificial RL systems to solve complex problems that have relied on developments in deep neural networks, which were inspired by networks in biological systems. We then discuss hierarchical RL, a framework developed for learning in artificial agents. It is likely to prove useful in the future for understanding biological systems, as there is little known about the neural circuitry that underlies hierarchical learning in biological agents. Finally, we consider neuromorphic engineering, an area of research that draws explicitly from biology to solve real-world engineering problems.</p></div></div></section><section aria-labelledby="Sec2"><div class="c-article-section" id="Sec2-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec2">Biological systems underlying RL</h2><div class="c-article-section__content" id="Sec2-content"><p>The theoretical constructs of model-free and model-based reinforcement learning were developed to solve learning problems in artificial systems. They have, however, been used to understand learning problems in biological systems at both the behavioural and neural levels. The neural basis of RL in mammalian systems, particularly model-free RL, is arguably one of the best understood in systems neuroscience<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Rescorla, R. A. &amp; Wagner, A. R. in Classical Conditioning II: Current Research and Theory (eds Black, A. H. &amp; Prokasy, W. F.) 64–99 (Appleton-Century-Crofts, New York, 1972)." href="#ref-CR17" id="ref-link-section-d105468e517">17</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Schultz, W., Dayan, P. &amp; Montague, P. R. A neural substrate of prediction and reward. Science 275, 1593–1599 (1997)." href="#ref-CR18" id="ref-link-section-d105468e517_1">18</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Montague, P. R., Dayan, P. &amp; Sejnowski, T. J. A framework for mesencephalic dopamine systems based on predictive Hebbian learning. J. Neurosci. 16, 1936–1947 (1996)." href="#ref-CR19" id="ref-link-section-d105468e517_2">19</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Houk, J. C., Adamas, J. L. &amp; Barto, A. G. in Models of Information Processing in the Basal Ganglia (eds Houk, J. C., Davis, J. L. &amp; Beiser, D. G.) 249–274 (MIT Press, Cambridge, 1995)." href="#ref-CR20" id="ref-link-section-d105468e517_3">20</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 21" title="Frank, M. J. Dynamic dopamine modulation in the basal ganglia: a neurocomputational account of cognitive deficits in medicated and nonmedicated Parkinsonism. J. Cogn. Neurosci. 17, 51–72 (2005)." href="/articles/s42256-019-0025-4#ref-CR21" id="ref-link-section-d105468e520">21</a></sup>. This is due to the success of temporal-difference RL<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 18" title="Schultz, W., Dayan, P. &amp; Montague, P. R. A neural substrate of prediction and reward. Science 275, 1593–1599 (1997)." href="/articles/s42256-019-0025-4#ref-CR18" id="ref-link-section-d105468e524">18</a></sup> and Rescorla–Wagner theories for predicting the activity of dopamine neurons, and the effects of activating dopamine neurons on behaviour. Theories of model-free RL have emphasized the role of frontal-striatal systems<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 20" title="Houk, J. C., Adamas, J. L. &amp; Barto, A. G. in Models of Information Processing in the Basal Ganglia (eds Houk, J. C., Davis, J. L. &amp; Beiser, D. G.) 249–274 (MIT Press, Cambridge, 1995)." href="/articles/s42256-019-0025-4#ref-CR20" id="ref-link-section-d105468e528">20</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 21" title="Frank, M. J. Dynamic dopamine modulation in the basal ganglia: a neurocomputational account of cognitive deficits in medicated and nonmedicated Parkinsonism. J. Cogn. Neurosci. 17, 51–72 (2005)." href="/articles/s42256-019-0025-4#ref-CR21" id="ref-link-section-d105468e531">21</a></sup>, which are the anatomically defined networks connecting prefrontal cortex to the striatum<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 22" title="Haber, S. N., Kim, K. S., Mailly, P. &amp; Calzavara, R. Reward-related cortical inputs define a large striatal region in primates that interface with associative cortical connections, providing a substrate for incentive-based learning. J. Neurosci. 26, 8368–8376 (2006)." href="/articles/s42256-019-0025-4#ref-CR22" id="ref-link-section-d105468e535">22</a></sup>, and dopamine-driven plasticity in these circuits (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s42256-019-0025-4#Fig2">2</a>). According to one model, cortex represents the set of available choices<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 23" title="Mink, J. W. The basal ganglia: focused selection and inhibition of competing motor programs. Prog. Neurobiol. 50, 381–425 (1996)." href="/articles/s42256-019-0025-4#ref-CR23" id="ref-link-section-d105468e543">23</a></sup>. The strength of cortical synapses on striatal cells encodes information about the values of each of the choices<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 21" title="Frank, M. J. Dynamic dopamine modulation in the basal ganglia: a neurocomputational account of cognitive deficits in medicated and nonmedicated Parkinsonism. J. Cogn. Neurosci. 17, 51–72 (2005)." href="/articles/s42256-019-0025-4#ref-CR21" id="ref-link-section-d105468e547">21</a></sup>. Stronger synapses drive increased activity in striatal cells. Therefore, the activity of striatal cells represents the values of the options represented by cortex<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 24" title="Lau, B. &amp; Glimcher, P. W. Value representations in the primate striatum during matching behavior. Neuron 58, 451–463 (2008)." href="/articles/s42256-019-0025-4#ref-CR24" id="ref-link-section-d105468e551">24</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 25" title="O’Doherty, J. et al. Dissociable roles of ventral and dorsal striatum in instrumental conditioning. Science 304, 452–454 (2004)." href="/articles/s42256-019-0025-4#ref-CR25" id="ref-link-section-d105468e554">25</a></sup>. The striatal activity drives choice activity, either via downstream circuitry through the basal ganglia and return loops through the thalamus to the cortex, or via descending projections to brain-stem motor output areas. After making a choice and experiencing an outcome, dopamine encodes a reward prediction error, RPE = <i>r</i> – <i>v</i><sub><i>i</i></sub>. The RPE is the difference between the expected value of the chosen option, <i>v</i><sub><i>i</i></sub>, encoded by the striatum, and the experienced outcome, <i>r</i>. If the RPE is positive, the outcome was better than expected, and there is a phasic increase in dopamine. If the RPE is negative, the outcome was worse than expected, and there is a phasic decrease in dopamine. This change in dopamine concentration drives plasticity on the frontal-striatal synapses representing the chosen option. Increases in dopamine drive increases in synaptic strength and decreases in dopamine drive decreases in synaptic strength (ignoring for simplification direct and indirect pathways). The next time these choice options are experienced, the activity of the striatal neurons will reflect this updated synaptic efficacy, firing more for options that had a positive RPE in the previous trial, and less for options that had a negative RPE. This process in its simplest form is captured by the Rescorla–Wagner equation, which is a stateless RL update model<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 17" title="Rescorla, R. A. &amp; Wagner, A. R. in Classical Conditioning II: Current Research and Theory (eds Black, A. H. &amp; Prokasy, W. F.) 64–99 (Appleton-Century-Crofts, New York, 1972)." href="/articles/s42256-019-0025-4#ref-CR17" id="ref-link-section-d105468e577">17</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 26" title="Averbeck, B. B. &amp; Costa, V. D. Motivational neural circuits underlying reinforcement learning. Nat. Neurosci. 20, 505–512 (2017)." href="/articles/s42256-019-0025-4#ref-CR26" id="ref-link-section-d105468e580">26</a></sup>.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2"><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 2: Anatomy of a model of reinforcement learning shown on a schematic representation of the rhesus monkey striatum.</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/articles/s42256-019-0025-4/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs42256-019-0025-4/MediaObjects/42256_2019_25_Fig2_HTML.png?as=webp"></source><img aria-describedby="figure-2-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs42256-019-0025-4/MediaObjects/42256_2019_25_Fig2_HTML.png" alt="figure2" loading="lazy" width="685" height="448" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p>The model is focused on dopamine and its effects in the striatum. DStr: dorsal striatum. VStr: ventral striatum. Red lines indicate anatomical inputs from the indicated neural population to the striatum. Cortical inputs are excitatory. The dopamine input arises in the midbrain dopamine neurons.</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/articles/s42256-019-0025-4/figures/2" data-track-dest="link:Figure2 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><div id="Equa" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$v_i\left( {k + 1} \right) = v_i\left( k \right) + \alpha (r(k) - v_i\left( k \right))$$</span></div></div><p>This equation summarizes the interaction of neural activity in three brain areas—cortex, which represents the options, <i>i</i>; the striatum, which represents their values <i>v</i><sub><i>i</i></sub>; and mid-brain dopamine neurons, which code RPEs. The equation further describes at a formal level the process of changing value representations during learning that underlie behaviour, where the size of the update is controlled by a learning rate parameter, <i>α</i>. (Note that the original Rescorla–Wagner equation was developed in the context of Pavlovian cue conditioning and not choices.)</p><p>Temporal-difference (TD) learning, first developed for artificial systems<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 27" title="Sutton, R. S. Learning to predict by the methods of temporal differences. Mach. Learn. 3, 9–44 (1988)." href="/articles/s42256-019-0025-4#ref-CR27" id="ref-link-section-d105468e740">27</a></sup>, provides an extension of the Rescorla–Wagner model, to cases where action values depend on states. The state is defined by the information relevant to choosing an option and can be, for example, time. The TD update rule for actions, <i>i</i>, is given by</p><div id="Equb" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$v_i\left( {s_t} \right) \leftarrow v_i\left( {s_t} \right) + \alpha (r\left( t \right) - v_i\left( {s_t} \right) + \gamma v_i\left( {s_{t + 1}} \right))$$</span></div></div><p>In this case, we have used the assignment operator ← to indicate the update after an event. The variable <i>s</i><sub><i>t</i></sub> is the state at time <i>t</i>, and <i>γ</i> is a discount parameter that discounts the value of future states. The TD RPE is given by</p><div id="Equc" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\delta \left( t \right) = r\left( t \right) - v_i\left( {s_t} \right) + \gamma v_i\left( {s_{t + 1}} \right)$$</span></div></div><p>This general theory has been highly successful, and it predicts much behavioural and neural data. For example, a substantial body of work has shown that dopamine neurons code TD RPEs under diverse conditions<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 28" title="Schultz, W. Dopamine reward prediction error coding. Dialog. Clin. Neurosci. 18, 23–32 (2016)." href="/articles/s42256-019-0025-4#ref-CR28" id="ref-link-section-d105468e1068">28</a></sup>, and activating dopamine neurons is equivalent to experiencing an RPE, with respect to learning<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Steinberg, E. E. et al. A causal link between prediction errors, dopamine neurons and learning. Nat. Neurosci. 16, 966–973 (2013)." href="#ref-CR29" id="ref-link-section-d105468e1072">29</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Saunders, B. T., Richard, J. M., Margolis, E. B. &amp; Janak, P. H. Dopamine neurons create Pavlovian conditioned stimuli with circuit-defined motivational properties. Nat. Neurosci. 21, 1072–1083 (2018)." href="#ref-CR30" id="ref-link-section-d105468e1072_1">30</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 31" title="Sharpe, M. J. et al. Dopamine transients are sufficient and necessary for acquisition of model-based associations. Nat. Neurosci. 20, 735–742 (2017)." href="/articles/s42256-019-0025-4#ref-CR31" id="ref-link-section-d105468e1075">31</a></sup>.</p><p>However, this model leaves many details unspecified. For example, it is now clear that a larger set of interconnected areas underlies RL (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s42256-019-0025-4#Fig3">3</a>). These networks are organized around a set of overlapping but segregated cortical-basal ganglia-thalamo-cortical systems. Broadly speaking, there is one system interconnected with the dorsal striatum that mediates learning about rewarding spatial-cognitive processes—for example, spatially directed eye-movements<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Averbeck, B. B., Sohn, J. W. &amp; Lee, D. Activity in prefrontal cortex during dynamic selection of action sequences. Nat. Neurosci. 9, 276–282 (2006)." href="#ref-CR32" id="ref-link-section-d105468e1085">32</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Seo, M., Lee, E. &amp; Averbeck, B. B. Action selection and action value in frontal-striatal circuits. Neuron 74, 947–960 (2012)." href="#ref-CR33" id="ref-link-section-d105468e1085_1">33</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 34" title="Lee, E., Seo, M., Dal Monte, O. &amp; Averbeck, B. B. Injection of a dopamine type 2 receptor antagonist into the dorsal striatum disrupts choices driven by previous outcomes, but not perceptual inference. J. Neurosci. 35, 6298–6306 (2015)." href="/articles/s42256-019-0025-4#ref-CR34" id="ref-link-section-d105468e1088">34</a></sup>— and another system interconnected with the ventral striatum that mediates learning about rewarding stimuli, particularly visual stimuli in the primate<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 35" title="Averbeck, B. B., Lehman, J., Jacobson, M. &amp; Haber, S. N. Estimates of projection overlap and zones of convergence within frontal-striatal circuits. J. Neurosci. 34, 9497–9505 (2014)." href="/articles/s42256-019-0025-4#ref-CR35" id="ref-link-section-d105468e1092">35</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 36" title="Rothenhoefer, K. M. et al. Effects of ventral striatum lesions on stimulus versus action based reinforcement learning. J. Neurosci. 37, 6902–6914 (2017)." href="/articles/s42256-019-0025-4#ref-CR36" id="ref-link-section-d105468e1095">36</a></sup>. The ventral system also has strong inputs from the amygdala, which plays an important role in learning about the values (positive and negative) of stimuli in the environment<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 26" title="Averbeck, B. B. &amp; Costa, V. D. Motivational neural circuits underlying reinforcement learning. Nat. Neurosci. 20, 505–512 (2017)." href="/articles/s42256-019-0025-4#ref-CR26" id="ref-link-section-d105468e1099">26</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 37" title="Friedman, D. P., Aggleton, J. P. &amp; Saunders, R. C. Comparison of hippocampal, amygdala, and perirhinal projections to the nucleus accumbens: combined anterograde and retrograde tracing study in the Macaque brain. J. Comp. Neurol. 450, 345–365 (2002)." href="/articles/s42256-019-0025-4#ref-CR37" id="ref-link-section-d105468e1102">37</a></sup>. There does not appear to be a corresponding structure for the dorsal circuit. The hippocampus, for example, which is not included here, projects to the ventral striatum, not the dorsal striatum<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 37" title="Friedman, D. P., Aggleton, J. P. &amp; Saunders, R. C. Comparison of hippocampal, amygdala, and perirhinal projections to the nucleus accumbens: combined anterograde and retrograde tracing study in the Macaque brain. J. Comp. Neurol. 450, 345–365 (2002)." href="/articles/s42256-019-0025-4#ref-CR37" id="ref-link-section-d105468e1106">37</a></sup>. It is also important to point out that while we present the dorsal and ventral systems as separate circuits, there is a gradient of circuitry interconnecting all parts of the prefrontal cortex to corresponding areas in the striatum<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 35" title="Averbeck, B. B., Lehman, J., Jacobson, M. &amp; Haber, S. N. Estimates of projection overlap and zones of convergence within frontal-striatal circuits. J. Neurosci. 34, 9497–9505 (2014)." href="/articles/s42256-019-0025-4#ref-CR35" id="ref-link-section-d105468e1111">35</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 38" title="Alexander, G. E., DeLong, M. R. &amp; Strick, P. L. Parallel organization of functionally segregated circuits linking basal ganglia and cortex. Annu. Rev. Neurosci. 9, 357–381 (1986)." href="/articles/s42256-019-0025-4#ref-CR38" id="ref-link-section-d105468e1114">38</a></sup>. The dorsal and ventral circuits represent two poles in the system. Furthermore, the role of the dorsal system in eye movements and the role of the ventral system in vision follows to some extent from the tasks that have been used to study them.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-3"><figure><figcaption><b id="Fig3" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 3: Expanded conception of neural circuitry underlying reinforcement learning.</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/articles/s42256-019-0025-4/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs42256-019-0025-4/MediaObjects/42256_2019_25_Fig3_HTML.png?as=webp"></source><img aria-describedby="figure-3-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs42256-019-0025-4/MediaObjects/42256_2019_25_Fig3_HTML.png" alt="figure3" loading="lazy" width="685" height="454" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc"><p>Example systems are taken from a rhesus monkey. The lines indicate anatomical connections between the indicated regions. The network on the left, in red, is specialized for associating values to objects defined by sensory information. This network interconnects mostly ventral structures, including orbito-frontal cortex and the ventral striatum. The network on the right, in blue, is specialized for associating values to cognitive and spatial processes. This network interconects dorsal systems, including dorsal-lateral prefrontal cortex and the dorsal striatum. These systems have a parallel organization, such that the circuits from the cortex, through the basal ganglia (striatum and GPi) and thalamus and back to cortex are organized similarly. The amygdala, however, only participates in the sensory network shown on the left. OFC: orbital prefrontal cortex. lPFC: lateral prefrontal cortex. MD: medial-dorsal thalamus. GPe/GPi: globus pallidus external and internal segments.</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/articles/s42256-019-0025-4/figures/3" data-track-dest="link:Figure3 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div></div></div></section><section aria-labelledby="Sec3"><div class="c-article-section" id="Sec3-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec3">Multiple timescales of learning</h2><div class="c-article-section__content" id="Sec3-content"><p>Biological agents must solve learning problems on multiple timescales and the neural systems underlying RL in biological agents reflect this need. In the ventral system (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s42256-019-0025-4#Fig3">3</a>, red lines), there is an interacting and parallel organization of the amygdala and striatum. Work that has examined the relative contribution of these two systems to RL has suggested that plasticity within the amygdala operates on fast timescales, through an activity-dependent mechanism, whereas plasticity in the striatum operates on slower timescales, through a dopamine-dependent mechanism<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 7" title="Costa, V. D., Dal Monte, O., Lucas, D. R., Murray, E. A. &amp; Averbeck, B. B. Amygdala and ventral striatum make distinct contributions to reinforcement learning. Neuron 92, 505–517 (2016)." href="/articles/s42256-019-0025-4#ref-CR7" id="ref-link-section-d105468e1148">7</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 26" title="Averbeck, B. B. &amp; Costa, V. D. Motivational neural circuits underlying reinforcement learning. Nat. Neurosci. 20, 505–512 (2017)." href="/articles/s42256-019-0025-4#ref-CR26" id="ref-link-section-d105468e1151">26</a></sup>. Having parallel neural systems that learn at different rates allows biological agents to learn efficiently and track changing values in environments characterized by non-stationarities on different timescales<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 39" title="Averbeck, B. B. Amygdala and ventral striatum population codes implement multiple learning rates for reinforcement learning. In IEEE Symposium Series on Computational Intelligence (IEEE, 2017)." href="/articles/s42256-019-0025-4#ref-CR39" id="ref-link-section-d105468e1155">39</a></sup>. The slower striatal/dopamine-dependent system learns more effectively in noisy environments (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s42256-019-0025-4#Fig1">1c</a>), when the values of choices evolve on slower timescales. The amygdala/activity-dependent system, on the other hand, learns more effectively when environments and the underlying values of choices evolve more quickly. However, the amygdala system is more susceptible to noise. The amygdala, due to its rapid activity-dependent plasticity mechanisms, erroneously tracks noisy fluctuations in values, which can lead to inaccurate value estimates if noise is large relative to signal. The striatum, because it updates values slowly, tends to integrate out this noise. Because these two systems both track values, a downstream system must mediate between them, combining the value estimates from each system, according to an ongoing reliability estimate. This mediation process is known as mixture-of-experts in machine learning, where the concept was first developed<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 40" title="Jacobs, R. A., Jordan, M. I., Nowlan, S. J. &amp; Hinton, G. E. Adaptive mixtures of local experts. Neural Comput. 3, 79–87 (1991)." href="/articles/s42256-019-0025-4#ref-CR40" id="ref-link-section-d105468e1162">40</a></sup>. If one of the systems is providing more accurate value estimates, its contribution to behaviour should be up-weighted, relative to the other. It is currently not clear where this downstream system is, although it may be in cortical motor structures, and therefore effector specific. Overall, however, this organization reflects a general principle underlying the biological solutions to RL problems. Specifically, the brain uses multiple interconnected systems to solve the RL problem (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s42256-019-0025-4#Fig3">3</a>). It decomposes the problem into sub-problems and uses multiple parallel but interacting systems to solve learning problems flexibly.</p><p>In computational neuroscience, studies of synaptic plasticity have shown that the timescales of learning are directly related to the dynamical processes of neurons and synapses<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 41" title="Pfister, J. P., Toyoizumi, T., Barber, D. &amp; Gerstner, W. Optimal spike-timing-dependent plasticity for precise action potential firing in supervised learning. Neural Comput. 18, 1318–1348 (2006)." href="/articles/s42256-019-0025-4#ref-CR41" id="ref-link-section-d105468e1173">41</a></sup>. Specifically, these studies found that the spike-timing-dependent plasticity learning window, which determines the magnitude of weight updates (that is, synaptic plasticity), is a direct reflection of the post-synaptic potential, typically in the 1 ms to 100 ms range depending on the neuron and synapse type. More generally, the theory implies that the timescales of the plasticity processes match the timescales of neural activity. Furthermore, theoretical and modelling studies show that having both slow and fast timescales achieves extended memory capacity<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 42" title="Benna, M. K. &amp; Fusi, S. Computational principles of biological memory. Preprint at &#xA;                    https://arxiv.org/abs/1507.07580&#xA;                    &#xA;                   (2015)." href="/articles/s42256-019-0025-4#ref-CR42" id="ref-link-section-d105468e1177">42</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 43" title="Lahiri, S. &amp; Ganguli, S. A memory frontier for complex synapses. In Advances in Neural Information Processing Systems Vol. 26 (eds Burges, C. J. C., Bottou, L., Welling, M., Ghahramani, Z. &amp; Weinberger, K. Q.) 1034–1042 (NIPS, 2013)." href="/articles/s42256-019-0025-4#ref-CR43" id="ref-link-section-d105468e1180">43</a></sup>, improves performance<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 44" title="Koutnik, J., Greff, K., Gomez, F. &amp; Schmidhuber, J. A clockwork RNN. Preprint at &#xA;                    https://arxiv.org/abs/1402.3511&#xA;                    &#xA;                   (2014)." href="/articles/s42256-019-0025-4#ref-CR44" id="ref-link-section-d105468e1184">44</a></sup> and speeds up learning<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 45" title="Neil, D., M., P. &amp; Liu, S.-C. Phased LSTM: accelerating recurrent network training for long or event-based sequences. In Advances in Neural Information Processing Systems Vol. 29 (eds Lee, D. D., Sugiyama, M., Luxburg, U. V., Guyon, I. &amp; Garnett, R.) 3882–3890 (NIPS, 2016)." href="/articles/s42256-019-0025-4#ref-CR45" id="ref-link-section-d105468e1188">45</a></sup>. Therefore, even at the level of single neurons, learning with multiple timescales is advantageous. The multiplicity of timescales is also a central feature of certain artificial recurrent neural networks, such as those composed of long short-term memory (LSTM) units<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 46" title="Hochreiter, S. &amp; Schmidhuber, J. Long short-term memory. Neural Comput. 9, 1735–1780 (1997)." href="/articles/s42256-019-0025-4#ref-CR46" id="ref-link-section-d105468e1192">46</a></sup>. LSTMs were originally designed to improve the temporal horizon of learning by introducing a memory element whose decay time constant is dynamic and data-dependent, inspired by working memory processes studied in biology. Interestingly, models using working memory or multiple clock rates have been shown to reproduce some of the LSTM’s computational capabilities<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 44" title="Koutnik, J., Greff, K., Gomez, F. &amp; Schmidhuber, J. A clockwork RNN. Preprint at &#xA;                    https://arxiv.org/abs/1402.3511&#xA;                    &#xA;                   (2014)." href="/articles/s42256-019-0025-4#ref-CR44" id="ref-link-section-d105468e1197">44</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 47" title="O’Reilly, R. C. &amp; Frank, M. J. Making working memory work: a computational model of learning in the prefrontal cortex and basal ganglia. Neural Comput. 18, 283–328 (2006)." href="/articles/s42256-019-0025-4#ref-CR47" id="ref-link-section-d105468e1200">47</a></sup>.</p><p>The continuous operation of neural dynamics in the brain entails that, in contrast to conventional machine learning and RL in artificial agents, learning in the brain is an ongoing process. In continuous learning, parameters are updated (that is, plasticity) sequentially, following each data sample<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 48" title="Bishop, C. M. Pattern Recognition and Machine Learning (Springer, New York, 2006)." href="/articles/s42256-019-0025-4#ref-CR48" id="ref-link-section-d105468e1207">48</a></sup> (that is, ‘online’ in the machine learning sense). In contrast, batch updates commonly used in machine learning involve processing many events or ‘trials’ before connection weights are updated. This requires storage of these trials. Statistical learning theory shows that it is advantageous in gradient-based learning to operate in the sequential fashion<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 49" title="Bottou, L. &amp; LeCun, Y. Large scale online learning. In Advances in Neural Information Processing Systems Vol. 16 (eds Thrun, S., Saul, L. K. &amp; Schölkopf, B.) (NIPS, 2004)." href="/articles/s42256-019-0025-4#ref-CR49" id="ref-link-section-d105468e1211">49</a></sup>, both in terms of memory and computational complexity, provided data are independent and identically distributed (iid). When the iid case is violated, correlations in the data sampling can lead to catastrophic interference<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 50" title="McCloskey, M. &amp; Cohen, N. J. in Psychology of Learning and Motivation&#xA;                           : Advances in Research and Theory Vol. 24 (ed. Bower, G. H.) 109–165 (1989)." href="/articles/s42256-019-0025-4#ref-CR50" id="ref-link-section-d105468e1215">50</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 51" title="McClelland, J. L., McNaughton, B. L. &amp; O’Reilly, R. C. Why there are complementary learning systems in the hippocampus and neocortex: insights from the successes and failures of connectionist models of learning and memory. Psychol. Rev. 102, 419–457 (1995)." href="/articles/s42256-019-0025-4#ref-CR51" id="ref-link-section-d105468e1218">51</a></sup>—that is, old knowledge tends to be overwritten by new knowledge. This is particularly problematic in RL because data are inherently correlated and the agent modifies the data-sampling process through the choice of actions. Two main avenues exist to combat this problem: complementary learning and replay mechanisms. With replay mechanisms, older experiences are presented to the network again<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="McClelland, J. L., McNaughton, B. L. &amp; O’Reilly, R. C. Why there are complementary learning systems in the hippocampus and neocortex: insights from the successes and failures of connectionist models of learning and memory. Psychol. Rev. 102, 419–457 (1995)." href="#ref-CR51" id="ref-link-section-d105468e1222">51</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Kumaran, D., Hassabis, D. &amp; McClelland, J. L. What learning systems do intelligent agents need? Complementary learning systems theory updated. Trends Cogn. Sci. 20, 512–534 (2016)." href="#ref-CR52" id="ref-link-section-d105468e1222_1">52</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Mnih, V. et al. Human-level control through deep reinforcement learning. Nature 518, 529–533 (2015)." href="#ref-CR53" id="ref-link-section-d105468e1222_2">53</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 54" title="Lin, L.-J. Self-improving reactive agents based on reinforcement learning, planning and teaching. Mach. Learn. 8, 293–321 (1992)." href="/articles/s42256-019-0025-4#ref-CR54" id="ref-link-section-d105468e1225">54</a></sup>. With complementary learning, synapse-specific learning rates change according to past task relevance<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 12" title="Kirkpatrick, J. et al. Overcoming catastrophic forgetting in neural networks. Proc. Natl Acad. Sci. USA 114, 3521–3526 (2017)." href="/articles/s42256-019-0025-4#ref-CR12" id="ref-link-section-d105468e1229">12</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 55" title="Zenke, F., Poole, B. &amp; Ganguli, S. Continual learning through synaptic intelligence. Preprint at &#xA;                    https://arxiv.org/abs/1703.04200&#xA;                    &#xA;                   (2017)." href="/articles/s42256-019-0025-4#ref-CR55" id="ref-link-section-d105468e1232">55</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 56" title="Aljundi, R., Babiloni, F., Elhoseiny, M., Rohrbach, M. &amp; Tuytelaars, T. Memory aware synapses: learning what (not) to forget. Preprint at &#xA;                    https://arxiv.org/abs/1711.09601&#xA;                    &#xA;                   (2017)." href="/articles/s42256-019-0025-4#ref-CR56" id="ref-link-section-d105468e1235">56</a></sup>. In this case, a network mechanism estimates the importance of neurons and synapses in a task and selectively stiffens their parameters. Both mechanisms were inspired by the brain.</p></div></div></section><section aria-labelledby="Sec4"><div class="c-article-section" id="Sec4-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec4">Learning to learn and model-based RL</h2><div class="c-article-section__content" id="Sec4-content"><p>In addition to the model-free learning systems discussed above, mammalian systems can learn using more sophisticated model-based inference strategies. As a side note, the term model-based originally referred specifically to RL learning problems in which state transition functions were known, which allows for Bellman updates<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 57" title="Daw, N. D., Niv, Y. &amp; Dayan, P. Uncertainty-based competition between prefrontal and dorsolateral striatal systems for behavioral control. Nat. Neurosci. 8, 1704–1711 (2005)." href="/articles/s42256-019-0025-4#ref-CR57" id="ref-link-section-d105468e1247">57</a></sup>. However, the term has come to more generally mean any learning process that relies on knowledge of the statistics of the environment, and therefore uses a statistical—usually Bayesian—model. Work in this area has borrowed extensively from concepts first developed to solve learning problems in artificial agents. There is substantial behavioural evidence for model-based inference strategies<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Costa, V. D., Tran, V. L., Turchi, J. &amp; Averbeck, B. B. Reversal learning and dopamine: a Bayesian perspective. J. Neurosci. 35, 2407–2416 (2015)." href="#ref-CR58" id="ref-link-section-d105468e1251">58</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Daw, N. D., Gershman, S. J., Seymour, B., Dayan, P. &amp; Dolan, R. J. Model-based influences on humans’ choices and striatal prediction errors. Neuron 69, 1204–1215 (2011)." href="#ref-CR59" id="ref-link-section-d105468e1251_1">59</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Glascher, J., Daw, N., Dayan, P. &amp; O’Doherty, J. P. States versus rewards: dissociable neural prediction error signals underlying model-based and model-free reinforcement learning. Neuron 66, 585–595 (2010)." href="#ref-CR60" id="ref-link-section-d105468e1251_2">60</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 61" title="Doll, B. B., Simon, D. A. &amp; Daw, N. D. The ubiquity of model-based reinforcement learning. Curr. Opin. Neurobiol. 22, 1075–1081 (2012)." href="/articles/s42256-019-0025-4#ref-CR61" id="ref-link-section-d105468e1254">61</a></sup>, but much less is currently known about the neural circuitry, relative to model-free learning<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 59" title="Daw, N. D., Gershman, S. J., Seymour, B., Dayan, P. &amp; Dolan, R. J. Model-based influences on humans’ choices and striatal prediction errors. Neuron 69, 1204–1215 (2011)." href="/articles/s42256-019-0025-4#ref-CR59" id="ref-link-section-d105468e1258">59</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 62" title="Wunderlich, K., Smittenaar, P. &amp; Dolan, R. J. Dopamine enhances model-based over model-free choice behavior. Neuron 75, 418–424 (2012)." href="/articles/s42256-019-0025-4#ref-CR62" id="ref-link-section-d105468e1261">62</a></sup>. The original theory of model-based RL for biological agents placed model-based learning in prefrontal cortex<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 57" title="Daw, N. D., Niv, Y. &amp; Dayan, P. Uncertainty-based competition between prefrontal and dorsolateral striatal systems for behavioral control. Nat. Neurosci. 8, 1704–1711 (2005)." href="/articles/s42256-019-0025-4#ref-CR57" id="ref-link-section-d105468e1265">57</a></sup>. This was consistent with general ideas about cognitive planning processes being driven by the prefrontal cortex<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 63" title="Miller, E. K. The prefrontal cortex and cognitive control. Nat. Rev. Neurosci. 1, 59–65 (2000)." href="/articles/s42256-019-0025-4#ref-CR63" id="ref-link-section-d105468e1269">63</a></sup>. However, most subsequent work, and the original rat experiments on habit versus goal-directed systems that inspired the theory<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 64" title="Balleine, B. W. &amp; Dickinson, A. Goal-directed instrumental action: contingency and incentive learning and their cortical substrates. Neuropharmacology 37, 407–419 (1998)." href="/articles/s42256-019-0025-4#ref-CR64" id="ref-link-section-d105468e1274">64</a></sup>, does not support this distinction. Several studies have shown that both model-based and model-free learning rely on striatal-dependent processes<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 62" title="Wunderlich, K., Smittenaar, P. &amp; Dolan, R. J. Dopamine enhances model-based over model-free choice behavior. Neuron 75, 418–424 (2012)." href="/articles/s42256-019-0025-4#ref-CR62" id="ref-link-section-d105468e1278">62</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 65" title="Deserno, L. et al. Ventral striatal dopamine reflects behavioral and neural signatures of model-based control during sequential decision making. Proc. Natl Acad. Sci. USA 112, 1595–1600 (2015)." href="/articles/s42256-019-0025-4#ref-CR65" id="ref-link-section-d105468e1281">65</a></sup>, although some studies have suggested that prefrontal cortex underlies aspects of model-based learning<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 60" title="Glascher, J., Daw, N., Dayan, P. &amp; O’Doherty, J. P. States versus rewards: dissociable neural prediction error signals underlying model-based and model-free reinforcement learning. Neuron 66, 585–595 (2010)." href="/articles/s42256-019-0025-4#ref-CR60" id="ref-link-section-d105468e1285">60</a></sup>. Therefore, it is clear that biological systems can use model-based approaches to learn, but the neural systems that underlie this form of learning are not currently understood.</p><p>Behavioural evidence for model-based learning comes in at least three forms. First, mammals can learn to learn<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 66" title="Harlow, H. F. The formation of learning sets. Psychol. Rev. 56, 51–65 (1949)." href="/articles/s42256-019-0025-4#ref-CR66" id="ref-link-section-d105468e1292">66</a></sup>. This means that the rate of learning on a new problem that is drawn from a class of problems with which one has experience improves as one is exposed to more examples from the class. Thus, the statistics of the underlying inference process, or the model that is generating the data, is learned over time. For example, in reversal learning experiments, animals are given a choice between two options, which can be two objects whose locations are randomized<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 62" title="Wunderlich, K., Smittenaar, P. &amp; Dolan, R. J. Dopamine enhances model-based over model-free choice behavior. Neuron 75, 418–424 (2012)." href="/articles/s42256-019-0025-4#ref-CR62" id="ref-link-section-d105468e1296">62</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 67" title="Iversen, S. D. &amp; Mishkin, M. Perseverative interference in monkeys following selective lesions of the inferior prefrontal convexity. Exp. Brain Res. 11, 376–386 (1970)." href="/articles/s42256-019-0025-4#ref-CR67" id="ref-link-section-d105468e1299">67</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 68" title="Jang, A. I. et al. The role of frontal cortical and medial-temporal lobe brain areas in learning a Bayesian prior belief on reversals. J. Neurosci. 35, 11751–11760 (2015)." href="/articles/s42256-019-0025-4#ref-CR68" id="ref-link-section-d105468e1302">68</a></sup>. Choice of one of the options leads to a reward, and choice of the other option leads to no reward. Once an animal has learned to choose the better option, the choice–outcome mapping is switched, such that the previously rewarded option is no longer rewarded, and the previously unrewarded option is rewarded. (In probabilistic versions of this problem, the choices differ in the frequency with which they are rewarded when chosen, and these frequencies switch at reversal.) When the animals are exposed to a series of these reversals, the rate at which they switch preferences improves with experience. Thus, it may take five or ten trials to switch preferences the first time the contingencies are reversed. However, with sufficient experience, the animals may reverse preferences in just one or two trials. This process can be captured by a model that assumes a Bayesian prior over the probability of reversals occurring in the world<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 68" title="Jang, A. I. et al. The role of frontal cortical and medial-temporal lobe brain areas in learning a Bayesian prior belief on reversals. J. Neurosci. 35, 11751–11760 (2015)." href="/articles/s42256-019-0025-4#ref-CR68" id="ref-link-section-d105468e1306">68</a></sup>. The prior starts out low, since the animals have mostly been exposed to stable stimulus-outcome mappings that do not reverse. Because the prior is low, the animals require substantial evidence before they infer that a reversal has taken place. When they fail to receive a reward for a previously rewarded choice, they believe it is noise in the reward delivery process, and not an actual reversal in choice–outcome mappings. However, with experience on the task, the prior on reversals increases, and the animals require less evidence before inferring that a reversal has occurred, and therefore they reverse their choice preferences more rapidly.</p><p>In artificial agents, learning to learn has been put forward as a principled approach to transfer learning, as the ability to generalize across a class of related tasks implies that information used to solve one task has been transferred to another. This idea is fundamental to the recent meta-reinforcement learning approach<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 69" title="Wang, J. X. et al. Prefrontal cortex as a meta-reinforcement learning system. Nat. Neurosci. 21, 860–868 (2018)." href="/articles/s42256-019-0025-4#ref-CR69" id="ref-link-section-d105468e1313">69</a></sup>, where synaptic plasticity driven by dopamine sets up activity-based learning in the prefrontal cortex. Interestingly, successfully transferring knowledge among a class of related problems is equivalent to generalization in the statistical machine learning sense and implies a principled solution to the catastrophic forgetting problem discussed above.</p><p>Second, and related to the first form of model-based learning, animals can use probabilistic inference, or latent state inference, to solve learning problems, when they have had adequate experience with the statistics of the problem<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 70" title="Wilson, R. C., Takahashi, Y. K., Schoenbaum, G. &amp; Niv, Y. Orbitofrontal cortex as a cognitive map of task space. Neuron 81, 267–279 (2014)." href="/articles/s42256-019-0025-4#ref-CR70" id="ref-link-section-d105468e1320">70</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 71" title="Schuck, N. W., Cai, M. B., Wilson, R. C. &amp; Niv, Y. Human orbitofrontal cortex represents a cognitive map of state space. Neuron 91, 1402–1412 (2016)." href="/articles/s42256-019-0025-4#ref-CR71" id="ref-link-section-d105468e1323">71</a></sup>. With sufficient experience, animals can learn that a particular statistical model is optimal for solving an experimental problem. These models can then solve learning problems more effectively than model-free learning approaches. Probabilistic inference is guaranteed to be optimal, if the mammalian system is capable of learning the correct model<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 72" title="DeGroot, M. H. Optimal Statistical Decisions (Wiley, Hoboken, 1970)." href="/articles/s42256-019-0025-4#ref-CR72" id="ref-link-section-d105468e1327">72</a></sup>. In stochastic reversal learning, after the animal has learned that reversals occur, detecting a reversal statistically can be done efficiently using Bayesian inference. This is state inference, since the reward environment is in one of two states (that is, either choice one or choice two is more frequently rewarded). This process can be faster and more efficient than carrying out model-free value updates. To solve this problem with model-free value updates, the animal would have to update the value of the chosen option, using feedback, on each trial. In addition to the efficiency of Bayesian state inference, it has also been shown that animals can learn priors over reversal points, in tasks where reversals tend to happen at predictable points in time<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 58" title="Costa, V. D., Tran, V. L., Turchi, J. &amp; Averbeck, B. B. Reversal learning and dopamine: a Bayesian perspective. J. Neurosci. 35, 2407–2416 (2015)." href="/articles/s42256-019-0025-4#ref-CR58" id="ref-link-section-d105468e1331">58</a></sup>. This is more sophisticated than the prior discussed above, which is a prior on the occurrence of reversals. Priors on the timing of reversals reflect knowledge that reversals tend to occur at particular points in time, and therefore implicitly assume that they occur. These priors play an important role when stochastic choice–outcome mappings make inference difficult. For example, if the optimal choice in a two-armed bandit task delivers rewards 60% of the time and the sub-optimal choice delivers rewards 40% of the time, reversals in the choice outcome mapping will be hard to detect based upon the received rewards and the priors can improve performance. It is not always straightforward, however, to dissociate fast model-free learning from model-based learning, and therefore careful task design and model fitting is required to demonstrate model-based learning in biological systems. Much of the work on these inference processes has suggested that they occur in cortex<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 70" title="Wilson, R. C., Takahashi, Y. K., Schoenbaum, G. &amp; Niv, Y. Orbitofrontal cortex as a cognitive map of task space. Neuron 81, 267–279 (2014)." href="/articles/s42256-019-0025-4#ref-CR70" id="ref-link-section-d105468e1335">70</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 71" title="Schuck, N. W., Cai, M. B., Wilson, R. C. &amp; Niv, Y. Human orbitofrontal cortex represents a cognitive map of state space. Neuron 91, 1402–1412 (2016)." href="/articles/s42256-019-0025-4#ref-CR71" id="ref-link-section-d105468e1338">71</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 73" title="Starkweather, C. K., Babayan, B. M., Uchida, N. &amp; Gershman, S. J. Dopamine reward prediction errors reflect hidden-state inference across time. Nat. Neurosci. 20, 581–589 (2017)." href="/articles/s42256-019-0025-4#ref-CR73" id="ref-link-section-d105468e1341">73</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 74" title="Starkweather, C. K., Gershman, S. J. &amp; Uchida, N. The medial prefrontal cortex shapes dopamine reward prediction errors under state uncertainty. Neuron 98, 616–629 (2018)." href="/articles/s42256-019-0025-4#ref-CR74" id="ref-link-section-d105468e1344">74</a></sup>. This raises the question of whether these processes require plasticity, or whether they rely on faster computational mechanisms, like attractor dynamics. It is possible, for example, that the inference process drives activity in cortical networks into an attractor basin, similar to the mechanism that may underlie working memory<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 75" title="Wang, X. J. Synaptic basis of cortical persistent activity: the importance of NMDA receptors to working memory. J. Neurosci. 19, 9587–9603 (1999)." href="/articles/s42256-019-0025-4#ref-CR75" id="ref-link-section-d105468e1348">75</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 76" title="Schöner, G. in The Cambridge Handbook of Computational Psychology (ed. Sun, R.) 101–126 (Cambridge Univ. Press, Cambridge, 2008)." href="/articles/s42256-019-0025-4#ref-CR76" id="ref-link-section-d105468e1351">76</a></sup>.</p><p>A third and final form of faster learning is model-based, Bellman RL, which is known more accurately as dynamic programming. In this form of model-based learning, one has knowledge of the statistics of the environment<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 9" title="Bertsekas, D. P. Dynamic Programming and Optimal Control (Athena Scientific, Belmont, 1995)." href="/articles/s42256-019-0025-4#ref-CR9" id="ref-link-section-d105468e1359">9</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 77" title="Averbeck, B. B. Theory of choice in bandit, information sampling and foraging tasks. PLoS Comput. Biol. 11, e1004164 (2015)." href="/articles/s42256-019-0025-4#ref-CR77" id="ref-link-section-d105468e1362">77</a></sup>. These statistics include the state action reward function, <span class="mathjax-tex">\(r\left( {s_t,a} \right)\)</span>, the state value function, <span class="mathjax-tex">\(u_t\left( {s_t} \right)\)</span>, and the state-transition function, <span class="mathjax-tex">\(p(j|s_t,a)\)</span>. When these functions are known, one can use Bellman’s equation to arrive at rapid, but computationally demanding, solutions to problems.</p>
                <div id="Equd" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$u_t\left( {s_t} \right) = \mathop {{\max }}\limits_{a \in A_{s_t}} \left\{ {r\left( {s_t,a} \right) + \gamma \mathop {\sum }\limits_{j \in S} p(j|s_t,a)u_{t + 1}(j)} \right\}$$</span></div></div>
              </div></div></section><section aria-labelledby="Sec5"><div class="c-article-section" id="Sec5-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec5">Artificial connectionist RL agents</h2><div class="c-article-section__content" id="Sec5-content"><p>The recent successes of machine learning and RL techniques in artificial systems are spurring widespread interest in solving practical problems with artificial intelligence, such as natural language processing, speech generation, image recognition, autonomous driving and game playing. Much of this renewed interest is due to breakthroughs in deep neural networks and advances in the technologies that support them. In fact, in artificial agents, task-relevant features of the environment (states) must be inferred from high-dimensional sensory data—for example, pixel intensity values from a camera. Human observers can immediately identify the objects and their relative locations in visual data, and assign meaning to these objects. Solving these problems in artificial systems and achieving state-of-the-art performance requires specialized structures and massive amounts of training data<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 53" title="Mnih, V. et al. Human-level control through deep reinforcement learning. Nature 518, 529–533 (2015)." href="/articles/s42256-019-0025-4#ref-CR53" id="ref-link-section-d105468e1735">53</a></sup>. Early pattern recognition models used hand-created features or linear models to extract states from high-dimensional sensory stimuli<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 80" title="Riesenhuber, M. &amp; Poggio, T. Hierarchical models of object recognition in cortex. Nat. Neurosci. 2, 1019–1025 (1999)." href="/articles/s42256-019-0025-4#ref-CR80" id="ref-link-section-d105468e1739">80</a></sup>. These methods required domain-specific knowledge and learning was limited to the selected domain. Thanks to large datasets, and improved computing technologies, deep learning was surprisingly successful in mapping high-dimensional sensory stimuli to task relevant output, or in the case of RL, in mapping from sensor data to chosen action values. Because these neural networks are general-purpose function approximators, they require few domain-specific assumptions to learn task-relevant representations of the environment<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 81" title="Williams, R. J. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Mach. Learn. 8, 229–256 (1992)." href="/articles/s42256-019-0025-4#ref-CR81" id="ref-link-section-d105468e1743">81</a></sup>. Early implementations were successful at solving complex tasks, such as backgammon<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 82" title="Tesauro, G. Temporal difference learning and TD-Gammon. Commun. ACM 38, 58–68 (1995)." href="/articles/s42256-019-0025-4#ref-CR82" id="ref-link-section-d105468e1747">82</a></sup> and autonomous vehicle control<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 83" title="Pomerleau, D. A. ALVINN: An autonomous land vehicle in a neural network. In Advances in Neural Information Processing Systems Vol. 1 (ed. Touretzky, D. S.) (NIPS, 1988)." href="/articles/s42256-019-0025-4#ref-CR83" id="ref-link-section-d105468e1751">83</a></sup>. With improved hardware and algorithms that prevent learning instabilities, these early approaches matured into algorithms that can now match or exceed human capabilities in a wide variety of domains, such as in game playing and motor control<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 53" title="Mnih, V. et al. Human-level control through deep reinforcement learning. Nature 518, 529–533 (2015)." href="/articles/s42256-019-0025-4#ref-CR53" id="ref-link-section-d105468e1756">53</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Levine, S., Finn, C., Darrell, T. &amp; Abbeel, P. End-to-end training of deep visuomotor policies. J. Mach. Learn. Res. 17, 1334–1373 (2016)." href="#ref-CR84" id="ref-link-section-d105468e1759">84</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Gu, S., Lillicrap, T., Sutskever, I. &amp; Levine, S. Continuous deep Q-learning with model-based acceleration. In Proc. 33rd International Conference on Machine Learning Vol. 48 2829–2838 (PMLR, 2016)." href="#ref-CR85" id="ref-link-section-d105468e1759_1">85</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 86" title="Burda, Y., Edwards, H., Storkey, A. &amp; Klimov, O. Exploration by random network distillation. Preprint at &#xA;                    https://arxiv.org/abs/1810.12894&#xA;                    &#xA;                   (2018)." href="/articles/s42256-019-0025-4#ref-CR86" id="ref-link-section-d105468e1762">86</a></sup>.</p><p>Can the success of deep networks and deep RL be leveraged to better understand biological agents? Interestingly, the mathematical framework of artificial recurrent neural networks can adequately describe the discrete-time approximations of simple models of biological neural networks (for example, leaky integrate-and-fire neurons). Indeed, biological neural networks are recurrent (that is, they are stateful and have recurrent connections), binary (that is, they communicate via action potentials) and operate in continuous-time (a neuron can emit an action potential at any point in time)<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 87" title="Vezhnevets, A. S. et al. Feudal networks for hierarchical reinforcement learning. Preprint at &#xA;                    https://arxiv.org/abs/1703.01161&#xA;                    &#xA;                   (2017)." href="/articles/s42256-019-0025-4#ref-CR87" id="ref-link-section-d105468e1769">87</a></sup>, and such properties are commonly studied in artificial neural networks. One of the most constraining differences between biological and mainstream artificial learning systems is architectural: internal states such as neurotransmitter concentrations, synaptic states and membrane potentials are local. Broadly speaking, locality is characterized by the set of variables available to the processing elements (for example, the neuron and the synapse). Many critical computations in machine learning require information that is non-local—for example, to solve the credit-assignment problem. Making non-local information available to the neural processes requires dedicated channels that communicate this information<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 88" title="Neftci, E. O. Data and power efficient intelligence with neuromorphic learning machines. iScience 5, 52–68 (2018)." href="/articles/s42256-019-0025-4#ref-CR88" id="ref-link-section-d105468e1773">88</a></sup>. The dopamine pathway is one such example. The information provided by the dopamine system is, however, only evaluative. Thus, an important challenge in bridging neuroscience and machine learning is to understand how plasticity processes can utilize this evaluative feedback efficiently for learning. Interestingly, an increasing body of work demonstrates that approximate forms of gradient backpropagation compatible with biological neural networks naturally incorporate such feedback, and models trained with them achieve near state-of-the-art results on classical classification benchmarks<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Kaiser, J., Mostafa, H. &amp; Neftci, E. O. Synaptic plasticity dynamics for deep continuous local learning. Preprint at &#xA;                    https://arxiv.org/abs/1811.10766&#xA;                    &#xA;                   (2018)." href="#ref-CR89" id="ref-link-section-d105468e1777">89</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Neftci, E. O., Augustine, C., Paul, S. &amp; Detorakis, G. Event-driven random back-propagation: enabling neuromorphic deep learning machines. Front. Neurosci. 11, 324 (2017)." href="#ref-CR90" id="ref-link-section-d105468e1777_1">90</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 91" title="Lillicrap, T. P., Cownden, D., Tweed, D. B. &amp; Akerman, C. J. Random synaptic feedback weights support error backpropagation for deep learning. Nat. Commun. 7, 13276 (2016)." href="/articles/s42256-019-0025-4#ref-CR91" id="ref-link-section-d105468e1780">91</a></sup>. Synaptic plasticity rules can be derived from gradient descent that lead to ‘three-factor’ rules, consistent with an error-modulated Hebbian learning (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s42256-019-0025-4#Fig4">4</a>). Furthermore, the normative derivation of the learning reveals plasticity dynamics that are matched to the neural and synaptic time constants discussed earlier (roughly 1 ms to 100 ms), such that spatiotemporal patterns of spikes can be efficiently learned.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-4"><figure><figcaption><b id="Fig4" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 4: A model of spike-based deep, continuous local learning (DCLL).</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/articles/s42256-019-0025-4/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs42256-019-0025-4/MediaObjects/42256_2019_25_Fig4_HTML.png?as=webp"></source><img aria-describedby="figure-4-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs42256-019-0025-4/MediaObjects/42256_2019_25_Fig4_HTML.png" alt="figure4" loading="lazy" width="685" height="337" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc"><p>In a feedforward network, each layer of spiking neurons feeds additionally into a local classifier (diamond shaped) through fixed, random connections. The local classifier is trained to produce auxiliary targets <span class="mathjax-tex">\(\hat y\)</span>. In this learning scheme, errors in the local classifiers are propagated through the random connections to train weights coming in to the spiking layer, but no further (curvy, dashed line), thus the learning is local to the layer. The synaptic plasticity rule here is derived from gradient descent. The resulting rule is composed of a pre-synaptic term (the pre-synaptic trace), a post-synaptic term (the derivative of the neural activation probability) and an error, and is consistent with a modulated Hebbian scheme. The left panel shows snapshots of the neural states during learning in the top layer, where <i>u</i>(<i>t</i>) is the membrane potential and <i>ε</i> is the postsynaptic potential response. In this example, the network is trained to produce three time-varying auxiliary targets (targets,<span class="mathjax-tex">\(\hat y\)</span>; predictions, <i>y</i>). This learning architecture and dynamics achieves near state-of-the-art classification on spatiotemporal patterns. Adapted from ref. <sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 89" title="Kaiser, J., Mostafa, H. &amp; Neftci, E. O. Synaptic plasticity dynamics for deep continuous local learning. Preprint at &#xA;                    https://arxiv.org/abs/1811.10766&#xA;                    &#xA;                   (2018)." href="/articles/s42256-019-0025-4#ref-CR89" id="ref-link-section-d105468e1850">89</a></sup> (preprint).</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/articles/s42256-019-0025-4/figures/4" data-track-dest="link:Figure4 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>Although these results have not yet been extended to deep RL, this demonstrated equivalence between biological and artificial neural networks<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 88" title="Neftci, E. O. Data and power efficient intelligence with neuromorphic learning machines. iScience 5, 52–68 (2018)." href="/articles/s42256-019-0025-4#ref-CR88" id="ref-link-section-d105468e1865">88</a></sup> is suggestive of multiple, direct points of contact between machine learning algorithms and neuroscience. From this contact, two key challenges emerge. First, deep neural networks learning in real time require impractical amounts of experience to reach human-level classification accuracies (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s42256-019-0025-4#Fig1">1d</a>), even on simple classical vision benchmarks and in control (the latter easily requiring millions of samples—for example, in game playing<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 53" title="Mnih, V. et al. Human-level control through deep reinforcement learning. Nature 518, 529–533 (2015)." href="/articles/s42256-019-0025-4#ref-CR53" id="ref-link-section-d105468e1872">53</a></sup> or grasping with a robotic control<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 92" title="Levine, S., Pastor, P., Krizhevsky, A., Ibarz, J. &amp; Quillen, D. Learning hand-eye coordination for robotic grasping with deep learning and large-scale data collection. Int. J. Robot. Res. 37, 421–436 (2018)." href="/articles/s42256-019-0025-4#ref-CR92" id="ref-link-section-d105468e1876">92</a></sup>). Rapid learning is important because, behaviourally, an agent must adapt its internal representation at least as fast as the timescale of changes in the environment. The second challenge that emerges is that biological agents learn ‘on-behaviour’, which implies non-iid sampling of the environment or dataset, as discussed earlier (see section ‘Multiple timescales of learning’).</p><p>The slowness of deep learning can be partly attributed to the gradient descent-based nature of the training algorithms, which require many updates in small increments to stably approach a local minimum. Consequently, there is a fundamental trade-off in speed of learning between the amount of domain-specific knowledge that is built into a model and the number of trials necessary to learn the underlying problem. This is the same reason that model-based RL is more data efficient than model-free RL. The more domain knowledge that is built into a system, the more data efficient it will be. By design, deep RL is initialized with little or no domain-specific knowledge, and so gradient descent cannot significantly improve a deep RL model based on a single experience. In contrast, model-based RL, which incorporates problem-specific knowledge and other related methods that do not rely on gradient descent, such as tabular approaches and episodic control<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1" title="Sutton, R. S. &amp; Barto, A. G. Reinforcement Learning: An Introduction (MIT Press, Cambridge, 1998)." href="/articles/s42256-019-0025-4#ref-CR1" id="ref-link-section-d105468e1883">1</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 93" title="Blundell, C. et al. Model-free episodic control. Preprint at &#xA;                    https://arxiv.org/abs/1606.04460&#xA;                    &#xA;                   (2016)." href="/articles/s42256-019-0025-4#ref-CR93" id="ref-link-section-d105468e1886">93</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 94" title="Gershman, S. J. &amp; Daw, N. D. Reinforcement learning and episodic memory in humans and animals: an integrative framework. Ann. Rev. Psychol. 68, 101–128 (2017)." href="/articles/s42256-019-0025-4#ref-CR94" id="ref-link-section-d105468e1889">94</a></sup>, are generally data efficient. The complementary features of model-free and model-based RL suggest that successful artificial agents are likely to include both components (possibly in a hierarchical fashion). Meta-learning techniques that ‘pre-train’ a neural network on a class of related tasks<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 95" title="Finn, C., Abbeel, P. &amp; Levine, S. Model-agnostic meta-learning for fast adaptation of deep networks. Preprint at &#xA;                    https://arxiv.org/abs/1703.03400&#xA;                    &#xA;                   (2017)." href="/articles/s42256-019-0025-4#ref-CR95" id="ref-link-section-d105468e1893">95</a></sup> (in the spirit of Bayesian priors discussed earlier), hierarchy (modularity) and efficient model learning<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 96" title="Ha, D. &amp; Schmidhuber, J. World models. Preprint at &#xA;                    https://arxiv.org/abs/1803.10122&#xA;                    &#xA;                   (2018)." href="/articles/s42256-019-0025-4#ref-CR96" id="ref-link-section-d105468e1897">96</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 97" title="Zambaldi, V. et al. Relational deep reinforcement learning. Preprint at &#xA;                    https://arxiv.org/abs/1806.01830&#xA;                    &#xA;                   (2018)." href="/articles/s42256-019-0025-4#ref-CR97" id="ref-link-section-d105468e1900">97</a></sup> are poised to play key roles in solving the data-efficiency problem.</p><p>The separate components of the RL problem are challenging in different ways, and therefore biological agents have evolved multiple systems optimized to the separate problems. For example, flexibly updating state values in changing environments requires solving problems that differ from state inference (for example, identification of objects and their locations)<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 98" title="Daw, N. D., O’Doherty, J. P., Dayan, P., Seymour, B. &amp; Dolan, R. J. Cortical substrates for exploratory decisions in humans. Nature 441, 876–879 (2006)." href="/articles/s42256-019-0025-4#ref-CR98" id="ref-link-section-d105468e1908">98</a></sup>. Object identification is challenging because the mapping between retinal output (in biological systems) or pixel values (in cameras) and object categories is highly nonlinear<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 99" title="Kriegeskorte, N. Deep neural networks: a new framework for modeling biological vision and brain information processing. Annu. Rev. Vision Sci. 1, 417–446 (2015)." href="/articles/s42256-019-0025-4#ref-CR99" id="ref-link-section-d105468e1912">99</a></sup>. Flexibly updating state values is challenging because the information necessary to update values has to be remembered or stored over time<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 100" title="Bernacchia, A., Seo, H., Lee, D. &amp; Wang, X. J. A reservoir of time constants for memory traces in cortical neurons. Nat. Neurosci. 14, 366–372 (2011)." href="/articles/s42256-019-0025-4#ref-CR100" id="ref-link-section-d105468e1916">100</a></sup>, and the reward outcomes have to be attributed to the appropriate preceding decisions, to solve the credit assignment problem<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 101" title="Walton, M. E., Behrens, T. E., Buckley, M. J., Rudebeck, P. H. &amp; Rushworth, M. F. Separable learning systems in the macaque brain and the role of orbitofrontal cortex in contingent learning. Neuron 65, 927–939 (2010)." href="/articles/s42256-019-0025-4#ref-CR101" id="ref-link-section-d105468e1920">101</a></sup>. Bayesian inference models are also complex. For example, they often require nonlinear interactions of variables across hierarchical levels<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 102" title="Iglesias, S. et al. Hierarchical prediction errors in midbrain and basal forebrain during sensory learning. Neuron 80, 519–530 (2013)." href="/articles/s42256-019-0025-4#ref-CR102" id="ref-link-section-d105468e1924">102</a></sup>. Therefore, learning based on these models requires computations of high complexity. In addition to the separation of the RL problem into state inference, value updating and action selection problems, biological agents also use multiple neural systems that are optimized to learn under different conditions. This division of labour leads to efficient solutions in biological wetware.</p></div></div></section><section aria-labelledby="Sec6"><div class="c-article-section" id="Sec6-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec6">Hierarchical RL</h2><div class="c-article-section__content" id="Sec6-content"><p>Although most learning problems that have been studied in biological systems have been simple, much of behaviour, particularly human behaviour, is complex and hierarchical<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Badre, D. &amp; Frank, M. J. Mechanisms of hierarchical reinforcement learning in cortico-striatal circuits 2: evidence from fMRI. Cereb. Cortex 22, 527–536 (2012)." href="#ref-CR103" id="ref-link-section-d105468e1937">103</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Frank, M. J. &amp; Badre, D. Mechanisms of hierarchical reinforcement learning in corticostriatal circuits 1: computational analysis. Cereb. Cortex 22, 509–526 (2012)." href="#ref-CR104" id="ref-link-section-d105468e1937_1">104</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Botvinick, M. M. Hierarchical models of behavior and prefrontal function. Trends Cogn. Sci. 12, 201–208 (2008)." href="#ref-CR105" id="ref-link-section-d105468e1937_2">105</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Botvinick, M. M., Niv, Y. &amp; Barto, A. C. Hierarchically organized behavior and its neural foundations: a reinforcement learning perspective. Cognition 113, 262–280 (2009)." href="#ref-CR106" id="ref-link-section-d105468e1937_3">106</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Ribas-Fernandes, J. J. et al. A neural signature of hierarchical reinforcement learning. Neuron 71, 370–379 (2011)." href="#ref-CR107" id="ref-link-section-d105468e1937_4">107</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Botvinick, M. M. Hierarchical reinforcement learning and decision making. Curr. Opin. Neurobiol. 22, 956–962 (2012)." href="#ref-CR108" id="ref-link-section-d105468e1937_5">108</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 109" title="Botvinick, M. &amp; Weinstein, A. Model-based hierarchical reinforcement learning and human action control. Philos. Trans. R. Soc. Lond. B 369, 20130480 (2014)." href="/articles/s42256-019-0025-4#ref-CR109" id="ref-link-section-d105468e1940">109</a></sup>. For example, when we drive to the grocery store, we do not think of every specific muscle activation necessary to first walk to the table, then pick-up our car keys, go to the door and so on. Most of the low-level behaviour is automated and these lower-level components are then sequenced at a high level. Several of these ideas have been incorporated into recent hierarchical RL algorithms. Hierarchical reinforcement learning (HRL) strives to group sequences of related low-level actions into hierarchically organized sub-goals. When tasks are abstracted into sub-goals, they can be learned more efficiently<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 110" title="Dayan, P. &amp; Hinton, G. E. Feudal reinforcement learning. In Advances in Neural Information Processing Systems Vol. 5 (eds Hanson, S. J., Cowan, J. D. &amp; Giles, C. L.) 271–278 (NIPS, 1992)." href="/articles/s42256-019-0025-4#ref-CR110" id="ref-link-section-d105468e1944">110</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 111" title="Sutton, R. S., Precup, D. &amp; Singh, S. Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning. Artif. Intell. 112, 181–211 (1999)." href="/articles/s42256-019-0025-4#ref-CR111" id="ref-link-section-d105468e1947">111</a></sup>. This is another example of building knowledge into the learning algorithm to improve data efficiency and, correspondingly, learning rates. To fully exploit the abstraction, it is customary for each level of the hierarchy to operate independently of the other and the reward. The question is then shifted to defining meaningful sub-goals that each layer of the hierarchy should solve. Because meaningful sub-goals simplify the credit-assignment problem during learning, understanding how sub-goals are learned can provide insights into biological systems. Currently, work in artificial systems leads work in biological systems in this important area. Specifically, several models have been developed to train artificial systems using hierarchical approaches. But there is little work understanding how biological systems solve these problems. The models that have been developed to solve learning problems in artificial systems are likely to prove useful for understanding these problems in biological systems.</p><p>One of the first HRL algorithms, known as the options framework, was due to Sutton et al.<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 111" title="Sutton, R. S., Precup, D. &amp; Singh, S. Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning. Artif. Intell. 112, 181–211 (1999)." href="/articles/s42256-019-0025-4#ref-CR111" id="ref-link-section-d105468e1954">111</a></sup>. An option is a hand-engineered building block of behaviour and therefore incorporates substantial domain specific knowledge. For example, in a robot, an option named ‘recharge’ might consist of a policy for searching for a charging station, navigating towards it and initiating a docking sequence. The higher-level abstraction hides low-level actions, such as detailed navigation, and offers succinct building blocks for achieving goal-directed behaviour. In a related algorithm known as feudal RL, the system consists of managers that learn to set tasks for their sub-managers, and sub-managers oversee those tasks while learning how to achieve them. This type of learning is interesting in distributed scenarios, as sub-managers simply need to maximize rewards in their local context, using information at their assigned level of granularity<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 110" title="Dayan, P. &amp; Hinton, G. E. Feudal reinforcement learning. In Advances in Neural Information Processing Systems Vol. 5 (eds Hanson, S. J., Cowan, J. D. &amp; Giles, C. L.) 271–278 (NIPS, 1992)." href="/articles/s42256-019-0025-4#ref-CR110" id="ref-link-section-d105468e1958">110</a></sup>. Whereas the hierarchy was fixed in the options framework, other more recent models, including feudal networks and option critics, have focused on learning this hierarchical structure. These models aim to automatically learn hierarchical task structure such as sub-policies or sub-goals<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 87" title="Vezhnevets, A. S. et al. Feudal networks for hierarchical reinforcement learning. Preprint at &#xA;                    https://arxiv.org/abs/1703.01161&#xA;                    &#xA;                   (2017)." href="/articles/s42256-019-0025-4#ref-CR87" id="ref-link-section-d105468e1962">87</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 112" title="Bacon, P. L., Harb, J. &amp; Precup, D. The option-critic architecture. Proc. Thirty-First AAAI Conference on Artificial Intelligence 1726–1734 (AAAI, 2017)." href="/articles/s42256-019-0025-4#ref-CR112" id="ref-link-section-d105468e1965">112</a></sup>. A key challenge in HRL is to set the intermediate targets or sub-goals. One option is to use an intrinsic motivator, such as curiosity, which seeks novelty about the environment<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 86" title="Burda, Y., Edwards, H., Storkey, A. &amp; Klimov, O. Exploration by random network distillation. Preprint at &#xA;                    https://arxiv.org/abs/1810.12894&#xA;                    &#xA;                   (2018)." href="/articles/s42256-019-0025-4#ref-CR86" id="ref-link-section-d105468e1969">86</a></sup>, or auxiliary tasks such as predicting visual features<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 113" title="Jaderberg, M. et al. Reinforcement learning with unsupervised auxiliary tasks. Preprint at &#xA;                    https://arxiv.org/abs/1611.05397&#xA;                    &#xA;                   (2016)." href="/articles/s42256-019-0025-4#ref-CR113" id="ref-link-section-d105468e1973">113</a></sup>, to promote exploration and predict future states of the environment<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 114" title="Friston, K. The free-energy principle: a unified brain theory? Nat. Rev. Neurosci. 11, 127–138 (2010)." href="/articles/s42256-019-0025-4#ref-CR114" id="ref-link-section-d105468e1978">114</a></sup>. Another related approach is to provide a supervisory signal, such as in imitation learning<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 115" title="Ross, S., Gordon, G. J. &amp; Bagnell, J. A. A reduction of imitation learning and structured prediction to no-regret online learning. Preprint at &#xA;                    https://arxiv.org/abs/1011.0686&#xA;                    &#xA;                   (2010)." href="/articles/s42256-019-0025-4#ref-CR115" id="ref-link-section-d105468e1982">115</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 116" title="Le, H. M. et al. Hierarchical imitation and reinforcement learning. Preprint at &#xA;                    https://arxiv.org/abs/1803.00590&#xA;                    &#xA;                   (2018)." href="/articles/s42256-019-0025-4#ref-CR116" id="ref-link-section-d105468e1985">116</a></sup> where an expert instruction is used when available.</p><p>While the study of the neural systems that underlie HRL in biological agents is just beginning, work has suggested that the same frontal networks that underlie model-free RL are relevant<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 107" title="Ribas-Fernandes, J. J. et al. A neural signature of hierarchical reinforcement learning. Neuron 71, 370–379 (2011)." href="/articles/s42256-019-0025-4#ref-CR107" id="ref-link-section-d105468e1992">107</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 117" title="Koechlin, E., Ody, C. &amp; Kouneiher, F. The architecture of cognitive control in the human prefrontal cortex. Science 302, 1181–1185 (2003)." href="/articles/s42256-019-0025-4#ref-CR117" id="ref-link-section-d105468e1995">117</a></sup>. These studies have, for example, suggested that prefrontal cortex, particularly dorsal-lateral prefrontal cortex, contains a gradient, such that caudal areas implement low-level aspects of behaviours, and rostral areas implement abstracted aspects of behaviours, further up the hierarchy<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 103" title="Badre, D. &amp; Frank, M. J. Mechanisms of hierarchical reinforcement learning in cortico-striatal circuits 2: evidence from fMRI. Cereb. Cortex 22, 527–536 (2012)." href="/articles/s42256-019-0025-4#ref-CR103" id="ref-link-section-d105468e1999">103</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 117" title="Koechlin, E., Ody, C. &amp; Kouneiher, F. The architecture of cognitive control in the human prefrontal cortex. Science 302, 1181–1185 (2003)." href="/articles/s42256-019-0025-4#ref-CR117" id="ref-link-section-d105468e2002">117</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 118" title="Badre, D. &amp; D’Esposito, M. Functional magnetic resonance imaging evidence for a hierarchical organization of the prefrontal cortex. J. Cogn. Neurosci. 19, 2082–2099 (2007)." href="/articles/s42256-019-0025-4#ref-CR118" id="ref-link-section-d105468e2005">118</a></sup>. A major outstanding question about hierarchical control in biological systems, as is the case with artificial agents, is how they learn to group behaviours into sub-goals. Little is known about this process beyond behavioural descriptions<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 119" title="Muessgens, D., Thirugnanasambandam, N., Shitara, H., Popa, T. &amp; Hallett, M. Dissociable roles of preSMA in motor sequence chunking and hand switching—a TMS study. J. Neurophysiol. 116, 2637–2646 (2016)." href="/articles/s42256-019-0025-4#ref-CR119" id="ref-link-section-d105468e2009">119</a></sup>. However, the recent advances in artificial systems provide a conceptual framework for studying these problems more effectively in biological systems. For example, the auxiliary targets in deep, continuous local learning (DCLL) (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s42256-019-0025-4#Fig4">4</a>) provide one such framework that can incorporate intermediate targets and goals.</p></div></div></section><section aria-labelledby="Sec7"><div class="c-article-section" id="Sec7-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec7">Neuromorphic approaches</h2><div class="c-article-section__content" id="Sec7-content"><p>Although machine learning and neural networks share a common history with brain science, much of the recent development has strayed from these roots. A key reason for this branching is that the computers we use are different than the brain in many aspects. When some of the constraints imposed by the modern von Neumann computer architectures are relaxed, inference and learning performance can improve. For example, introducing continuous-time and online, sample-by-sample parameter updates, similar to synaptic plasticity in the brain<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 90" title="Neftci, E. O., Augustine, C., Paul, S. &amp; Detorakis, G. Event-driven random back-propagation: enabling neuromorphic deep learning machines. Front. Neurosci. 11, 324 (2017)." href="/articles/s42256-019-0025-4#ref-CR90" id="ref-link-section-d105468e2024">90</a></sup>, requires fewer basic operations compared to batch learning. However, on mainstream computers, neural network computations are generally carried out in batch fashion to exploit hardware parallelism, and parameter updates incur memory overhead, making near-continuous time updates suboptimal. Another example is capsule networks<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 120" title="Sabour, S., Frosst, N. &amp; Hinton, G. E. Dynamic routing between capsules. In Advances in Neural Information Processing Systems Vol. 30 (eds Guyon, I. et al.) 3856–3866 (2017)." href="/articles/s42256-019-0025-4#ref-CR120" id="ref-link-section-d105468e2028">120</a></sup>, which are best implemented on massively parallel (brain-like) hardware. These observations suggest that methods that are computationally prohibitive on conventional computers are tractable and sometimes even advantageous in massively parallel computing systems like the brain.</p><p>Neuromorphic engineering strives to bridge device physics and behaviour by taking inspiration from the brain’s building blocks, such as spiking neural networks. The recent development of neuromorphic hardware and accelerators with on-chip adaptive capabilities<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 121" title="Davies, M. et al. Loihi: a neuromorphic manycore processor with on-chip learning. IEEE Micro 38, 82–99 (2018)." href="/articles/s42256-019-0025-4#ref-CR121" id="ref-link-section-d105468e2035">121</a>–<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 123" title="Qiao, N. et al. A reconfigurable on-line learning spiking neuromorphic processor comprising 256 neurons and 128K synapses. Front. Neurosci. 9, 141 (2015)." href="/articles/s42256-019-0025-4#ref-CR123" id="ref-link-section-d105468e2038">123</a></sup> offers a platform for designing and evaluating brain-inspired processing and learning algorithms. Example systems were demonstrated as programmable, general-purpose sensorimotor processors<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 124" title="Neftci, E. et al. Synthesizing cognition in neuromorphic electronic systems. Proc. Natl Acad. Sci. USA 110, 3468–3476 (2013)." href="/articles/s42256-019-0025-4#ref-CR124" id="ref-link-section-d105468e2042">124</a></sup> and reinforcement learning<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 125" title="Friedmann, S., Fremaux, N., Schemmel, J., Gerstner, W. &amp; Meier, K. Reward-based learning under hardware constraints-using a RISC processor embedded in a neuromorphic substrate. Front. Neurosci. 7, 160 (2013)." href="/articles/s42256-019-0025-4#ref-CR125" id="ref-link-section-d105468e2046">125</a></sup>.</p><p>This hardware strives to emulate in digital or analogue technologies the dynamical and architectural properties of the brain. They consist of a large number of biologically plausible model neurons and are often equipped with synaptic plasticity to support online learning. These learning dynamics are compatible with modelling efforts in computational neuroscience, such as the three-factor learning rule sketched in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s42256-019-0025-4#Fig4">4</a>.</p><p>While higher levels of implementation are possible to study and even implement reinforcement learning<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 126" title="Hassabis, D., Kumaran, D., Summerfield, C. &amp; Botvinick, M. Neuroscience-inspired artificial intelligence. Neuron 95, 245–258 (2017)." href="/articles/s42256-019-0025-4#ref-CR126" id="ref-link-section-d105468e2059">126</a></sup>, these do not directly address how the realities of the physical machine, such as device-to-device variability, noise and non-locality, shape animals’ inference and learning strategies. The fact that neuromorphic computing is closely dictated by its physical substrate raises computational challenges that are typically overlooked when modelling with conventional digital hardware. These challenges arise from the engineering and communication challenges of co-locating processing and memory, the energetic and hardware cost of such memory which leads to parameter and state quantization, and the unreliability of the substrate in the case of emerging devices or analogue technologies. While the spiking nature of neurons has a minor performance impact provided the credit assignment problem is addressed (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s42256-019-0025-4#Fig4">4</a>), the quantization of the synaptic weight parameters below 8 bits of precision during learning starts to impact classification performance<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 127" title="Courbariaux, M., Bengio, Y. &amp; David, J.-P. Training deep neural networks with low precision multiplications. Preprint at &#xA;                    https://arxiv.org/abs/1412.7024&#xA;                    &#xA;                   (2014)." href="/articles/s42256-019-0025-4#ref-CR127" id="ref-link-section-d105468e2066">127</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 128" title="Detorakis, G. et al. Neural and synaptic array transceiver: a brain-inspired computing framework for embedded learning. Front. Neurosci. 12, 583 (2018)." href="/articles/s42256-019-0025-4#ref-CR128" id="ref-link-section-d105468e2069">128</a></sup> and remains an open challenge. These challenges are also present in the brain, and so computational modelling at the interface of artificial and biological agents plays a key role in addressing these issues.</p><p>Neuromorphic vision sensors that capture the features of biological retinas<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 129" title="Liu, S. C. &amp; Delbruck, T. Neuromorphic sensory systems. Curr. Opin. Neurobiol. 20, 288–295 (2010)." href="/articles/s42256-019-0025-4#ref-CR129" id="ref-link-section-d105468e2077">129</a></sup> are already changing the landscape of computer vision in industry and academia. While current neuromorphic devices as general-purpose or RL processors are still in research phases, the discovery of new memory devices and the looming end of Moore’s law is calling for such alternative computing strategies. Looking forward, with such hardware systems, the bridges between artificial and biological learning can directly translate into smart, adaptive technologies that can benefit medical treatments, transportation and embedded computing.</p></div></div></section><section aria-labelledby="Sec8"><div class="c-article-section" id="Sec8-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec8">Conclusions</h2><div class="c-article-section__content" id="Sec8-content"><p>Artificial agents can be developed to carry out many tasks currently carried out by people. Self-driving cars are just one example currently under development. For these agents to be successful, they must be able to adapt to diverse conditions and learn continuously. Insights gained from the study of continuous learning in biological agents, including the use of multiple learning systems that operate in parallel, and that are optimized to learning in different environments, may be useful for developing more effective artificial agents. In addition, biological systems have decomposed the RL problem into sensory processing, value update and action output components. This allows the brain to optimize processing to the timescales of plasticity necessary for each system.</p><p>Most of the work in biological systems is based on either simple Pavlovian conditioning paradigms, which do not require an overt behavioural response, or two-armed bandit tasks in which animals have to learn which action is most valuable and update those action values as they change over time. Although Pavlovian conditioning and bandit learning are fundamental to much of the learning by biological systems, real behaviour in natural environments is much more complex. These problems are being addressed in artificial systems using hierarchical reinforcement learning. Many of the algorithms developed for studying these problems in artificial systems may be useful in biological systems. In addition, one of the difficult problems with HRL is learning how to decompose complex problems into sub-goals. At a behavioural level, biological systems do this routinely, and therefore insights from the study of behaviour in biological systems may translate to algorithms in artificial systems. Correspondingly, algorithms developed for artificial systems can help frame problems in biological experiments.</p><p>Understanding how the multiple neural systems in the brain can give rise to ongoing learning in diverse environments is already inspiring solutions for complex engineering problems, in the form of novel algorithms and brain-inspired, neuromorphic hardware that can implement large spiking neural networks. Neuromorphic hardware operates on similar dynamical and architectural constraints as the brain, and thus provides an appealing platform for evaluating neuroscience-inspired solutions. Recently developed neuromorphic hardware tools are emerging as ideal candidates for real-world tasks on mobile platforms, thanks to their continuous inference and learning, which occur on an extremely tight energy budget.</p><p>Most state-of-the-art algorithms for RL take a domain general, or generalized function approximation approach and require vast amounts of data and training time. Decomposing the problem into state inference, value updating and action selection components, as is done in the brain, may allow for more efficient learning and the ability to track changes in the environment on fast timescales, similar to biological systems. Ongoing work at the interface of biological and artificial agents capable of reinforcement learning will provide deeper insights into the brain, and more effective artificial agents for solving real-world problems.</p></div></div></section>
                        

                        <section aria-labelledby="Bib1"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references"><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Sutton, R. S. &amp; Barto, A. G. Reinforcement Learning: An Introduction (MIT Press, Cambridge, 1998)." /><span class="c-article-references__counter">1.</span><p class="c-article-references__text" id="ref-CR1">Sutton, R. S. &amp; Barto, A. G. <i>Reinforcement Learning: An Introduction</i> (MIT Press, Cambridge, 1998).</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="KH. Pribram, " /><meta itemprop="datePublished" content="1960" /><meta itemprop="headline" content="Pribram, K. H. A review of theory in physiological psychology. Annu. Rev. Psychol. 11, 1–40 (1960)." /><span class="c-article-references__counter">2.</span><p class="c-article-references__text" id="ref-CR2">Pribram, K. H. A review of theory in physiological psychology. <i>Annu. Rev. Psychol.</i> <b>11</b>, 1–40 (1960).</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1146%2Fannurev.ps.11.020160.000245" aria-label="View reference 2">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 2 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20review%20of%20theory%20in%20physiological%20psychology&amp;journal=Annu.%20Rev.%20Psychol.&amp;volume=11&amp;pages=1-40&amp;publication_year=1960&amp;author=Pribram%2CKH">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="PH. Janak, KM. Tye, " /><meta itemprop="datePublished" content="2015" /><meta itemprop="headline" content="Janak, P. H. &amp; Tye, K. M. From circuits to behaviour in the amygdala. Nature 517, 284–292 (2015)." /><span class="c-article-references__counter">3.</span><p class="c-article-references__text" id="ref-CR3">Janak, P. H. &amp; Tye, K. M. From circuits to behaviour in the amygdala. <i>Nature</i> <b>517</b>, 284–292 (2015).</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1038%2Fnature14188" aria-label="View reference 3">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 3 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=From%20circuits%20to%20behaviour%20in%20the%20amygdala&amp;journal=Nature&amp;volume=517&amp;pages=284-292&amp;publication_year=2015&amp;author=Janak%2CPH&amp;author=Tye%2CKM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="P. Namburi, " /><meta itemprop="datePublished" content="2015" /><meta itemprop="headline" content="Namburi, P. et al. A circuit mechanism for differentiating positive and negative associations. Nature 520, 675" /><span class="c-article-references__counter">4.</span><p class="c-article-references__text" id="ref-CR4">Namburi, P. et al. A circuit mechanism for differentiating positive and negative associations. <i>Nature</i> <b>520</b>, 675–678 (2015).</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1038%2Fnature14366" aria-label="View reference 4">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 4 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20circuit%20mechanism%20for%20differentiating%20positive%20and%20negative%20associations&amp;journal=Nature&amp;volume=520&amp;pages=675-678&amp;publication_year=2015&amp;author=Namburi%2CP">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="JJ. Paton, MA. Belova, SE. Morrison, CD. Salzman, " /><meta itemprop="datePublished" content="2006" /><meta itemprop="headline" content="Paton, J. J., Belova, M. A., Morrison, S. E. &amp; Salzman, C. D. The primate amygdala represents the positive and" /><span class="c-article-references__counter">5.</span><p class="c-article-references__text" id="ref-CR5">Paton, J. J., Belova, M. A., Morrison, S. E. &amp; Salzman, C. D. The primate amygdala represents the positive and negative value of visual stimuli during learning. <i>Nature</i> <b>439</b>, 865–870 (2006).</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1038%2Fnature04490" aria-label="View reference 5">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 5 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20primate%20amygdala%20represents%20the%20positive%20and%20negative%20value%20of%20visual%20stimuli%20during%20learning&amp;journal=Nature&amp;volume=439&amp;pages=865-870&amp;publication_year=2006&amp;author=Paton%2CJJ&amp;author=Belova%2CMA&amp;author=Morrison%2CSE&amp;author=Salzman%2CCD">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="AA. Hamid, " /><meta itemprop="datePublished" content="2016" /><meta itemprop="headline" content="Hamid, A. A. et al. Mesolimbic dopamine signals the value of work. Nat. Neurosci. 19, 117–126 (2016)." /><span class="c-article-references__counter">6.</span><p class="c-article-references__text" id="ref-CR6">Hamid, A. A. et al. Mesolimbic dopamine signals the value of work. <i>Nat. Neurosci.</i> <b>19</b>, 117–126 (2016).</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1038%2Fnn.4173" aria-label="View reference 6">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 6 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Mesolimbic%20dopamine%20signals%20the%20value%20of%20work&amp;journal=Nat.%20Neurosci.&amp;volume=19&amp;pages=117-126&amp;publication_year=2016&amp;author=Hamid%2CAA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="VD. Costa, O. Dal Monte, DR. Lucas, EA. Murray, BB. Averbeck, " /><meta itemprop="datePublished" content="2016" /><meta itemprop="headline" content="Costa, V. D., Dal Monte, O., Lucas, D. R., Murray, E. A. &amp; Averbeck, B. B. Amygdala and ventral striatum make " /><span class="c-article-references__counter">7.</span><p class="c-article-references__text" id="ref-CR7">Costa, V. D., Dal Monte, O., Lucas, D. R., Murray, E. A. &amp; Averbeck, B. B. Amygdala and ventral striatum make distinct contributions to reinforcement learning. <i>Neuron</i> <b>92</b>, 505–517 (2016).</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.neuron.2016.09.025" aria-label="View reference 7">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 7 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Amygdala%20and%20ventral%20striatum%20make%20distinct%20contributions%20to%20reinforcement%20learning&amp;journal=Neuron&amp;volume=92&amp;pages=505-517&amp;publication_year=2016&amp;author=Costa%2CVD&amp;author=Dal%20Monte%2CO&amp;author=Lucas%2CDR&amp;author=Murray%2CEA&amp;author=Averbeck%2CBB">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Puterman, M. L. Markov Decision Processes: Discrete Stochastic Dynamic Programming (Wiley, New York, 1994)." /><span class="c-article-references__counter">8.</span><p class="c-article-references__text" id="ref-CR8">Puterman, M. L. <i>Markov Decision Processes: Discrete Stochastic Dynamic Programming</i> (Wiley, New York, 1994).</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Bertsekas, D. P. Dynamic Programming and Optimal Control (Athena Scientific, Belmont, 1995)." /><span class="c-article-references__counter">9.</span><p class="c-article-references__text" id="ref-CR9">Bertsekas, D. P. <i>Dynamic Programming and Optimal Control</i> (Athena Scientific, Belmont, 1995).</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Vapnik, V. The Nature of Statistical Learning Theory (Springer, New York, 2013)." /><span class="c-article-references__counter">10.</span><p class="c-article-references__text" id="ref-CR10">Vapnik, V. <i>The Nature of Statistical Learning Theory</i> (Springer, New York, 2013).</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Hessel, M. et al. Multi-task deep reinforcement learning with PopArt. Preprint at https://arxiv.org/abs/1809.0" /><span class="c-article-references__counter">11.</span><p class="c-article-references__text" id="ref-CR11">Hessel, M. et al. Multi-task deep reinforcement learning with PopArt. Preprint at <a href="https://arxiv.org/abs/1809.04474">https://arxiv.org/abs/1809.04474</a> (2018).</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="J. Kirkpatrick, " /><meta itemprop="datePublished" content="2017" /><meta itemprop="headline" content="Kirkpatrick, J. et al. Overcoming catastrophic forgetting in neural networks. Proc. Natl Acad. Sci. USA 114, 3" /><span class="c-article-references__counter">12.</span><p class="c-article-references__text" id="ref-CR12">Kirkpatrick, J. et al. Overcoming catastrophic forgetting in neural networks. <i>Proc. Natl Acad. Sci. USA</i> <b>114</b>, 3521–3526 (2017).</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.ams.org/mathscinet-getitem?mr=3635506" aria-label="View reference 12 on MathSciNet">MathSciNet</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.emis.de/MATH-item?1404.92015" aria-label="View reference 12 on MATH">MATH</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1073%2Fpnas.1611835114" aria-label="View reference 12">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 12 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Overcoming%20catastrophic%20forgetting%20in%20neural%20networks&amp;journal=Proc.%20Natl%20Acad.%20Sci.%20USA&amp;volume=114&amp;pages=3521-3526&amp;publication_year=2017&amp;author=Kirkpatrick%2CJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="A. Banino, " /><meta itemprop="datePublished" content="2018" /><meta itemprop="headline" content="Banino, A. et al. Vector-based navigation using grid-like representations in artificial agents. Nature 557, 42" /><span class="c-article-references__counter">13.</span><p class="c-article-references__text" id="ref-CR13">Banino, A. et al. Vector-based navigation using grid-like representations in artificial agents. <i>Nature</i> <b>557</b>, 429–433 (2018).</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1038%2Fs41586-018-0102-6" aria-label="View reference 13">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 13 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Vector-based%20navigation%20using%20grid-like%20representations%20in%20artificial%20agents&amp;journal=Nature&amp;volume=557&amp;pages=429-433&amp;publication_year=2018&amp;author=Banino%2CA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="MG. Mattar, ND. Daw, " /><meta itemprop="datePublished" content="2018" /><meta itemprop="headline" content="Mattar, M. G. &amp; Daw, N. D. Prioritized memory access explains planning and hippocampal replay. Nat. Neurosci. " /><span class="c-article-references__counter">14.</span><p class="c-article-references__text" id="ref-CR14">Mattar, M. G. &amp; Daw, N. D. Prioritized memory access explains planning and hippocampal replay. <i>Nat. Neurosci.</i> <b>21</b>, 1609–1617 (2018).</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1038%2Fs41593-018-0232-z" aria-label="View reference 14">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 14 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Prioritized%20memory%20access%20explains%20planning%20and%20hippocampal%20replay&amp;journal=Nat.%20Neurosci.&amp;volume=21&amp;pages=1609-1617&amp;publication_year=2018&amp;author=Mattar%2CMG&amp;author=Daw%2CND">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="F. Rosenblatt, " /><meta itemprop="datePublished" content="1958" /><meta itemprop="headline" content="Rosenblatt, F. The perceptron: a probabilistic model for information-storage and organization in the brain. Ps" /><span class="c-article-references__counter">15.</span><p class="c-article-references__text" id="ref-CR15">Rosenblatt, F. The perceptron: a probabilistic model for information-storage and organization in the brain. <i>Psychol. Rev.</i> <b>65</b>, 386–408 (1958).</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1037%2Fh0042519" aria-label="View reference 15">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 15 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20perceptron%3A%20a%20probabilistic%20model%20for%20information-storage%20and%20organization%20in%20the%20brain&amp;journal=Psychol.%20Rev.&amp;volume=65&amp;pages=386-408&amp;publication_year=1958&amp;author=Rosenblatt%2CF">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="GE. Hinton, P. Dayan, BJ. Frey, RM. Neal, " /><meta itemprop="datePublished" content="1995" /><meta itemprop="headline" content="Hinton, G. E., Dayan, P., Frey, B. J. &amp; Neal, R. M. The “wake-sleep” algorithm for unsupervised neural network" /><span class="c-article-references__counter">16.</span><p class="c-article-references__text" id="ref-CR16">Hinton, G. E., Dayan, P., Frey, B. J. &amp; Neal, R. M. The “wake-sleep” algorithm for unsupervised neural networks. <i>Science</i> <b>268</b>, 1158–1161 (1995).</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1126%2Fscience.7761831" aria-label="View reference 16">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 16 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20%E2%80%9Cwake-sleep%E2%80%9D%20algorithm%20for%20unsupervised%20neural%20networks&amp;journal=Science&amp;volume=268&amp;pages=1158-1161&amp;publication_year=1995&amp;author=Hinton%2CGE&amp;author=Dayan%2CP&amp;author=Frey%2CBJ&amp;author=Neal%2CRM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Rescorla, R. A. &amp; Wagner, A. R. in Classical Conditioning II: Current Research and Theory (eds Black, A. H. &amp; " /><span class="c-article-references__counter">17.</span><p class="c-article-references__text" id="ref-CR17">Rescorla, R. A. &amp; Wagner, A. R. in <i>Classical Conditioning II: Current Research and Theory</i> (eds Black, A. H. &amp; Prokasy, W. F.) 64–99 (Appleton-Century-Crofts, New York, 1972).</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="W. Schultz, P. Dayan, PR. Montague, " /><meta itemprop="datePublished" content="1997" /><meta itemprop="headline" content="Schultz, W., Dayan, P. &amp; Montague, P. R. A neural substrate of prediction and reward. Science 275, 1593–1599 (" /><span class="c-article-references__counter">18.</span><p class="c-article-references__text" id="ref-CR18">Schultz, W., Dayan, P. &amp; Montague, P. R. A neural substrate of prediction and reward. <i>Science</i> <b>275</b>, 1593–1599 (1997).</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1126%2Fscience.275.5306.1593" aria-label="View reference 18">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 18 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20neural%20substrate%20of%20prediction%20and%20reward&amp;journal=Science&amp;volume=275&amp;pages=1593-1599&amp;publication_year=1997&amp;author=Schultz%2CW&amp;author=Dayan%2CP&amp;author=Montague%2CPR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="PR. Montague, P. Dayan, TJ. Sejnowski, " /><meta itemprop="datePublished" content="1996" /><meta itemprop="headline" content="Montague, P. R., Dayan, P. &amp; Sejnowski, T. J. A framework for mesencephalic dopamine systems based on predicti" /><span class="c-article-references__counter">19.</span><p class="c-article-references__text" id="ref-CR19">Montague, P. R., Dayan, P. &amp; Sejnowski, T. J. A framework for mesencephalic dopamine systems based on predictive Hebbian learning. <i>J. Neurosci.</i> <b>16</b>, 1936–1947 (1996).</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1523%2FJNEUROSCI.16-05-01936.1996" aria-label="View reference 19">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 19 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20framework%20for%20mesencephalic%20dopamine%20systems%20based%20on%20predictive%20Hebbian%20learning&amp;journal=J.%20Neurosci.&amp;volume=16&amp;pages=1936-1947&amp;publication_year=1996&amp;author=Montague%2CPR&amp;author=Dayan%2CP&amp;author=Sejnowski%2CTJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Houk, J. C., Adamas, J. L. &amp; Barto, A. G. in Models of Information Processing in the Basal Ganglia (eds Houk, " /><span class="c-article-references__counter">20.</span><p class="c-article-references__text" id="ref-CR20">Houk, J. C., Adamas, J. L. &amp; Barto, A. G. in <i>Models of Information Processing in the</i> <i>Basal</i> <i>Ganglia</i> (eds Houk, J. C., Davis, J. L. &amp; Beiser, D. G.) 249–274 (MIT Press, Cambridge, 1995).</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="MJ. Frank, " /><meta itemprop="datePublished" content="2005" /><meta itemprop="headline" content="Frank, M. J. Dynamic dopamine modulation in the basal ganglia: a neurocomputational account of cognitive defic" /><span class="c-article-references__counter">21.</span><p class="c-article-references__text" id="ref-CR21">Frank, M. J. Dynamic dopamine modulation in the basal ganglia: a neurocomputational account of cognitive deficits in medicated and nonmedicated Parkinsonism. <i>J. Cogn. Neurosci.</i> <b>17</b>, 51–72 (2005).</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1162%2F0898929052880093" aria-label="View reference 21">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 21 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Dynamic%20dopamine%20modulation%20in%20the%20basal%20ganglia%3A%20a%20neurocomputational%20account%20of%20cognitive%20deficits%20in%20medicated%20and%20nonmedicated%20Parkinsonism&amp;journal=J.%20Cogn.%20Neurosci.&amp;volume=17&amp;pages=51-72&amp;publication_year=2005&amp;author=Frank%2CMJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="SN. Haber, KS. Kim, P. Mailly, R. Calzavara, " /><meta itemprop="datePublished" content="2006" /><meta itemprop="headline" content="Haber, S. N., Kim, K. S., Mailly, P. &amp; Calzavara, R. Reward-related cortical inputs define a large striatal re" /><span class="c-article-references__counter">22.</span><p class="c-article-references__text" id="ref-CR22">Haber, S. N., Kim, K. S., Mailly, P. &amp; Calzavara, R. Reward-related cortical inputs define a large striatal region in primates that interface with associative cortical connections, providing a substrate for incentive-based learning. <i>J. Neurosci.</i> <b>26</b>, 8368–8376 (2006).</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1523%2FJNEUROSCI.0271-06.2006" aria-label="View reference 22">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 22 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Reward-related%20cortical%20inputs%20define%20a%20large%20striatal%20region%20in%20primates%20that%20interface%20with%20associative%20cortical%20connections%2C%20providing%20a%20substrate%20for%20incentive-based%20learning&amp;journal=J.%20Neurosci.&amp;volume=26&amp;pages=8368-8376&amp;publication_year=2006&amp;author=Haber%2CSN&amp;author=Kim%2CKS&amp;author=Mailly%2CP&amp;author=Calzavara%2CR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="JW. Mink, " /><meta itemprop="datePublished" content="1996" /><meta itemprop="headline" content="Mink, J. W. The basal ganglia: focused selection and inhibition of competing motor programs. Prog. Neurobiol. " /><span class="c-article-references__counter">23.</span><p class="c-article-references__text" id="ref-CR23">Mink, J. W. The basal ganglia: focused selection and inhibition of competing motor programs. <i>Prog. Neurobiol.</i> <b>50</b>, 381–425 (1996).</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2FS0301-0082%2896%2900042-1" aria-label="View reference 23">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 23 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20basal%20ganglia%3A%20focused%20selection%20and%20inhibition%20of%20competing%20motor%20programs&amp;journal=Prog.%20Neurobiol.&amp;volume=50&amp;pages=381-425&amp;publication_year=1996&amp;author=Mink%2CJW">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="B. Lau, PW. Glimcher, " /><meta itemprop="datePublished" content="2008" /><meta itemprop="headline" content="Lau, B. &amp; Glimcher, P. W. Value representations in the primate striatum during matching behavior. Neuron 58, 4" /><span class="c-article-references__counter">24.</span><p class="c-article-references__text" id="ref-CR24">Lau, B. &amp; Glimcher, P. W. Value representations in the primate striatum during matching behavior. <i>Neuron</i> <b>58</b>, 451–463 (2008).</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.neuron.2008.02.021" aria-label="View reference 24">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 24 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Value%20representations%20in%20the%20primate%20striatum%20during%20matching%20behavior&amp;journal=Neuron&amp;volume=58&amp;pages=451-463&amp;publication_year=2008&amp;author=Lau%2CB&amp;author=Glimcher%2CPW">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="J. O’Doherty, " /><meta itemprop="datePublished" content="2004" /><meta itemprop="headline" content="O’Doherty, J. et al. Dissociable roles of ventral and dorsal striatum in instrumental conditioning. Science 30" /><span class="c-article-references__counter">25.</span><p class="c-article-references__text" id="ref-CR25">O’Doherty, J. et al. Dissociable roles of ventral and dorsal striatum in instrumental conditioning. <i>Science</i> <b>304</b>, 452–454 (2004).</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1126%2Fscience.1094285" aria-label="View reference 25">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 25 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Dissociable%20roles%20of%20ventral%20and%20dorsal%20striatum%20in%20instrumental%20conditioning&amp;journal=Science&amp;volume=304&amp;pages=452-454&amp;publication_year=2004&amp;author=O%E2%80%99Doherty%2CJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="BB. Averbeck, VD. Costa, " /><meta itemprop="datePublished" content="2017" /><meta itemprop="headline" content="Averbeck, B. B. &amp; Costa, V. D. Motivational neural circuits underlying reinforcement learning. Nat. Neurosci. " /><span class="c-article-references__counter">26.</span><p class="c-article-references__text" id="ref-CR26">Averbeck, B. B. &amp; Costa, V. D. Motivational neural circuits underlying reinforcement learning. <i>Nat. Neurosci.</i> <b>20</b>, 505–512 (2017).</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1038%2Fnn.4506" aria-label="View reference 26">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 26 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Motivational%20neural%20circuits%20underlying%20reinforcement%20learning&amp;journal=Nat.%20Neurosci.&amp;volume=20&amp;pages=505-512&amp;publication_year=2017&amp;author=Averbeck%2CBB&amp;author=Costa%2CVD">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="RS. Sutton, " /><meta itemprop="datePublished" content="1988" /><meta itemprop="headline" content="Sutton, R. S. Learning to predict by the methods of temporal differences. Mach. Learn. 3, 9–44 (1988)." /><span class="c-article-references__counter">27.</span><p class="c-article-references__text" id="ref-CR27">Sutton, R. S. Learning to predict by the methods of temporal differences. <i>Mach. Learn.</i> <b>3</b>, 9–44 (1988).</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 27 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Learning%20to%20predict%20by%20the%20methods%20of%20temporal%20differences&amp;journal=Mach.%20Learn.&amp;volume=3&amp;pages=9-44&amp;publication_year=1988&amp;author=Sutton%2CRS">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Schultz, W. Dopamine reward prediction error coding. Dialog. Clin. Neurosci. 18, 23–32 (2016)." /><span class="c-article-references__counter">28.</span><p class="c-article-references__text" id="ref-CR28">Schultz, W. Dopamine reward prediction error coding. <i>Dialog. Clin. Neurosci.</i> <b>18</b>, 23–32 (2016).</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="EE. Steinberg, " /><meta itemprop="datePublished" content="2013" /><meta itemprop="headline" content="Steinberg, E. E. et al. A causal link between prediction errors, dopamine neurons and learning. Nat. Neurosci." /><span class="c-article-references__counter">29.</span><p class="c-article-references__text" id="ref-CR29">Steinberg, E. E. et al. A causal link between prediction errors, dopamine neurons and learning. <i>Nat. Neurosci.</i> <b>16</b>, 966–973 (2013).</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1038%2Fnn.3413" aria-label="View reference 29">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 29 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20causal%20link%20between%20prediction%20errors%2C%20dopamine%20neurons%20and%20learning&amp;journal=Nat.%20Neurosci.&amp;volume=16&amp;pages=966-973&amp;publication_year=2013&amp;author=Steinberg%2CEE">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="BT. Saunders, JM. Richard, EB. Margolis, PH. Janak, " /><meta itemprop="datePublished" content="2018" /><meta itemprop="headline" content="Saunders, B. T., Richard, J. M., Margolis, E. B. &amp; Janak, P. H. Dopamine neurons create Pavlovian conditioned " /><span class="c-article-references__counter">30.</span><p class="c-article-references__text" id="ref-CR30">Saunders, B. T., Richard, J. M., Margolis, E. B. &amp; Janak, P. H. Dopamine neurons create Pavlovian conditioned stimuli with circuit-defined motivational properties. <i>Nat. Neurosci.</i> <b>21</b>, 1072–1083 (2018).</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1038%2Fs41593-018-0191-4" aria-label="View reference 30">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 30 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Dopamine%20neurons%20create%20Pavlovian%20conditioned%20stimuli%20with%20circuit-defined%20motivational%20properties&amp;journal=Nat.%20Neurosci.&amp;volume=21&amp;pages=1072-1083&amp;publication_year=2018&amp;author=Saunders%2CBT&amp;author=Richard%2CJM&amp;author=Margolis%2CEB&amp;author=Janak%2CPH">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="MJ. Sharpe, " /><meta itemprop="datePublished" content="2017" /><meta itemprop="headline" content="Sharpe, M. J. et al. Dopamine transients are sufficient and necessary for acquisition of model-based associati" /><span class="c-article-references__counter">31.</span><p class="c-article-references__text" id="ref-CR31">Sharpe, M. J. et al. Dopamine transients are sufficient and necessary for acquisition of model-based associations. <i>Nat. Neurosci.</i> <b>20</b>, 735–742 (2017).</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1038%2Fnn.4538" aria-label="View reference 31">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 31 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Dopamine%20transients%20are%20sufficient%20and%20necessary%20for%20acquisition%20of%20model-based%20associations&amp;journal=Nat.%20Neurosci.&amp;volume=20&amp;pages=735-742&amp;publication_year=2017&amp;author=Sharpe%2CMJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="BB. Averbeck, JW. Sohn, D. Lee, " /><meta itemprop="datePublished" content="2006" /><meta itemprop="headline" content="Averbeck, B. B., Sohn, J. W. &amp; Lee, D. Activity in prefrontal cortex during dynamic selection of action sequen" /><span class="c-article-references__counter">32.</span><p class="c-article-references__text" id="ref-CR32">Averbeck, B. B., Sohn, J. W. &amp; Lee, D. Activity in prefrontal cortex during dynamic selection of action sequences. <i>Nat. Neurosci.</i> <b>9</b>, 276–282 (2006).</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1038%2Fnn1634" aria-label="View reference 32">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 32 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Activity%20in%20prefrontal%20cortex%20during%20dynamic%20selection%20of%20action%20sequences&amp;journal=Nat.%20Neurosci.&amp;volume=9&amp;pages=276-282&amp;publication_year=2006&amp;author=Averbeck%2CBB&amp;author=Sohn%2CJW&amp;author=Lee%2CD">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="M. Seo, E. Lee, BB. Averbeck, " /><meta itemprop="datePublished" content="2012" /><meta itemprop="headline" content="Seo, M., Lee, E. &amp; Averbeck, B. B. Action selection and action value in frontal-striatal circuits. Neuron 74, " /><span class="c-article-references__counter">33.</span><p class="c-article-references__text" id="ref-CR33">Seo, M., Lee, E. &amp; Averbeck, B. B. Action selection and action value in frontal-striatal circuits. <i>Neuron</i> <b>74</b>, 947–960 (2012).</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.neuron.2012.03.037" aria-label="View reference 33">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 33 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Action%20selection%20and%20action%20value%20in%20frontal-striatal%20circuits&amp;journal=Neuron&amp;volume=74&amp;pages=947-960&amp;publication_year=2012&amp;author=Seo%2CM&amp;author=Lee%2CE&amp;author=Averbeck%2CBB">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="E. Lee, M. Seo, O. Dal Monte, BB. Averbeck, " /><meta itemprop="datePublished" content="2015" /><meta itemprop="headline" content="Lee, E., Seo, M., Dal Monte, O. &amp; Averbeck, B. B. Injection of a dopamine type 2 receptor antagonist into the " /><span class="c-article-references__counter">34.</span><p class="c-article-references__text" id="ref-CR34">Lee, E., Seo, M., Dal Monte, O. &amp; Averbeck, B. B. Injection of a dopamine type 2 receptor antagonist into the dorsal striatum disrupts choices driven by previous outcomes, but not perceptual inference. <i>J. Neurosci.</i> <b>35</b>, 6298–6306 (2015).</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1523%2FJNEUROSCI.4561-14.2015" aria-label="View reference 34">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 34 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Injection%20of%20a%20dopamine%20type%202%20receptor%20antagonist%20into%20the%20dorsal%20striatum%20disrupts%20choices%20driven%20by%20previous%20outcomes%2C%20but%20not%20perceptual%20inference&amp;journal=J.%20Neurosci.&amp;volume=35&amp;pages=6298-6306&amp;publication_year=2015&amp;author=Lee%2CE&amp;author=Seo%2CM&amp;author=Dal%20Monte%2CO&amp;author=Averbeck%2CBB">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="BB. Averbeck, J. Lehman, M. Jacobson, SN. Haber, " /><meta itemprop="datePublished" content="2014" /><meta itemprop="headline" content="Averbeck, B. B., Lehman, J., Jacobson, M. &amp; Haber, S. N. Estimates of projection overlap and zones of converge" /><span class="c-article-references__counter">35.</span><p class="c-article-references__text" id="ref-CR35">Averbeck, B. B., Lehman, J., Jacobson, M. &amp; Haber, S. N. Estimates of projection overlap and zones of convergence within frontal-striatal circuits. <i>J. Neurosci.</i> <b>34</b>, 9497–9505 (2014).</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1523%2FJNEUROSCI.5806-12.2014" aria-label="View reference 35">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 35 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Estimates%20of%20projection%20overlap%20and%20zones%20of%20convergence%20within%20frontal-striatal%20circuits&amp;journal=J.%20Neurosci.&amp;volume=34&amp;pages=9497-9505&amp;publication_year=2014&amp;author=Averbeck%2CBB&amp;author=Lehman%2CJ&amp;author=Jacobson%2CM&amp;author=Haber%2CSN">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="KM. Rothenhoefer, " /><meta itemprop="datePublished" content="2017" /><meta itemprop="headline" content="Rothenhoefer, K. M. et al. Effects of ventral striatum lesions on stimulus versus action based reinforcement l" /><span class="c-article-references__counter">36.</span><p class="c-article-references__text" id="ref-CR36">Rothenhoefer, K. M. et al. Effects of ventral striatum lesions on stimulus versus action based reinforcement learning. <i>J. Neurosci.</i> <b>37</b>, 6902–6914 (2017).</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1523%2FJNEUROSCI.0631-17.2017" aria-label="View reference 36">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 36 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Effects%20of%20ventral%20striatum%20lesions%20on%20stimulus%20versus%20action%20based%20reinforcement%20learning&amp;journal=J.%20Neurosci.&amp;volume=37&amp;pages=6902-6914&amp;publication_year=2017&amp;author=Rothenhoefer%2CKM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="DP. Friedman, JP. Aggleton, RC. Saunders, " /><meta itemprop="datePublished" content="2002" /><meta itemprop="headline" content="Friedman, D. P., Aggleton, J. P. &amp; Saunders, R. C. Comparison of hippocampal, amygdala, and perirhinal project" /><span class="c-article-references__counter">37.</span><p class="c-article-references__text" id="ref-CR37">Friedman, D. P., Aggleton, J. P. &amp; Saunders, R. C. Comparison of hippocampal, amygdala, and perirhinal projections to the nucleus accumbens: combined anterograde and retrograde tracing study in the Macaque brain. <i>J. Comp. Neurol.</i> <b>450</b>, 345–365 (2002).</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1002%2Fcne.10336" aria-label="View reference 37">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 37 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Comparison%20of%20hippocampal%2C%20amygdala%2C%20and%20perirhinal%20projections%20to%20the%20nucleus%20accumbens%3A%20combined%20anterograde%20and%20retrograde%20tracing%20study%20in%20the%20Macaque%20brain&amp;journal=J.%20Comp.%20Neurol.&amp;volume=450&amp;pages=345-365&amp;publication_year=2002&amp;author=Friedman%2CDP&amp;author=Aggleton%2CJP&amp;author=Saunders%2CRC">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="GE. Alexander, MR. DeLong, PL. Strick, " /><meta itemprop="datePublished" content="1986" /><meta itemprop="headline" content="Alexander, G. E., DeLong, M. R. &amp; Strick, P. L. Parallel organization of functionally segregated circuits link" /><span class="c-article-references__counter">38.</span><p class="c-article-references__text" id="ref-CR38">Alexander, G. E., DeLong, M. R. &amp; Strick, P. L. Parallel organization of functionally segregated circuits linking basal ganglia and cortex. <i>Annu. Rev. Neurosci.</i> <b>9</b>, 357–381 (1986).</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1146%2Fannurev.ne.09.030186.002041" aria-label="View reference 38">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 38 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Parallel%20organization%20of%20functionally%20segregated%20circuits%20linking%20basal%20ganglia%20and%20cortex&amp;journal=Annu.%20Rev.%20Neurosci.&amp;volume=9&amp;pages=357-381&amp;publication_year=1986&amp;author=Alexander%2CGE&amp;author=DeLong%2CMR&amp;author=Strick%2CPL">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Averbeck, B. B. Amygdala and ventral striatum population codes implement multiple learning rates for reinforce" /><span class="c-article-references__counter">39.</span><p class="c-article-references__text" id="ref-CR39">Averbeck, B. B. Amygdala and ventral striatum population codes implement multiple learning rates for reinforcement learning. In <i>IEEE Symposium Series on Computational Intelligence</i> (IEEE, 2017).</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="RA. Jacobs, MI. Jordan, SJ. Nowlan, GE. Hinton, " /><meta itemprop="datePublished" content="1991" /><meta itemprop="headline" content="Jacobs, R. A., Jordan, M. I., Nowlan, S. J. &amp; Hinton, G. E. Adaptive mixtures of local experts. Neural Comput." /><span class="c-article-references__counter">40.</span><p class="c-article-references__text" id="ref-CR40">Jacobs, R. A., Jordan, M. I., Nowlan, S. J. &amp; Hinton, G. E. Adaptive mixtures of local experts. <i>Neural Comput.</i> <b>3</b>, 79–87 (1991).</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1162%2Fneco.1991.3.1.79" aria-label="View reference 40">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 40 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Adaptive%20mixtures%20of%20local%20experts&amp;journal=Neural%20Comput.&amp;volume=3&amp;pages=79-87&amp;publication_year=1991&amp;author=Jacobs%2CRA&amp;author=Jordan%2CMI&amp;author=Nowlan%2CSJ&amp;author=Hinton%2CGE">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="JP. Pfister, T. Toyoizumi, D. Barber, W. Gerstner, " /><meta itemprop="datePublished" content="2006" /><meta itemprop="headline" content="Pfister, J. P., Toyoizumi, T., Barber, D. &amp; Gerstner, W. Optimal spike-timing-dependent plasticity for precise" /><span class="c-article-references__counter">41.</span><p class="c-article-references__text" id="ref-CR41">Pfister, J. P., Toyoizumi, T., Barber, D. &amp; Gerstner, W. Optimal spike-timing-dependent plasticity for precise action potential firing in supervised learning. <i>Neural Comput.</i> <b>18</b>, 1318–1348 (2006).</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.ams.org/mathscinet-getitem?mr=2218838" aria-label="View reference 41 on MathSciNet">MathSciNet</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.emis.de/MATH-item?1092.92008" aria-label="View reference 41 on MATH">MATH</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1162%2Fneco.2006.18.6.1318" aria-label="View reference 41">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 41 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Optimal%20spike-timing-dependent%20plasticity%20for%20precise%20action%20potential%20firing%20in%20supervised%20learning&amp;journal=Neural%20Comput.&amp;volume=18&amp;pages=1318-1348&amp;publication_year=2006&amp;author=Pfister%2CJP&amp;author=Toyoizumi%2CT&amp;author=Barber%2CD&amp;author=Gerstner%2CW">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Benna, M. K. &amp; Fusi, S. Computational principles of biological memory. Preprint at https://arxiv.org/abs/1507." /><span class="c-article-references__counter">42.</span><p class="c-article-references__text" id="ref-CR42">Benna, M. K. &amp; Fusi, S. Computational principles of biological memory. Preprint at <a href="https://arxiv.org/abs/1507.07580">https://arxiv.org/abs/1507.07580</a> (2015).</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Lahiri, S. &amp; Ganguli, S. A memory frontier for complex synapses. In Advances in Neural Information Processing " /><span class="c-article-references__counter">43.</span><p class="c-article-references__text" id="ref-CR43">Lahiri, S. &amp; Ganguli, S. A memory frontier for complex synapses. In <i>Advances in Neural Information Processing Systems</i> Vol. 26 (eds Burges, C. J. C., Bottou, L., Welling, M., Ghahramani, Z. &amp; Weinberger, K. Q.) 1034–1042 (NIPS, 2013).</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Koutnik, J., Greff, K., Gomez, F. &amp; Schmidhuber, J. A clockwork RNN. Preprint at https://arxiv.org/abs/1402.35" /><span class="c-article-references__counter">44.</span><p class="c-article-references__text" id="ref-CR44">Koutnik, J., Greff, K., Gomez, F. &amp; Schmidhuber, J. A clockwork RNN. Preprint at <a href="https://arxiv.org/abs/1402.3511">https://arxiv.org/abs/1402.3511</a> (2014).</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Neil, D., M., P. &amp; Liu, S.-C. Phased LSTM: accelerating recurrent network training for long or event-based seq" /><span class="c-article-references__counter">45.</span><p class="c-article-references__text" id="ref-CR45">Neil, D., M., P. &amp; Liu, S.-C. Phased LSTM: accelerating recurrent network training for long or event-based sequences. In <i>Advances in Neural Information Processing Systems</i> Vol. 29 (eds Lee, D. D., Sugiyama, M., Luxburg, U. V., Guyon, I. &amp; Garnett, R.) 3882–3890 (NIPS, 2016).</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="S. Hochreiter, J. Schmidhuber, " /><meta itemprop="datePublished" content="1997" /><meta itemprop="headline" content="Hochreiter, S. &amp; Schmidhuber, J. Long short-term memory. Neural Comput. 9, 1735–1780 (1997)." /><span class="c-article-references__counter">46.</span><p class="c-article-references__text" id="ref-CR46">Hochreiter, S. &amp; Schmidhuber, J. Long short-term memory. <i>Neural Comput.</i> <b>9</b>, 1735–1780 (1997).</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1162%2Fneco.1997.9.8.1735" aria-label="View reference 46">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 46 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Long%20short-term%20memory&amp;journal=Neural%20Comput.&amp;volume=9&amp;pages=1735-1780&amp;publication_year=1997&amp;author=Hochreiter%2CS&amp;author=Schmidhuber%2CJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="RC. O’Reilly, MJ. Frank, " /><meta itemprop="datePublished" content="2006" /><meta itemprop="headline" content="O’Reilly, R. C. &amp; Frank, M. J. Making working memory work: a computational model of learning in the prefrontal" /><span class="c-article-references__counter">47.</span><p class="c-article-references__text" id="ref-CR47">O’Reilly, R. C. &amp; Frank, M. J. Making working memory work: a computational model of learning in the prefrontal cortex and basal ganglia. <i>Neural Comput.</i> <b>18</b>, 283–328 (2006).</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.ams.org/mathscinet-getitem?mr=2188058" aria-label="View reference 47 on MathSciNet">MathSciNet</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.emis.de/MATH-item?1090.92008" aria-label="View reference 47 on MATH">MATH</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1162%2F089976606775093909" aria-label="View reference 47">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 47 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Making%20working%20memory%20work%3A%20a%20computational%20model%20of%20learning%20in%20the%20prefrontal%20cortex%20and%20basal%20ganglia&amp;journal=Neural%20Comput.&amp;volume=18&amp;pages=283-328&amp;publication_year=2006&amp;author=O%E2%80%99Reilly%2CRC&amp;author=Frank%2CMJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Bishop, C. M. Pattern Recognition and Machine Learning (Springer, New York, 2006)." /><span class="c-article-references__counter">48.</span><p class="c-article-references__text" id="ref-CR48">Bishop, C. M. <i>Pattern Recognition and Machine Learning</i> (Springer, New York, 2006).</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Bottou, L. &amp; LeCun, Y. Large scale online learning. In Advances in Neural Information Processing Systems Vol. " /><span class="c-article-references__counter">49.</span><p class="c-article-references__text" id="ref-CR49">Bottou, L. &amp; LeCun, Y. Large scale online learning. In <i>Advances in Neural Information Processing Systems</i> Vol. 16 (eds Thrun, S., Saul, L. K. &amp; Schölkopf, B.) (NIPS, 2004).</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="McCloskey, M. &amp; Cohen, N. J. in Psychology of Learning and Motivation&#xA;                           : Advances in" /><span class="c-article-references__counter">50.</span><p class="c-article-references__text" id="ref-CR50">McCloskey, M. &amp; Cohen, N. J. in <i>Psychology of Learning and Motivation</i>
                           <i>: Advances in Research and Theory</i> Vol. 24 (ed. Bower, G. H.) 109–165 (1989).</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="JL. McClelland, BL. McNaughton, RC. O’Reilly, " /><meta itemprop="datePublished" content="1995" /><meta itemprop="headline" content="McClelland, J. L., McNaughton, B. L. &amp; O’Reilly, R. C. Why there are complementary learning systems in the hip" /><span class="c-article-references__counter">51.</span><p class="c-article-references__text" id="ref-CR51">McClelland, J. L., McNaughton, B. L. &amp; O’Reilly, R. C. Why there are complementary learning systems in the hippocampus and neocortex: insights from the successes and failures of connectionist models of learning and memory. <i>Psychol. Rev.</i> <b>102</b>, 419–457 (1995).</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1037%2F0033-295X.102.3.419" aria-label="View reference 51">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 51 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Why%20there%20are%20complementary%20learning%20systems%20in%20the%20hippocampus%20and%20neocortex%3A%20insights%20from%20the%20successes%20and%20failures%20of%20connectionist%20models%20of%20learning%20and%20memory&amp;journal=Psychol.%20Rev.&amp;volume=102&amp;pages=419-457&amp;publication_year=1995&amp;author=McClelland%2CJL&amp;author=McNaughton%2CBL&amp;author=O%E2%80%99Reilly%2CRC">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="D. Kumaran, D. Hassabis, JL. McClelland, " /><meta itemprop="datePublished" content="2016" /><meta itemprop="headline" content="Kumaran, D., Hassabis, D. &amp; McClelland, J. L. What learning systems do intelligent agents need? Complementary " /><span class="c-article-references__counter">52.</span><p class="c-article-references__text" id="ref-CR52">Kumaran, D., Hassabis, D. &amp; McClelland, J. L. What learning systems do intelligent agents need? Complementary learning systems theory updated. <i>Trends Cogn. Sci.</i> <b>20</b>, 512–534 (2016).</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.tics.2016.05.004" aria-label="View reference 52">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 52 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=What%20learning%20systems%20do%20intelligent%20agents%20need%3F%20Complementary%20learning%20systems%20theory%20updated&amp;journal=Trends%20Cogn.%20Sci.&amp;volume=20&amp;pages=512-534&amp;publication_year=2016&amp;author=Kumaran%2CD&amp;author=Hassabis%2CD&amp;author=McClelland%2CJL">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="V. Mnih, " /><meta itemprop="datePublished" content="2015" /><meta itemprop="headline" content="Mnih, V. et al. Human-level control through deep reinforcement learning. Nature 518, 529–533 (2015)." /><span class="c-article-references__counter">53.</span><p class="c-article-references__text" id="ref-CR53">Mnih, V. et al. Human-level control through deep reinforcement learning. <i>Nature</i> <b>518</b>, 529–533 (2015).</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1038%2Fnature14236" aria-label="View reference 53">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 53 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Human-level%20control%20through%20deep%20reinforcement%20learning&amp;journal=Nature&amp;volume=518&amp;pages=529-533&amp;publication_year=2015&amp;author=Mnih%2CV">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="LJ. Lin, " /><meta itemprop="datePublished" content="1992" /><meta itemprop="headline" content="Lin, L.-J. Self-improving reactive agents based on reinforcement learning, planning and teaching. Mach. Learn." /><span class="c-article-references__counter">54.</span><p class="c-article-references__text" id="ref-CR54">Lin, L.-J. Self-improving reactive agents based on reinforcement learning, planning and teaching. <i>Mach. Learn.</i> <b>8</b>, 293–321 (1992).</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 54 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Self-improving%20reactive%20agents%20based%20on%20reinforcement%20learning%2C%20planning%20and%20teaching&amp;journal=Mach.%20Learn.&amp;volume=8&amp;pages=293-321&amp;publication_year=1992&amp;author=Lin%2CLJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Zenke, F., Poole, B. &amp; Ganguli, S. Continual learning through synaptic intelligence. Preprint at https://arxiv" /><span class="c-article-references__counter">55.</span><p class="c-article-references__text" id="ref-CR55">Zenke, F., Poole, B. &amp; Ganguli, S. Continual learning through synaptic intelligence. Preprint at <a href="https://arxiv.org/abs/1703.04200">https://arxiv.org/abs/1703.04200</a> (2017).</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Aljundi, R., Babiloni, F., Elhoseiny, M., Rohrbach, M. &amp; Tuytelaars, T. Memory aware synapses: learning what (" /><span class="c-article-references__counter">56.</span><p class="c-article-references__text" id="ref-CR56">Aljundi, R., Babiloni, F., Elhoseiny, M., Rohrbach, M. &amp; Tuytelaars, T. Memory aware synapses: learning what (not) to forget. Preprint at <a href="https://arxiv.org/abs/1711.09601">https://arxiv.org/abs/1711.09601</a> (2017).</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="ND. Daw, Y. Niv, P. Dayan, " /><meta itemprop="datePublished" content="2005" /><meta itemprop="headline" content="Daw, N. D., Niv, Y. &amp; Dayan, P. Uncertainty-based competition between prefrontal and dorsolateral striatal sys" /><span class="c-article-references__counter">57.</span><p class="c-article-references__text" id="ref-CR57">Daw, N. D., Niv, Y. &amp; Dayan, P. Uncertainty-based competition between prefrontal and dorsolateral striatal systems for behavioral control. <i>Nat. Neurosci.</i> <b>8</b>, 1704–1711 (2005).</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1038%2Fnn1560" aria-label="View reference 57">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 57 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Uncertainty-based%20competition%20between%20prefrontal%20and%20dorsolateral%20striatal%20systems%20for%20behavioral%20control&amp;journal=Nat.%20Neurosci.&amp;volume=8&amp;pages=1704-1711&amp;publication_year=2005&amp;author=Daw%2CND&amp;author=Niv%2CY&amp;author=Dayan%2CP">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Costa, V. D., Tran, V. L., Turchi, J. &amp; Averbeck, B. B. Reversal learning and dopamine: a Bayesian perspective" /><span class="c-article-references__counter">58.</span><p class="c-article-references__text" id="ref-CR58">Costa, V. D., Tran, V. L., Turchi, J. &amp; Averbeck, B. B. Reversal learning and dopamine: a Bayesian perspective. <i>J. Neurosci.</i> <b>35</b>, 2407–2416 (2015).</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="ND. Daw, SJ. Gershman, B. Seymour, P. Dayan, RJ. Dolan, " /><meta itemprop="datePublished" content="2011" /><meta itemprop="headline" content="Daw, N. D., Gershman, S. J., Seymour, B., Dayan, P. &amp; Dolan, R. J. Model-based influences on humans’ choices a" /><span class="c-article-references__counter">59.</span><p class="c-article-references__text" id="ref-CR59">Daw, N. D., Gershman, S. J., Seymour, B., Dayan, P. &amp; Dolan, R. J. Model-based influences on humans’ choices and striatal prediction errors. <i>Neuron</i> <b>69</b>, 1204–1215 (2011).</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.neuron.2011.02.027" aria-label="View reference 59">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 59 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Model-based%20influences%20on%20humans%E2%80%99%20choices%20and%20striatal%20prediction%20errors&amp;journal=Neuron&amp;volume=69&amp;pages=1204-1215&amp;publication_year=2011&amp;author=Daw%2CND&amp;author=Gershman%2CSJ&amp;author=Seymour%2CB&amp;author=Dayan%2CP&amp;author=Dolan%2CRJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="J. Glascher, N. Daw, P. Dayan, JP. O’Doherty, " /><meta itemprop="datePublished" content="2010" /><meta itemprop="headline" content="Glascher, J., Daw, N., Dayan, P. &amp; O’Doherty, J. P. States versus rewards: dissociable neural prediction error" /><span class="c-article-references__counter">60.</span><p class="c-article-references__text" id="ref-CR60">Glascher, J., Daw, N., Dayan, P. &amp; O’Doherty, J. P. States versus rewards: dissociable neural prediction error signals underlying model-based and model-free reinforcement learning. <i>Neuron</i> <b>66</b>, 585–595 (2010).</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.neuron.2010.04.016" aria-label="View reference 60">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 60 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=States%20versus%20rewards%3A%20dissociable%20neural%20prediction%20error%20signals%20underlying%20model-based%20and%20model-free%20reinforcement%20learning&amp;journal=Neuron&amp;volume=66&amp;pages=585-595&amp;publication_year=2010&amp;author=Glascher%2CJ&amp;author=Daw%2CN&amp;author=Dayan%2CP&amp;author=O%E2%80%99Doherty%2CJP">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="BB. Doll, DA. Simon, ND. Daw, " /><meta itemprop="datePublished" content="2012" /><meta itemprop="headline" content="Doll, B. B., Simon, D. A. &amp; Daw, N. D. The ubiquity of model-based reinforcement learning. Curr. Opin. Neurobi" /><span class="c-article-references__counter">61.</span><p class="c-article-references__text" id="ref-CR61">Doll, B. B., Simon, D. A. &amp; Daw, N. D. The ubiquity of model-based reinforcement learning. <i>Curr. Opin. Neurobiol.</i> <b>22</b>, 1075–1081 (2012).</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.conb.2012.08.003" aria-label="View reference 61">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 61 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20ubiquity%20of%20model-based%20reinforcement%20learning&amp;journal=Curr.%20Opin.%20Neurobiol.&amp;volume=22&amp;pages=1075-1081&amp;publication_year=2012&amp;author=Doll%2CBB&amp;author=Simon%2CDA&amp;author=Daw%2CND">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="K. Wunderlich, P. Smittenaar, RJ. Dolan, " /><meta itemprop="datePublished" content="2012" /><meta itemprop="headline" content="Wunderlich, K., Smittenaar, P. &amp; Dolan, R. J. Dopamine enhances model-based over model-free choice behavior. N" /><span class="c-article-references__counter">62.</span><p class="c-article-references__text" id="ref-CR62">Wunderlich, K., Smittenaar, P. &amp; Dolan, R. J. Dopamine enhances model-based over model-free choice behavior. <i>Neuron</i> <b>75</b>, 418–424 (2012).</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.neuron.2012.03.042" aria-label="View reference 62">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 62 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Dopamine%20enhances%20model-based%20over%20model-free%20choice%20behavior&amp;journal=Neuron&amp;volume=75&amp;pages=418-424&amp;publication_year=2012&amp;author=Wunderlich%2CK&amp;author=Smittenaar%2CP&amp;author=Dolan%2CRJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="EK. Miller, " /><meta itemprop="datePublished" content="2000" /><meta itemprop="headline" content="Miller, E. K. The prefrontal cortex and cognitive control. Nat. Rev. Neurosci. 1, 59–65 (2000)." /><span class="c-article-references__counter">63.</span><p class="c-article-references__text" id="ref-CR63">Miller, E. K. The prefrontal cortex and cognitive control. <i>Nat. Rev. Neurosci.</i> <b>1</b>, 59–65 (2000).</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1038%2F35036228" aria-label="View reference 63">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 63 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20prefrontal%20cortex%20and%20cognitive%20control&amp;journal=Nat.%20Rev.%20Neurosci.&amp;volume=1&amp;pages=59-65&amp;publication_year=2000&amp;author=Miller%2CEK">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="BW. Balleine, A. Dickinson, " /><meta itemprop="datePublished" content="1998" /><meta itemprop="headline" content="Balleine, B. W. &amp; Dickinson, A. Goal-directed instrumental action: contingency and incentive learning and thei" /><span class="c-article-references__counter">64.</span><p class="c-article-references__text" id="ref-CR64">Balleine, B. W. &amp; Dickinson, A. Goal-directed instrumental action: contingency and incentive learning and their cortical substrates. <i>Neuropharmacology</i> <b>37</b>, 407–419 (1998).</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2FS0028-3908%2898%2900033-1" aria-label="View reference 64">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 64 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Goal-directed%20instrumental%20action%3A%20contingency%20and%20incentive%20learning%20and%20their%20cortical%20substrates&amp;journal=Neuropharmacology&amp;volume=37&amp;pages=407-419&amp;publication_year=1998&amp;author=Balleine%2CBW&amp;author=Dickinson%2CA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="L. Deserno, " /><meta itemprop="datePublished" content="2015" /><meta itemprop="headline" content="Deserno, L. et al. Ventral striatal dopamine reflects behavioral and neural signatures of model-based control " /><span class="c-article-references__counter">65.</span><p class="c-article-references__text" id="ref-CR65">Deserno, L. et al. Ventral striatal dopamine reflects behavioral and neural signatures of model-based control during sequential decision making. <i>Proc. Natl Acad. Sci. USA</i> <b>112</b>, 1595–1600 (2015).</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1073%2Fpnas.1417219112" aria-label="View reference 65">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 65 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Ventral%20striatal%20dopamine%20reflects%20behavioral%20and%20neural%20signatures%20of%20model-based%20control%20during%20sequential%20decision%20making&amp;journal=Proc.%20Natl%20Acad.%20Sci.%20USA&amp;volume=112&amp;pages=1595-1600&amp;publication_year=2015&amp;author=Deserno%2CL">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="HF. Harlow, " /><meta itemprop="datePublished" content="1949" /><meta itemprop="headline" content="Harlow, H. F. The formation of learning sets. Psychol. Rev. 56, 51–65 (1949)." /><span class="c-article-references__counter">66.</span><p class="c-article-references__text" id="ref-CR66">Harlow, H. F. The formation of learning sets. <i>Psychol. Rev.</i> <b>56</b>, 51–65 (1949).</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1037%2Fh0062474" aria-label="View reference 66">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 66 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20formation%20of%20learning%20sets&amp;journal=Psychol.%20Rev.&amp;volume=56&amp;pages=51-65&amp;publication_year=1949&amp;author=Harlow%2CHF">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="SD. Iversen, M. Mishkin, " /><meta itemprop="datePublished" content="1970" /><meta itemprop="headline" content="Iversen, S. D. &amp; Mishkin, M. Perseverative interference in monkeys following selective lesions of the inferior" /><span class="c-article-references__counter">67.</span><p class="c-article-references__text" id="ref-CR67">Iversen, S. D. &amp; Mishkin, M. Perseverative interference in monkeys following selective lesions of the inferior prefrontal convexity. <i>Exp. Brain Res.</i> <b>11</b>, 376–386 (1970).</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1007%2FBF00237911" aria-label="View reference 67">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 67 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Perseverative%20interference%20in%20monkeys%20following%20selective%20lesions%20of%20the%20inferior%20prefrontal%20convexity&amp;journal=Exp.%20Brain%20Res.&amp;volume=11&amp;pages=376-386&amp;publication_year=1970&amp;author=Iversen%2CSD&amp;author=Mishkin%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="AI. Jang, " /><meta itemprop="datePublished" content="2015" /><meta itemprop="headline" content="Jang, A. I. et al. The role of frontal cortical and medial-temporal lobe brain areas in learning a Bayesian pr" /><span class="c-article-references__counter">68.</span><p class="c-article-references__text" id="ref-CR68">Jang, A. I. et al. The role of frontal cortical and medial-temporal lobe brain areas in learning a Bayesian prior belief on reversals. <i>J. Neurosci.</i> <b>35</b>, 11751–11760 (2015).</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1523%2FJNEUROSCI.1594-15.2015" aria-label="View reference 68">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 68 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20role%20of%20frontal%20cortical%20and%20medial-temporal%20lobe%20brain%20areas%20in%20learning%20a%20Bayesian%20prior%20belief%20on%20reversals&amp;journal=J.%20Neurosci.&amp;volume=35&amp;pages=11751-11760&amp;publication_year=2015&amp;author=Jang%2CAI">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="JX. Wang, " /><meta itemprop="datePublished" content="2018" /><meta itemprop="headline" content="Wang, J. X. et al. Prefrontal cortex as a meta-reinforcement learning system. Nat. Neurosci. 21, 860–868 (2018" /><span class="c-article-references__counter">69.</span><p class="c-article-references__text" id="ref-CR69">Wang, J. X. et al. Prefrontal cortex as a meta-reinforcement learning system. <i>Nat. Neurosci.</i> <b>21</b>, 860–868 (2018).</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1038%2Fs41593-018-0147-8" aria-label="View reference 69">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 69 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Prefrontal%20cortex%20as%20a%20meta-reinforcement%20learning%20system&amp;journal=Nat.%20Neurosci.&amp;volume=21&amp;pages=860-868&amp;publication_year=2018&amp;author=Wang%2CJX">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="RC. Wilson, YK. Takahashi, G. Schoenbaum, Y. Niv, " /><meta itemprop="datePublished" content="2014" /><meta itemprop="headline" content="Wilson, R. C., Takahashi, Y. K., Schoenbaum, G. &amp; Niv, Y. Orbitofrontal cortex as a cognitive map of task spac" /><span class="c-article-references__counter">70.</span><p class="c-article-references__text" id="ref-CR70">Wilson, R. C., Takahashi, Y. K., Schoenbaum, G. &amp; Niv, Y. Orbitofrontal cortex as a cognitive map of task space. <i>Neuron</i> <b>81</b>, 267–279 (2014).</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.neuron.2013.11.005" aria-label="View reference 70">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 70 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Orbitofrontal%20cortex%20as%20a%20cognitive%20map%20of%20task%20space&amp;journal=Neuron&amp;volume=81&amp;pages=267-279&amp;publication_year=2014&amp;author=Wilson%2CRC&amp;author=Takahashi%2CYK&amp;author=Schoenbaum%2CG&amp;author=Niv%2CY">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="NW. Schuck, MB. Cai, RC. Wilson, Y. Niv, " /><meta itemprop="datePublished" content="2016" /><meta itemprop="headline" content="Schuck, N. W., Cai, M. B., Wilson, R. C. &amp; Niv, Y. Human orbitofrontal cortex represents a cognitive map of st" /><span class="c-article-references__counter">71.</span><p class="c-article-references__text" id="ref-CR71">Schuck, N. W., Cai, M. B., Wilson, R. C. &amp; Niv, Y. Human orbitofrontal cortex represents a cognitive map of state space. <i>Neuron</i> <b>91</b>, 1402–1412 (2016).</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.neuron.2016.08.019" aria-label="View reference 71">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 71 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Human%20orbitofrontal%20cortex%20represents%20a%20cognitive%20map%20of%20state%20space&amp;journal=Neuron&amp;volume=91&amp;pages=1402-1412&amp;publication_year=2016&amp;author=Schuck%2CNW&amp;author=Cai%2CMB&amp;author=Wilson%2CRC&amp;author=Niv%2CY">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="DeGroot, M. H. Optimal Statistical Decisions (Wiley, Hoboken, 1970)." /><span class="c-article-references__counter">72.</span><p class="c-article-references__text" id="ref-CR72">DeGroot, M. H. <i>Optimal Statistical Decisions</i> (Wiley, Hoboken, 1970).</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="CK. Starkweather, BM. Babayan, N. Uchida, SJ. Gershman, " /><meta itemprop="datePublished" content="2017" /><meta itemprop="headline" content="Starkweather, C. K., Babayan, B. M., Uchida, N. &amp; Gershman, S. J. Dopamine reward prediction errors reflect hi" /><span class="c-article-references__counter">73.</span><p class="c-article-references__text" id="ref-CR73">Starkweather, C. K., Babayan, B. M., Uchida, N. &amp; Gershman, S. J. Dopamine reward prediction errors reflect hidden-state inference across time. <i>Nat. Neurosci.</i> <b>20</b>, 581–589 (2017).</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1038%2Fnn.4520" aria-label="View reference 73">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 73 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Dopamine%20reward%20prediction%20errors%20reflect%20hidden-state%20inference%20across%20time&amp;journal=Nat.%20Neurosci.&amp;volume=20&amp;pages=581-589&amp;publication_year=2017&amp;author=Starkweather%2CCK&amp;author=Babayan%2CBM&amp;author=Uchida%2CN&amp;author=Gershman%2CSJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="CK. Starkweather, SJ. Gershman, N. Uchida, " /><meta itemprop="datePublished" content="2018" /><meta itemprop="headline" content="Starkweather, C. K., Gershman, S. J. &amp; Uchida, N. The medial prefrontal cortex shapes dopamine reward predicti" /><span class="c-article-references__counter">74.</span><p class="c-article-references__text" id="ref-CR74">Starkweather, C. K., Gershman, S. J. &amp; Uchida, N. The medial prefrontal cortex shapes dopamine reward prediction errors under state uncertainty. <i>Neuron</i> <b>98</b>, 616–629 (2018).</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.neuron.2018.03.036" aria-label="View reference 74">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 74 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20medial%20prefrontal%20cortex%20shapes%20dopamine%20reward%20prediction%20errors%20under%20state%20uncertainty&amp;journal=Neuron&amp;volume=98&amp;pages=616-629&amp;publication_year=2018&amp;author=Starkweather%2CCK&amp;author=Gershman%2CSJ&amp;author=Uchida%2CN">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="XJ. Wang, " /><meta itemprop="datePublished" content="1999" /><meta itemprop="headline" content="Wang, X. J. Synaptic basis of cortical persistent activity: the importance of NMDA receptors to working memory" /><span class="c-article-references__counter">75.</span><p class="c-article-references__text" id="ref-CR75">Wang, X. J. Synaptic basis of cortical persistent activity: the importance of NMDA receptors to working memory. <i>J. Neurosci.</i> <b>19</b>, 9587–9603 (1999).</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1523%2FJNEUROSCI.19-21-09587.1999" aria-label="View reference 75">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 75 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Synaptic%20basis%20of%20cortical%20persistent%20activity%3A%20the%20importance%20of%20NMDA%20receptors%20to%20working%20memory&amp;journal=J.%20Neurosci.&amp;volume=19&amp;pages=9587-9603&amp;publication_year=1999&amp;author=Wang%2CXJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Schöner, G. in The Cambridge Handbook of Computational Psychology (ed. Sun, R.) 101–126 (Cambridge Univ. Press" /><span class="c-article-references__counter">76.</span><p class="c-article-references__text" id="ref-CR76">Schöner, G. in <i>The Cambridge Handbook of Computational Psychology</i> (ed. Sun, R.) 101–126 (Cambridge Univ. Press, Cambridge, 2008).</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Averbeck, B. B. Theory of choice in bandit, information sampling and foraging tasks. PLoS Comput. Biol. 11, e1" /><span class="c-article-references__counter">77.</span><p class="c-article-references__text" id="ref-CR77">Averbeck, B. B. Theory of choice in bandit, information sampling and foraging tasks. <i>PLoS Comput. Biol.</i> <b>11</b>, e1004164 (2015).</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="AR. Otto, CM. Raio, A. Chiang, EA. Phelps, ND. Daw, " /><meta itemprop="datePublished" content="2013" /><meta itemprop="headline" content="Otto, A. R., Raio, C. M., Chiang, A., Phelps, E. A. &amp; Daw, N. D. Working-memory capacity protects model-based " /><span class="c-article-references__counter">78.</span><p class="c-article-references__text" id="ref-CR78">Otto, A. R., Raio, C. M., Chiang, A., Phelps, E. A. &amp; Daw, N. D. Working-memory capacity protects model-based learning from stress. <i>Proc. Natl Acad. Sci. USA</i> <b>110</b>, 20941–20946 (2013).</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1073%2Fpnas.1312011110" aria-label="View reference 78">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 78 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Working-memory%20capacity%20protects%20model-based%20learning%20from%20stress&amp;journal=Proc.%20Natl%20Acad.%20Sci.%20USA&amp;volume=110&amp;pages=20941-20946&amp;publication_year=2013&amp;author=Otto%2CAR&amp;author=Raio%2CCM&amp;author=Chiang%2CA&amp;author=Phelps%2CEA&amp;author=Daw%2CND">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="T. Akam, R. Costa, P. Dayan, " /><meta itemprop="datePublished" content="2015" /><meta itemprop="headline" content="Akam, T., Costa, R. &amp; Dayan, P. Simple plans or sophisticated habits? State, transition and learning interacti" /><span class="c-article-references__counter">79.</span><p class="c-article-references__text" id="ref-CR79">Akam, T., Costa, R. &amp; Dayan, P. Simple plans or sophisticated habits? State, transition and learning interactions in the two-step task. <i>PLoS. Comput. Biol.</i> <b>11</b>, e1004648 (2015).</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1371%2Fjournal.pcbi.1004648" aria-label="View reference 79">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 79 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Simple%20plans%20or%20sophisticated%20habits%3F%20State%2C%20transition%20and%20learning%20interactions%20in%20the%20two-step%20task&amp;journal=PLoS.%20Comput.%20Biol.&amp;volume=11&amp;publication_year=2015&amp;author=Akam%2CT&amp;author=Costa%2CR&amp;author=Dayan%2CP">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="M. Riesenhuber, T. Poggio, " /><meta itemprop="datePublished" content="1999" /><meta itemprop="headline" content="Riesenhuber, M. &amp; Poggio, T. Hierarchical models of object recognition in cortex. Nat. Neurosci. 2, 1019–1025 " /><span class="c-article-references__counter">80.</span><p class="c-article-references__text" id="ref-CR80">Riesenhuber, M. &amp; Poggio, T. Hierarchical models of object recognition in cortex. <i>Nat. Neurosci.</i> <b>2</b>, 1019–1025 (1999).</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1038%2F14819" aria-label="View reference 80">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 80 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Hierarchical%20models%20of%20object%20recognition%20in%20cortex&amp;journal=Nat.%20Neurosci.&amp;volume=2&amp;pages=1019-1025&amp;publication_year=1999&amp;author=Riesenhuber%2CM&amp;author=Poggio%2CT">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="RJ. Williams, " /><meta itemprop="datePublished" content="1992" /><meta itemprop="headline" content="Williams, R. J. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Mac" /><span class="c-article-references__counter">81.</span><p class="c-article-references__text" id="ref-CR81">Williams, R. J. Simple statistical gradient-following algorithms for connectionist reinforcement learning. <i>Mach. Learn.</i> <b>8</b>, 229–256 (1992).</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.emis.de/MATH-item?0772.68076" aria-label="View reference 81 on MATH">MATH</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 81 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Simple%20statistical%20gradient-following%20algorithms%20for%20connectionist%20reinforcement%20learning&amp;journal=Mach.%20Learn.&amp;volume=8&amp;pages=229-256&amp;publication_year=1992&amp;author=Williams%2CRJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="G. Tesauro, " /><meta itemprop="datePublished" content="1995" /><meta itemprop="headline" content="Tesauro, G. Temporal difference learning and TD-Gammon. Commun. ACM 38, 58–68 (1995)." /><span class="c-article-references__counter">82.</span><p class="c-article-references__text" id="ref-CR82">Tesauro, G. Temporal difference learning and TD-Gammon. <i>Commun. ACM</i> <b>38</b>, 58–68 (1995).</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1145%2F203330.203343" aria-label="View reference 82">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 82 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Temporal%20difference%20learning%20and%20TD-Gammon&amp;journal=Commun.%20ACM&amp;volume=38&amp;pages=58-68&amp;publication_year=1995&amp;author=Tesauro%2CG">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Pomerleau, D. A. ALVINN: An autonomous land vehicle in a neural network. In Advances in Neural Information Pro" /><span class="c-article-references__counter">83.</span><p class="c-article-references__text" id="ref-CR83">Pomerleau, D. A. ALVINN: An autonomous land vehicle in a neural network. In <i>Advances in Neural Information Processing Systems</i> Vol. 1 (ed. Touretzky, D. S.) (NIPS, 1988).</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="S. Levine, C. Finn, T. Darrell, P. Abbeel, " /><meta itemprop="datePublished" content="2016" /><meta itemprop="headline" content="Levine, S., Finn, C., Darrell, T. &amp; Abbeel, P. End-to-end training of deep visuomotor policies. J. Mach. Learn" /><span class="c-article-references__counter">84.</span><p class="c-article-references__text" id="ref-CR84">Levine, S., Finn, C., Darrell, T. &amp; Abbeel, P. End-to-end training of deep visuomotor policies. <i>J. Mach. Learn. Res.</i> <b>17</b>, 1334–1373 (2016).</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.ams.org/mathscinet-getitem?mr=3491133" aria-label="View reference 84 on MathSciNet">MathSciNet</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.emis.de/MATH-item?1360.68687" aria-label="View reference 84 on MATH">MATH</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 84 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=End-to-end%20training%20of%20deep%20visuomotor%20policies&amp;journal=J.%20Mach.%20Learn.%20Res.&amp;volume=17&amp;pages=1334-1373&amp;publication_year=2016&amp;author=Levine%2CS&amp;author=Finn%2CC&amp;author=Darrell%2CT&amp;author=Abbeel%2CP">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Gu, S., Lillicrap, T., Sutskever, I. &amp; Levine, S. Continuous deep Q-learning with model-based acceleration. In" /><span class="c-article-references__counter">85.</span><p class="c-article-references__text" id="ref-CR85">Gu, S., Lillicrap, T., Sutskever, I. &amp; Levine, S. Continuous deep Q-learning with model-based acceleration. In <i>Proc. 33rd International Conference on Machine Learning</i> Vol. 48 2829–2838 (PMLR, 2016).</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Burda, Y., Edwards, H., Storkey, A. &amp; Klimov, O. Exploration by random network distillation. Preprint at https" /><span class="c-article-references__counter">86.</span><p class="c-article-references__text" id="ref-CR86">Burda, Y., Edwards, H., Storkey, A. &amp; Klimov, O. Exploration by random network distillation. Preprint at <a href="https://arxiv.org/abs/1810.12894">https://arxiv.org/abs/1810.12894</a> (2018).</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Vezhnevets, A. S. et al. Feudal networks for hierarchical reinforcement learning. Preprint at https://arxiv.or" /><span class="c-article-references__counter">87.</span><p class="c-article-references__text" id="ref-CR87">Vezhnevets, A. S. et al. Feudal networks for hierarchical reinforcement learning. Preprint at <a href="https://arxiv.org/abs/1703.01161">https://arxiv.org/abs/1703.01161</a> (2017).</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="EO. Neftci, " /><meta itemprop="datePublished" content="2018" /><meta itemprop="headline" content="Neftci, E. O. Data and power efficient intelligence with neuromorphic learning machines. iScience 5, 52–68 (20" /><span class="c-article-references__counter">88.</span><p class="c-article-references__text" id="ref-CR88">Neftci, E. O. Data and power efficient intelligence with neuromorphic learning machines. <i>iScience</i> <b>5</b>, 52–68 (2018).</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.isci.2018.06.010" aria-label="View reference 88">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 88 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Data%20and%20power%20efficient%20intelligence%20with%20neuromorphic%20learning%20machines&amp;journal=iScience&amp;volume=5&amp;pages=52-68&amp;publication_year=2018&amp;author=Neftci%2CEO">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Kaiser, J., Mostafa, H. &amp; Neftci, E. O. Synaptic plasticity dynamics for deep continuous local learning. Prepr" /><span class="c-article-references__counter">89.</span><p class="c-article-references__text" id="ref-CR89">Kaiser, J., Mostafa, H. &amp; Neftci, E. O. Synaptic plasticity dynamics for deep continuous local learning. Preprint at <a href="https://arxiv.org/abs/1811.10766">https://arxiv.org/abs/1811.10766</a> (2018).</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="EO. Neftci, C. Augustine, S. Paul, G. Detorakis, " /><meta itemprop="datePublished" content="2017" /><meta itemprop="headline" content="Neftci, E. O., Augustine, C., Paul, S. &amp; Detorakis, G. Event-driven random back-propagation: enabling neuromor" /><span class="c-article-references__counter">90.</span><p class="c-article-references__text" id="ref-CR90">Neftci, E. O., Augustine, C., Paul, S. &amp; Detorakis, G. Event-driven random back-propagation: enabling neuromorphic deep learning machines. <i>Front. Neurosci.</i> <b>11</b>, 324 (2017).</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.3389%2Ffnins.2017.00324" aria-label="View reference 90">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 90 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Event-driven%20random%20back-propagation%3A%20enabling%20neuromorphic%20deep%20learning%20machines&amp;journal=Front.%20Neurosci.&amp;volume=11&amp;publication_year=2017&amp;author=Neftci%2CEO&amp;author=Augustine%2CC&amp;author=Paul%2CS&amp;author=Detorakis%2CG">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="TP. Lillicrap, D. Cownden, DB. Tweed, CJ. Akerman, " /><meta itemprop="datePublished" content="2016" /><meta itemprop="headline" content="Lillicrap, T. P., Cownden, D., Tweed, D. B. &amp; Akerman, C. J. Random synaptic feedback weights support error ba" /><span class="c-article-references__counter">91.</span><p class="c-article-references__text" id="ref-CR91">Lillicrap, T. P., Cownden, D., Tweed, D. B. &amp; Akerman, C. J. Random synaptic feedback weights support error backpropagation for deep learning. <i>Nat. Commun.</i> <b>7</b>, 13276 (2016).</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1038%2Fncomms13276" aria-label="View reference 91">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 91 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Random%20synaptic%20feedback%20weights%20support%20error%20backpropagation%20for%20deep%20learning&amp;journal=Nat.%20Commun.&amp;volume=7&amp;publication_year=2016&amp;author=Lillicrap%2CTP&amp;author=Cownden%2CD&amp;author=Tweed%2CDB&amp;author=Akerman%2CCJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Levine, S., Pastor, P., Krizhevsky, A., Ibarz, J. &amp; Quillen, D. Learning hand-eye coordination for robotic gra" /><span class="c-article-references__counter">92.</span><p class="c-article-references__text" id="ref-CR92">Levine, S., Pastor, P., Krizhevsky, A., Ibarz, J. &amp; Quillen, D. Learning hand-eye coordination for robotic grasping with deep learning and large-scale data collection. <i>Int. J. Robot. Res.</i> <b>37</b>, 421–436 (2018).</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Blundell, C. et al. Model-free episodic control. Preprint at https://arxiv.org/abs/1606.04460 (2016)." /><span class="c-article-references__counter">93.</span><p class="c-article-references__text" id="ref-CR93">Blundell, C. et al. Model-free episodic control. Preprint at <a href="https://arxiv.org/abs/1606.04460">https://arxiv.org/abs/1606.04460</a> (2016).</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="SJ. Gershman, ND. Daw, " /><meta itemprop="datePublished" content="2017" /><meta itemprop="headline" content="Gershman, S. J. &amp; Daw, N. D. Reinforcement learning and episodic memory in humans and animals: an integrative " /><span class="c-article-references__counter">94.</span><p class="c-article-references__text" id="ref-CR94">Gershman, S. J. &amp; Daw, N. D. Reinforcement learning and episodic memory in humans and animals: an integrative framework. <i>Ann. Rev. Psychol.</i> <b>68</b>, 101–128 (2017).</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1146%2Fannurev-psych-122414-033625" aria-label="View reference 94">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 94 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Reinforcement%20learning%20and%20episodic%20memory%20in%20humans%20and%20animals%3A%20an%20integrative%20framework&amp;journal=Ann.%20Rev.%20Psychol.&amp;volume=68&amp;pages=101-128&amp;publication_year=2017&amp;author=Gershman%2CSJ&amp;author=Daw%2CND">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Finn, C., Abbeel, P. &amp; Levine, S. Model-agnostic meta-learning for fast adaptation of deep networks. Preprint " /><span class="c-article-references__counter">95.</span><p class="c-article-references__text" id="ref-CR95">Finn, C., Abbeel, P. &amp; Levine, S. Model-agnostic meta-learning for fast adaptation of deep networks. Preprint at <a href="https://arxiv.org/abs/1703.03400">https://arxiv.org/abs/1703.03400</a> (2017).</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Ha, D. &amp; Schmidhuber, J. World models. Preprint at https://arxiv.org/abs/1803.10122 (2018)." /><span class="c-article-references__counter">96.</span><p class="c-article-references__text" id="ref-CR96">Ha, D. &amp; Schmidhuber, J. World models. Preprint at <a href="https://arxiv.org/abs/1803.10122">https://arxiv.org/abs/1803.10122</a> (2018).</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Zambaldi, V. et al. Relational deep reinforcement learning. Preprint at https://arxiv.org/abs/1806.01830 (2018" /><span class="c-article-references__counter">97.</span><p class="c-article-references__text" id="ref-CR97">Zambaldi, V. et al. Relational deep reinforcement learning. Preprint at <a href="https://arxiv.org/abs/1806.01830">https://arxiv.org/abs/1806.01830</a> (2018).</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Daw, N. D., O’Doherty, J. P., Dayan, P., Seymour, B. &amp; Dolan, R. J. Cortical substrates for exploratory decisi" /><span class="c-article-references__counter">98.</span><p class="c-article-references__text" id="ref-CR98">Daw, N. D., O’Doherty, J. P., Dayan, P., Seymour, B. &amp; Dolan, R. J. Cortical substrates for exploratory decisions in humans. <i>Nature</i> <b>441</b>, 876–879 (2006).</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Kriegeskorte, N. Deep neural networks: a new framework for modeling biological vision and brain information pr" /><span class="c-article-references__counter">99.</span><p class="c-article-references__text" id="ref-CR99">Kriegeskorte, N. Deep neural networks: a new framework for modeling biological vision and brain information processing. <i>Annu. Rev. Vision Sci.</i> <b>1</b>, 417–446 (2015).</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Bernacchia, A., Seo, H., Lee, D. &amp; Wang, X. J. A reservoir of time constants for memory traces in cortical neu" /><span class="c-article-references__counter">100.</span><p class="c-article-references__text" id="ref-CR100">Bernacchia, A., Seo, H., Lee, D. &amp; Wang, X. J. A reservoir of time constants for memory traces in cortical neurons. <i>Nat. Neurosci.</i> <b>14</b>, 366–372 (2011).</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Walton, M. E., Behrens, T. E., Buckley, M. J., Rudebeck, P. H. &amp; Rushworth, M. F. Separable learning systems i" /><span class="c-article-references__counter">101.</span><p class="c-article-references__text" id="ref-CR101">Walton, M. E., Behrens, T. E., Buckley, M. J., Rudebeck, P. H. &amp; Rushworth, M. F. Separable learning systems in the macaque brain and the role of orbitofrontal cortex in contingent learning. <i>Neuron</i> <b>65</b>, 927–939 (2010).</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="S. Iglesias, " /><meta itemprop="datePublished" content="2013" /><meta itemprop="headline" content="Iglesias, S. et al. Hierarchical prediction errors in midbrain and basal forebrain during sensory learning. Ne" /><span class="c-article-references__counter">102.</span><p class="c-article-references__text" id="ref-CR102">Iglesias, S. et al. Hierarchical prediction errors in midbrain and basal forebrain during sensory learning. <i>Neuron</i> <b>80</b>, 519–530 (2013).</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.neuron.2013.09.009" aria-label="View reference 102">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 102 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Hierarchical%20prediction%20errors%20in%20midbrain%20and%20basal%20forebrain%20during%20sensory%20learning&amp;journal=Neuron&amp;volume=80&amp;pages=519-530&amp;publication_year=2013&amp;author=Iglesias%2CS">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="D. Badre, MJ. Frank, " /><meta itemprop="datePublished" content="2012" /><meta itemprop="headline" content="Badre, D. &amp; Frank, M. J. Mechanisms of hierarchical reinforcement learning in cortico-striatal circuits 2: evi" /><span class="c-article-references__counter">103.</span><p class="c-article-references__text" id="ref-CR103">Badre, D. &amp; Frank, M. J. Mechanisms of hierarchical reinforcement learning in cortico-striatal circuits 2: evidence from fMRI. <i>Cereb. Cortex</i> <b>22</b>, 527–536 (2012).</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1093%2Fcercor%2Fbhr117" aria-label="View reference 103">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 103 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Mechanisms%20of%20hierarchical%20reinforcement%20learning%20in%20cortico-striatal%20circuits%202%3A%20evidence%20from%20fMRI&amp;journal=Cereb.%20Cortex&amp;volume=22&amp;pages=527-536&amp;publication_year=2012&amp;author=Badre%2CD&amp;author=Frank%2CMJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="MJ. Frank, D. Badre, " /><meta itemprop="datePublished" content="2012" /><meta itemprop="headline" content="Frank, M. J. &amp; Badre, D. Mechanisms of hierarchical reinforcement learning in corticostriatal circuits 1: comp" /><span class="c-article-references__counter">104.</span><p class="c-article-references__text" id="ref-CR104">Frank, M. J. &amp; Badre, D. Mechanisms of hierarchical reinforcement learning in corticostriatal circuits 1: computational analysis. <i>Cereb. Cortex</i> <b>22</b>, 509–526 (2012).</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1093%2Fcercor%2Fbhr114" aria-label="View reference 104">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 104 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Mechanisms%20of%20hierarchical%20reinforcement%20learning%20in%20corticostriatal%20circuits%201%3A%20computational%20analysis&amp;journal=Cereb.%20Cortex&amp;volume=22&amp;pages=509-526&amp;publication_year=2012&amp;author=Frank%2CMJ&amp;author=Badre%2CD">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="MM. Botvinick, " /><meta itemprop="datePublished" content="2008" /><meta itemprop="headline" content="Botvinick, M. M. Hierarchical models of behavior and prefrontal function. Trends Cogn. Sci. 12, 201–208 (2008)" /><span class="c-article-references__counter">105.</span><p class="c-article-references__text" id="ref-CR105">Botvinick, M. M. Hierarchical models of behavior and prefrontal function. <i>Trends Cogn. Sci.</i> <b>12</b>, 201–208 (2008).</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.tics.2008.02.009" aria-label="View reference 105">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 105 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Hierarchical%20models%20of%20behavior%20and%20prefrontal%20function&amp;journal=Trends%20Cogn.%20Sci.&amp;volume=12&amp;pages=201-208&amp;publication_year=2008&amp;author=Botvinick%2CMM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="MM. Botvinick, Y. Niv, AC. Barto, " /><meta itemprop="datePublished" content="2009" /><meta itemprop="headline" content="Botvinick, M. M., Niv, Y. &amp; Barto, A. C. Hierarchically organized behavior and its neural foundations: a reinf" /><span class="c-article-references__counter">106.</span><p class="c-article-references__text" id="ref-CR106">Botvinick, M. M., Niv, Y. &amp; Barto, A. C. Hierarchically organized behavior and its neural foundations: a reinforcement learning perspective. <i>Cognition</i> <b>113</b>, 262–280 (2009).</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.cognition.2008.08.011" aria-label="View reference 106">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 106 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Hierarchically%20organized%20behavior%20and%20its%20neural%20foundations%3A%20a%20reinforcement%20learning%20perspective&amp;journal=Cognition&amp;volume=113&amp;pages=262-280&amp;publication_year=2009&amp;author=Botvinick%2CMM&amp;author=Niv%2CY&amp;author=Barto%2CAC">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="JJ. Ribas-Fernandes, " /><meta itemprop="datePublished" content="2011" /><meta itemprop="headline" content="Ribas-Fernandes, J. J. et al. A neural signature of hierarchical reinforcement learning. Neuron 71, 370–379 (2" /><span class="c-article-references__counter">107.</span><p class="c-article-references__text" id="ref-CR107">Ribas-Fernandes, J. J. et al. A neural signature of hierarchical reinforcement learning. <i>Neuron</i> <b>71</b>, 370–379 (2011).</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.neuron.2011.05.042" aria-label="View reference 107">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 107 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20neural%20signature%20of%20hierarchical%20reinforcement%20learning&amp;journal=Neuron&amp;volume=71&amp;pages=370-379&amp;publication_year=2011&amp;author=Ribas-Fernandes%2CJJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="MM. Botvinick, " /><meta itemprop="datePublished" content="2012" /><meta itemprop="headline" content="Botvinick, M. M. Hierarchical reinforcement learning and decision making. Curr. Opin. Neurobiol. 22, 956–962 (" /><span class="c-article-references__counter">108.</span><p class="c-article-references__text" id="ref-CR108">Botvinick, M. M. Hierarchical reinforcement learning and decision making. <i>Curr. Opin. Neurobiol.</i> <b>22</b>, 956–962 (2012).</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.conb.2012.05.008" aria-label="View reference 108">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 108 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Hierarchical%20reinforcement%20learning%20and%20decision%20making&amp;journal=Curr.%20Opin.%20Neurobiol.&amp;volume=22&amp;pages=956-962&amp;publication_year=2012&amp;author=Botvinick%2CMM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="M. Botvinick, A. Weinstein, " /><meta itemprop="datePublished" content="2014" /><meta itemprop="headline" content="Botvinick, M. &amp; Weinstein, A. Model-based hierarchical reinforcement learning and human action control. Philos" /><span class="c-article-references__counter">109.</span><p class="c-article-references__text" id="ref-CR109">Botvinick, M. &amp; Weinstein, A. Model-based hierarchical reinforcement learning and human action control. <i>Philos. Trans. R. Soc. Lond. B</i> <b>369</b>, 20130480 (2014).</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1098%2Frstb.2013.0480" aria-label="View reference 109">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 109 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Model-based%20hierarchical%20reinforcement%20learning%20and%20human%20action%20control&amp;journal=Philos.%20Trans.%20R.%20Soc.%20Lond.%20B&amp;volume=369&amp;publication_year=2014&amp;author=Botvinick%2CM&amp;author=Weinstein%2CA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Dayan, P. &amp; Hinton, G. E. Feudal reinforcement learning. In Advances in Neural Information Processing Systems " /><span class="c-article-references__counter">110.</span><p class="c-article-references__text" id="ref-CR110">Dayan, P. &amp; Hinton, G. E. Feudal reinforcement learning. In <i>Advances in Neural Information Processing Systems</i> Vol. 5 (eds Hanson, S. J., Cowan, J. D. &amp; Giles, C. L.) 271–278 (NIPS, 1992).</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="RS. Sutton, D. Precup, S. Singh, " /><meta itemprop="datePublished" content="1999" /><meta itemprop="headline" content="Sutton, R. S., Precup, D. &amp; Singh, S. Between MDPs and semi-MDPs: A framework for temporal abstraction in rein" /><span class="c-article-references__counter">111.</span><p class="c-article-references__text" id="ref-CR111">Sutton, R. S., Precup, D. &amp; Singh, S. Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning. <i>Artif. Intell.</i> <b>112</b>, 181–211 (1999).</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.ams.org/mathscinet-getitem?mr=1716644" aria-label="View reference 111 on MathSciNet">MathSciNet</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.emis.de/MATH-item?0996.68151" aria-label="View reference 111 on MATH">MATH</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2FS0004-3702%2899%2900052-1" aria-label="View reference 111">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 111 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Between%20MDPs%20and%20semi-MDPs%3A%20A%20framework%20for%20temporal%20abstraction%20in%20reinforcement%20learning&amp;journal=Artif.%20Intell.&amp;volume=112&amp;pages=181-211&amp;publication_year=1999&amp;author=Sutton%2CRS&amp;author=Precup%2CD&amp;author=Singh%2CS">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Bacon, P. L., Harb, J. &amp; Precup, D. The option-critic architecture. Proc. Thirty-First AAAI Conference on Arti" /><span class="c-article-references__counter">112.</span><p class="c-article-references__text" id="ref-CR112">Bacon, P. L., Harb, J. &amp; Precup, D. The option-critic architecture. <i>Proc. Thirty-First AAAI Conference on Artificial Intelligence</i> 1726–1734 (AAAI, 2017).</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Jaderberg, M. et al. Reinforcement learning with unsupervised auxiliary tasks. Preprint at https://arxiv.org/a" /><span class="c-article-references__counter">113.</span><p class="c-article-references__text" id="ref-CR113">Jaderberg, M. et al. Reinforcement learning with unsupervised auxiliary tasks. Preprint at <a href="https://arxiv.org/abs/1611.05397">https://arxiv.org/abs/1611.05397</a> (2016).</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Friston, K. The free-energy principle: a unified brain theory? Nat. Rev. Neurosci. 11, 127–138 (2010)." /><span class="c-article-references__counter">114.</span><p class="c-article-references__text" id="ref-CR114">Friston, K. The free-energy principle: a unified brain theory? <i>Nat. Rev. Neurosci.</i> <b>11</b>, 127–138 (2010).</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Ross, S., Gordon, G. J. &amp; Bagnell, J. A. A reduction of imitation learning and structured prediction to no-reg" /><span class="c-article-references__counter">115.</span><p class="c-article-references__text" id="ref-CR115">Ross, S., Gordon, G. J. &amp; Bagnell, J. A. A reduction of imitation learning and structured prediction to no-regret online learning. Preprint at <a href="https://arxiv.org/abs/1011.0686">https://arxiv.org/abs/1011.0686</a> (2010).</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Le, H. M. et al. Hierarchical imitation and reinforcement learning. Preprint at https://arxiv.org/abs/1803.005" /><span class="c-article-references__counter">116.</span><p class="c-article-references__text" id="ref-CR116">Le, H. M. et al. Hierarchical imitation and reinforcement learning. Preprint at <a href="https://arxiv.org/abs/1803.00590">https://arxiv.org/abs/1803.00590</a> (2018).</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="E. Koechlin, C. Ody, F. Kouneiher, " /><meta itemprop="datePublished" content="2003" /><meta itemprop="headline" content="Koechlin, E., Ody, C. &amp; Kouneiher, F. The architecture of cognitive control in the human prefrontal cortex. Sc" /><span class="c-article-references__counter">117.</span><p class="c-article-references__text" id="ref-CR117">Koechlin, E., Ody, C. &amp; Kouneiher, F. The architecture of cognitive control in the human prefrontal cortex. <i>Science</i> <b>302</b>, 1181–1185 (2003).</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1126%2Fscience.1088545" aria-label="View reference 117">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 117 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20architecture%20of%20cognitive%20control%20in%20the%20human%20prefrontal%20cortex&amp;journal=Science&amp;volume=302&amp;pages=1181-1185&amp;publication_year=2003&amp;author=Koechlin%2CE&amp;author=Ody%2CC&amp;author=Kouneiher%2CF">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="D. Badre, M. D’Esposito, " /><meta itemprop="datePublished" content="2007" /><meta itemprop="headline" content="Badre, D. &amp; D’Esposito, M. Functional magnetic resonance imaging evidence for a hierarchical organization of t" /><span class="c-article-references__counter">118.</span><p class="c-article-references__text" id="ref-CR118">Badre, D. &amp; D’Esposito, M. Functional magnetic resonance imaging evidence for a hierarchical organization of the prefrontal cortex. <i>J. Cogn. Neurosci.</i> <b>19</b>, 2082–2099 (2007).</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1162%2Fjocn.2007.19.12.2082" aria-label="View reference 118">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 118 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Functional%20magnetic%20resonance%20imaging%20evidence%20for%20a%20hierarchical%20organization%20of%20the%20prefrontal%20cortex&amp;journal=J.%20Cogn.%20Neurosci.&amp;volume=19&amp;pages=2082-2099&amp;publication_year=2007&amp;author=Badre%2CD&amp;author=D%E2%80%99Esposito%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="D. Muessgens, N. Thirugnanasambandam, H. Shitara, T. Popa, M. Hallett, " /><meta itemprop="datePublished" content="2016" /><meta itemprop="headline" content="Muessgens, D., Thirugnanasambandam, N., Shitara, H., Popa, T. &amp; Hallett, M. Dissociable roles of preSMA in mot" /><span class="c-article-references__counter">119.</span><p class="c-article-references__text" id="ref-CR119">Muessgens, D., Thirugnanasambandam, N., Shitara, H., Popa, T. &amp; Hallett, M. Dissociable roles of preSMA in motor sequence chunking and hand switching—a TMS study. <i>J. Neurophysiol.</i> <b>116</b>, 2637–2646 (2016).</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1152%2Fjn.00565.2016" aria-label="View reference 119">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 119 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Dissociable%20roles%20of%20preSMA%20in%20motor%20sequence%20chunking%20and%20hand%20switching%E2%80%94a%20TMS%20study&amp;journal=J.%20Neurophysiol.&amp;volume=116&amp;pages=2637-2646&amp;publication_year=2016&amp;author=Muessgens%2CD&amp;author=Thirugnanasambandam%2CN&amp;author=Shitara%2CH&amp;author=Popa%2CT&amp;author=Hallett%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Sabour, S., Frosst, N. &amp; Hinton, G. E. Dynamic routing between capsules. In Advances in Neural Information Pro" /><span class="c-article-references__counter">120.</span><p class="c-article-references__text" id="ref-CR120">Sabour, S., Frosst, N. &amp; Hinton, G. E. Dynamic routing between capsules. In <i>Advances in Neural Information Processing Systems</i> Vol. 30 (eds Guyon, I. et al.) 3856–3866 (2017).</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="M. Davies, " /><meta itemprop="datePublished" content="2018" /><meta itemprop="headline" content="Davies, M. et al. Loihi: a neuromorphic manycore processor with on-chip learning. IEEE Micro 38, 82–99 (2018)." /><span class="c-article-references__counter">121.</span><p class="c-article-references__text" id="ref-CR121">Davies, M. et al. Loihi: a neuromorphic manycore processor with on-chip learning. <i>IEEE Micro</i> <b>38</b>, 82–99 (2018).</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FMM.2018.112130359" aria-label="View reference 121">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 121 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Loihi%3A%20a%20neuromorphic%20manycore%20processor%20with%20on-chip%20learning&amp;journal=IEEE%20Micro&amp;volume=38&amp;pages=82-99&amp;publication_year=2018&amp;author=Davies%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Friedmann, S. &amp; Schemmel, J. Demonstrating hybrid learning in a flexible neuromorphic hardware system. Preprin" /><span class="c-article-references__counter">122.</span><p class="c-article-references__text" id="ref-CR122">Friedmann, S. &amp; Schemmel, J. Demonstrating hybrid learning in a flexible neuromorphic hardware system. Preprint at <a href="https://arxiv.org/abs/1604.05080">https://arxiv.org/abs/1604.05080</a> (2016).</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Qiao, N. et al. A reconfigurable on-line learning spiking neuromorphic processor comprising 256 neurons and 12" /><span class="c-article-references__counter">123.</span><p class="c-article-references__text" id="ref-CR123">Qiao, N. et al. A reconfigurable on-line learning spiking neuromorphic processor comprising 256 neurons and 128K synapses. <i>Front. Neurosci.</i> <b>9</b>, 141 (2015).</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="E. Neftci, " /><meta itemprop="datePublished" content="2013" /><meta itemprop="headline" content="Neftci, E. et al. Synthesizing cognition in neuromorphic electronic systems. Proc. Natl Acad. Sci. USA 110, 34" /><span class="c-article-references__counter">124.</span><p class="c-article-references__text" id="ref-CR124">Neftci, E. et al. Synthesizing cognition in neuromorphic electronic systems. <i>Proc. Natl Acad. Sci. USA</i> <b>110</b>, 3468–3476 (2013).</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1073%2Fpnas.1212083110" aria-label="View reference 124">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 124 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Synthesizing%20cognition%20in%20neuromorphic%20electronic%20systems&amp;journal=Proc.%20Natl%20Acad.%20Sci.%20USA&amp;volume=110&amp;pages=3468-3476&amp;publication_year=2013&amp;author=Neftci%2CE">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="S. Friedmann, N. Fremaux, J. Schemmel, W. Gerstner, K. Meier, " /><meta itemprop="datePublished" content="2013" /><meta itemprop="headline" content="Friedmann, S., Fremaux, N., Schemmel, J., Gerstner, W. &amp; Meier, K. Reward-based learning under hardware constr" /><span class="c-article-references__counter">125.</span><p class="c-article-references__text" id="ref-CR125">Friedmann, S., Fremaux, N., Schemmel, J., Gerstner, W. &amp; Meier, K. Reward-based learning under hardware constraints-using a RISC processor embedded in a neuromorphic substrate. <i>Front. Neurosci.</i> <b>7</b>, 160 (2013).</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.3389%2Ffnins.2013.00160" aria-label="View reference 125">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 125 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Reward-based%20learning%20under%20hardware%20constraints-using%20a%20RISC%20processor%20embedded%20in%20a%20neuromorphic%20substrate&amp;journal=Front.%20Neurosci.&amp;volume=7&amp;publication_year=2013&amp;author=Friedmann%2CS&amp;author=Fremaux%2CN&amp;author=Schemmel%2CJ&amp;author=Gerstner%2CW&amp;author=Meier%2CK">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="D. Hassabis, D. Kumaran, C. Summerfield, M. Botvinick, " /><meta itemprop="datePublished" content="2017" /><meta itemprop="headline" content="Hassabis, D., Kumaran, D., Summerfield, C. &amp; Botvinick, M. Neuroscience-inspired artificial intelligence. Neur" /><span class="c-article-references__counter">126.</span><p class="c-article-references__text" id="ref-CR126">Hassabis, D., Kumaran, D., Summerfield, C. &amp; Botvinick, M. Neuroscience-inspired artificial intelligence. <i>Neuron</i> <b>95</b>, 245–258 (2017).</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.neuron.2017.06.011" aria-label="View reference 126">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 126 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Neuroscience-inspired%20artificial%20intelligence&amp;journal=Neuron&amp;volume=95&amp;pages=245-258&amp;publication_year=2017&amp;author=Hassabis%2CD&amp;author=Kumaran%2CD&amp;author=Summerfield%2CC&amp;author=Botvinick%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Courbariaux, M., Bengio, Y. &amp; David, J.-P. Training deep neural networks with low precision multiplications. P" /><span class="c-article-references__counter">127.</span><p class="c-article-references__text" id="ref-CR127">Courbariaux, M., Bengio, Y. &amp; David, J.-P. Training deep neural networks with low precision multiplications. Preprint at <a href="https://arxiv.org/abs/1412.7024">https://arxiv.org/abs/1412.7024</a> (2014).</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="G. Detorakis, " /><meta itemprop="datePublished" content="2018" /><meta itemprop="headline" content="Detorakis, G. et al. Neural and synaptic array transceiver: a brain-inspired computing framework for embedded " /><span class="c-article-references__counter">128.</span><p class="c-article-references__text" id="ref-CR128">Detorakis, G. et al. Neural and synaptic array transceiver: a brain-inspired computing framework for embedded learning. <i>Front. Neurosci.</i> <b>12</b>, 583 (2018).</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.3389%2Ffnins.2018.00583" aria-label="View reference 128">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 128 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Neural%20and%20synaptic%20array%20transceiver%3A%20a%20brain-inspired%20computing%20framework%20for%20embedded%20learning&amp;journal=Front.%20Neurosci.&amp;volume=12&amp;publication_year=2018&amp;author=Detorakis%2CG">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="SC. Liu, T. Delbruck, " /><meta itemprop="datePublished" content="2010" /><meta itemprop="headline" content="Liu, S. C. &amp; Delbruck, T. Neuromorphic sensory systems. Curr. Opin. Neurobiol. 20, 288–295 (2010)." /><span class="c-article-references__counter">129.</span><p class="c-article-references__text" id="ref-CR129">Liu, S. C. &amp; Delbruck, T. Neuromorphic sensory systems. <i>Curr. Opin. Neurobiol.</i> <b>20</b>, 288–295 (2010).</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.conb.2010.03.007" aria-label="View reference 129">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 129 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Neuromorphic%20sensory%20systems&amp;journal=Curr.%20Opin.%20Neurobiol.&amp;volume=20&amp;pages=288-295&amp;publication_year=2010&amp;author=Liu%2CSC&amp;author=Delbruck%2CT">
                    Google Scholar</a> 
                </p></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" href="/articles/s42256-019-0025-4-references.ris">Download references<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p></div></div></div></section><section aria-labelledby="Ack1"><div class="c-article-section" id="Ack1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Ack1">Acknowledgements</h2><div class="c-article-section__content" id="Ack1-content"><p>This work was supported by the Intramural Research Program of the National Institute of Mental Health (ZIA MH002928-01), and by the National Science Foundation under grant 1640081.</p></div></div></section><section aria-labelledby="author-information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><span class="c-article-author-information__subtitle u-visually-hidden" id="author-notes">Author notes</span><ol class="c-article-author-information__list"><li class="c-article-author-information__item" id="na1"><p>These authors contributed equally: Emre O. Neftci, Bruno B. Averbeck.</p></li></ol><h3 class="c-article__sub-heading" id="affiliations">Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">Department of Cognitive Sciences, Department of Computer Science, University of California Irvine, Irvine, CA, USA</p><p class="c-article-author-affiliation__authors-list">Emre O. Neftci</p></li><li id="Aff2"><p class="c-article-author-affiliation__address">Laboratory of Neuropsychology, National Institute of Mental Health, National Institutes of Health, Bethesda, MD, USA</p><p class="c-article-author-affiliation__authors-list">Bruno B. Averbeck</p></li></ol><div class="js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-1"><span class="c-article-authors-search__title u-h3 js-search-name">Emre O. Neftci</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?author=&#34;Emre O.+Neftci&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Emre O.+Neftci" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Emre O.+Neftci%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-2"><span class="c-article-authors-search__title u-h3 js-search-name">Bruno B. Averbeck</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?author=&#34;Bruno B.+Averbeck&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Bruno B.+Averbeck" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Bruno B.+Averbeck%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" rel="nofollow" href="/articles/s42256-019-0025-4/email/correspondent/c1/new">Bruno B. Averbeck</a>.</p></div></div></section><section aria-labelledby="ethics"><div class="c-article-section" id="ethics-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="ethics">Ethics declarations</h2><div class="c-article-section__content" id="ethics-content">
              
                <h3 class="c-article__sub-heading">Competing interests</h3>
                <p>The authors declare no competing interests.</p>
              
            </div></div></section><section aria-labelledby="additional-information"><div class="c-article-section" id="additional-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="additional-information">Additional information</h2><div class="c-article-section__content" id="additional-information-content"><p><b>Publisher’s note:</b> Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></div></div></section><section aria-labelledby="rightslink"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content"><p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=Reinforcement%20learning%20in%20artificial%20and%20biological%20systems&amp;author=Emre%20O.%20Neftci%20et%20al&amp;contentID=10.1038%2Fs42256-019-0025-4&amp;publication=2522-5839&amp;publicationDate=2019-03-04&amp;publisherName=SpringerNature&amp;orderBeanReset=true">Reprints and Permissions</a></p></div></div></section><section aria-labelledby="article-info"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="u-hide-print c-bibliographic-information__column c-bibliographic-information__column--border"><a data-crossmark="10.1038/s42256-019-0025-4" target="_blank" rel="noopener" href="https://crossmark.crossref.org/dialog/?doi=10.1038/s42256-019-0025-4" data-track="click" data-track-action="Click Crossmark" data-track-label="link" data-test="crossmark"><img width="57" height="81" alt="Verify currency and authenticity via CrossMark" src="data:image/svg+xml;base64,<svg height="81" width="57" xmlns="http://www.w3.org/2000/svg"><g fill="none" fill-rule="evenodd"><path d="m17.35 35.45 21.3-14.2v-17.03h-21.3" fill="#989898"/><path d="m38.65 35.45-21.3-14.2v-17.03h21.3" fill="#747474"/><path d="m28 .5c-12.98 0-23.5 10.52-23.5 23.5s10.52 23.5 23.5 23.5 23.5-10.52 23.5-23.5c0-6.23-2.48-12.21-6.88-16.62-4.41-4.4-10.39-6.88-16.62-6.88zm0 41.25c-9.8 0-17.75-7.95-17.75-17.75s7.95-17.75 17.75-17.75 17.75 7.95 17.75 17.75c0 4.71-1.87 9.22-5.2 12.55s-7.84 5.2-12.55 5.2z" fill="#535353"/><path d="m41 36c-5.81 6.23-15.23 7.45-22.43 2.9-7.21-4.55-10.16-13.57-7.03-21.5l-4.92-3.11c-4.95 10.7-1.19 23.42 8.78 29.71 9.97 6.3 23.07 4.22 30.6-4.86z" fill="#9c9c9c"/><path d="m.2 58.45c0-.75.11-1.42.33-2.01s.52-1.09.91-1.5c.38-.41.83-.73 1.34-.94.51-.22 1.06-.32 1.65-.32.56 0 1.06.11 1.51.35.44.23.81.5 1.1.81l-.91 1.01c-.24-.24-.49-.42-.75-.56-.27-.13-.58-.2-.93-.2-.39 0-.73.08-1.05.23-.31.16-.58.37-.81.66-.23.28-.41.63-.53 1.04-.13.41-.19.88-.19 1.39 0 1.04.23 1.86.68 2.46.45.59 1.06.88 1.84.88.41 0 .77-.07 1.07-.23s.59-.39.85-.68l.91 1c-.38.43-.8.76-1.28.99-.47.22-1 .34-1.58.34-.59 0-1.13-.1-1.64-.31-.5-.2-.94-.51-1.31-.91-.38-.4-.67-.9-.88-1.48-.22-.59-.33-1.26-.33-2.02zm8.4-5.33h1.61v2.54l-.05 1.33c.29-.27.61-.51.96-.72s.76-.31 1.24-.31c.73 0 1.27.23 1.61.71.33.47.5 1.14.5 2.02v4.31h-1.61v-4.1c0-.57-.08-.97-.25-1.21-.17-.23-.45-.35-.83-.35-.3 0-.56.08-.79.22-.23.15-.49.36-.78.64v4.8h-1.61zm7.37 6.45c0-.56.09-1.06.26-1.51.18-.45.42-.83.71-1.14.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.36c.07.62.29 1.1.65 1.44.36.33.82.5 1.38.5.29 0 .57-.04.83-.13s.51-.21.76-.37l.55 1.01c-.33.21-.69.39-1.09.53-.41.14-.83.21-1.26.21-.48 0-.92-.08-1.34-.25-.41-.16-.76-.4-1.07-.7-.31-.31-.55-.69-.72-1.13-.18-.44-.26-.95-.26-1.52zm4.6-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.07.45-.31.29-.5.73-.58 1.3zm2.5.62c0-.57.09-1.08.28-1.53.18-.44.43-.82.75-1.13s.69-.54 1.1-.71c.42-.16.85-.24 1.31-.24.45 0 .84.08 1.17.23s.61.34.85.57l-.77 1.02c-.19-.16-.38-.28-.56-.37-.19-.09-.39-.14-.61-.14-.56 0-1.01.21-1.35.63-.35.41-.52.97-.52 1.67 0 .69.17 1.24.51 1.66.34.41.78.62 1.32.62.28 0 .54-.06.78-.17.24-.12.45-.26.64-.42l.67 1.03c-.33.29-.69.51-1.08.65-.39.15-.78.23-1.18.23-.46 0-.9-.08-1.31-.24-.4-.16-.75-.39-1.05-.7s-.53-.69-.7-1.13c-.17-.45-.25-.96-.25-1.53zm6.91-6.45h1.58v6.17h.05l2.54-3.16h1.77l-2.35 2.8 2.59 4.07h-1.75l-1.77-2.98-1.08 1.23v1.75h-1.58zm13.69 1.27c-.25-.11-.5-.17-.75-.17-.58 0-.87.39-.87 1.16v.75h1.34v1.27h-1.34v5.6h-1.61v-5.6h-.92v-1.2l.92-.07v-.72c0-.35.04-.68.13-.98.08-.31.21-.57.4-.79s.42-.39.71-.51c.28-.12.63-.18 1.04-.18.24 0 .48.02.69.07.22.05.41.1.57.17zm.48 5.18c0-.57.09-1.08.27-1.53.17-.44.41-.82.72-1.13.3-.31.65-.54 1.04-.71.39-.16.8-.24 1.23-.24s.84.08 1.24.24c.4.17.74.4 1.04.71s.54.69.72 1.13c.19.45.28.96.28 1.53s-.09 1.08-.28 1.53c-.18.44-.42.82-.72 1.13s-.64.54-1.04.7-.81.24-1.24.24-.84-.08-1.23-.24-.74-.39-1.04-.7c-.31-.31-.55-.69-.72-1.13-.18-.45-.27-.96-.27-1.53zm1.65 0c0 .69.14 1.24.43 1.66.28.41.68.62 1.18.62.51 0 .9-.21 1.19-.62.29-.42.44-.97.44-1.66 0-.7-.15-1.26-.44-1.67-.29-.42-.68-.63-1.19-.63-.5 0-.9.21-1.18.63-.29.41-.43.97-.43 1.67zm6.48-3.44h1.33l.12 1.21h.05c.24-.44.54-.79.88-1.02.35-.24.7-.36 1.07-.36.32 0 .59.05.78.14l-.28 1.4-.33-.09c-.11-.01-.23-.02-.38-.02-.27 0-.56.1-.86.31s-.55.58-.77 1.1v4.2h-1.61zm-47.87 15h1.61v4.1c0 .57.08.97.25 1.2.17.24.44.35.81.35.3 0 .57-.07.8-.22.22-.15.47-.39.73-.73v-4.7h1.61v6.87h-1.32l-.12-1.01h-.04c-.3.36-.63.64-.98.86-.35.21-.76.32-1.24.32-.73 0-1.27-.24-1.61-.71-.33-.47-.5-1.14-.5-2.02zm9.46 7.43v2.16h-1.61v-9.59h1.33l.12.72h.05c.29-.24.61-.45.97-.63.35-.17.72-.26 1.1-.26.43 0 .81.08 1.15.24.33.17.61.4.84.71.24.31.41.68.53 1.11.13.42.19.91.19 1.44 0 .59-.09 1.11-.25 1.57-.16.47-.38.85-.65 1.16-.27.32-.58.56-.94.73-.35.16-.72.25-1.1.25-.3 0-.6-.07-.9-.2s-.59-.31-.87-.56zm0-2.3c.26.22.5.37.73.45.24.09.46.13.66.13.46 0 .84-.2 1.15-.6.31-.39.46-.98.46-1.77 0-.69-.12-1.22-.35-1.61-.23-.38-.61-.57-1.13-.57-.49 0-.99.26-1.52.77zm5.87-1.69c0-.56.08-1.06.25-1.51.16-.45.37-.83.65-1.14.27-.3.58-.54.93-.71s.71-.25 1.08-.25c.39 0 .73.07 1 .2.27.14.54.32.81.55l-.06-1.1v-2.49h1.61v9.88h-1.33l-.11-.74h-.06c-.25.25-.54.46-.88.64-.33.18-.69.27-1.06.27-.87 0-1.56-.32-2.07-.95s-.76-1.51-.76-2.65zm1.67-.01c0 .74.13 1.31.4 1.7.26.38.65.58 1.15.58.51 0 .99-.26 1.44-.77v-3.21c-.24-.21-.48-.36-.7-.45-.23-.08-.46-.12-.7-.12-.45 0-.82.19-1.13.59-.31.39-.46.95-.46 1.68zm6.35 1.59c0-.73.32-1.3.97-1.71.64-.4 1.67-.68 3.08-.84 0-.17-.02-.34-.07-.51-.05-.16-.12-.3-.22-.43s-.22-.22-.38-.3c-.15-.06-.34-.1-.58-.1-.34 0-.68.07-1 .2s-.63.29-.93.47l-.59-1.08c.39-.24.81-.45 1.28-.63.47-.17.99-.26 1.54-.26.86 0 1.51.25 1.93.76s.63 1.25.63 2.21v4.07h-1.32l-.12-.76h-.05c-.3.27-.63.48-.98.66s-.73.27-1.14.27c-.61 0-1.1-.19-1.48-.56-.38-.36-.57-.85-.57-1.46zm1.57-.12c0 .3.09.53.27.67.19.14.42.21.71.21.28 0 .54-.07.77-.2s.48-.31.73-.56v-1.54c-.47.06-.86.13-1.18.23-.31.09-.57.19-.76.31s-.33.25-.41.4c-.09.15-.13.31-.13.48zm6.29-3.63h-.98v-1.2l1.06-.07.2-1.88h1.34v1.88h1.75v1.27h-1.75v3.28c0 .8.32 1.2.97 1.2.12 0 .24-.01.37-.04.12-.03.24-.07.34-.11l.28 1.19c-.19.06-.4.12-.64.17-.23.05-.49.08-.76.08-.4 0-.74-.06-1.02-.18-.27-.13-.49-.3-.67-.52-.17-.21-.3-.48-.37-.78-.08-.3-.12-.64-.12-1.01zm4.36 2.17c0-.56.09-1.06.27-1.51s.41-.83.71-1.14c.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.37c.08.62.29 1.1.65 1.44.36.33.82.5 1.38.5.3 0 .58-.04.84-.13.25-.09.51-.21.76-.37l.54 1.01c-.32.21-.69.39-1.09.53s-.82.21-1.26.21c-.47 0-.92-.08-1.33-.25-.41-.16-.77-.4-1.08-.7-.3-.31-.54-.69-.72-1.13-.17-.44-.26-.95-.26-1.52zm4.61-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.08.45-.31.29-.5.73-.57 1.3zm3.01 2.23c.31.24.61.43.92.57.3.13.63.2.98.2.38 0 .65-.08.83-.23s.27-.35.27-.6c0-.14-.05-.26-.13-.37-.08-.1-.2-.2-.34-.28-.14-.09-.29-.16-.47-.23l-.53-.22c-.23-.09-.46-.18-.69-.3-.23-.11-.44-.24-.62-.4s-.33-.35-.45-.55c-.12-.21-.18-.46-.18-.75 0-.61.23-1.1.68-1.49.44-.38 1.06-.57 1.83-.57.48 0 .91.08 1.29.25s.71.36.99.57l-.74.98c-.24-.17-.49-.32-.73-.42-.25-.11-.51-.16-.78-.16-.35 0-.6.07-.76.21-.17.15-.25.33-.25.54 0 .14.04.26.12.36s.18.18.31.26c.14.07.29.14.46.21l.54.19c.23.09.47.18.7.29s.44.24.64.4c.19.16.34.35.46.58.11.23.17.5.17.82 0 .3-.06.58-.17.83-.12.26-.29.48-.51.68-.23.19-.51.34-.84.45-.34.11-.72.17-1.15.17-.48 0-.95-.09-1.41-.27-.46-.19-.86-.41-1.2-.68z" fill="#535353"/></g></svg>" /></a></div><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Neftci, E.O., Averbeck, B.B. Reinforcement learning in artificial and biological systems.
                    <i>Nat Mach Intell</i> <b>1, </b>133–143 (2019). https://doi.org/10.1038/s42256-019-0025-4</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" href="/articles/s42256-019-0025-4.ris">Download citation<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Received<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2018-09-06">06 September 2018</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Accepted<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2019-01-16">16 January 2019</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2019-03-04">04 March 2019</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Issue Date<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2019-03">March 2019</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--doi"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value"><a href="https://doi.org/10.1038/s42256-019-0025-4" data-track="click" data-track-action="view doi" data-track-label="link" itemprop="sameAs">https://doi.org/10.1038/s42256-019-0025-4</a></span></p></li></ul><div data-component="share-box"></div><div data-component="article-info-list"></div></div></div></div></div></section>

                        
    <section aria-labelledby="further-reading">
      <div class="c-article-section js-article-section" id="further-reading-section">
        <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="further-reading">Further reading</h2>
        <div class="c-article-section__content js-collapsible-section" id="further-reading-content">
        <ul class="c-article-further-reading__list" id="further-reading-list">
          
            <li class="c-article-further-reading__item js-ref-item" id="further-reading-item-1">
              <h3 class="c-article-further-reading__title">
                <a class="print-link"
                   data-track="click"
                   data-track-action="view further reading article"
                   data-track-label="link:An adaptive deep reinforcement learning approach for MIMO PID control of mobile robots"
                   href="https://doi.org/10.1016/j.isatra.2020.02.017"
                   id="title-link-1">
                  An adaptive deep reinforcement learning approach for MIMO PID control of mobile robots
                </a>
              </h3>
              
                <ul data-test="author-list"
                    class="c-article-further-reading__authors-list js-list-authors-3 js-etal-collapsed" data-component-authors-activator="authors-list"
                    id="authors-1">
                  <li><span
                        class="js-separator"></span><span>Ignacio Carlucho</span></li><li><span
                        class="js-separator">, </span><span>Mariano De Paula</span></li><li><span
                        class="js-separator">&nbsp;&amp;&nbsp;</span><span>Gerardo G. Acosta</span></li></ul>
              
              <p class="c-article-further-reading__journal-title" id="journal-name-1">
                <i>ISA Transactions</i>
                (2020)
              </p>
            </li>
          
            <li class="c-article-further-reading__item js-ref-item" id="further-reading-item-2">
              <h3 class="c-article-further-reading__title">
                <a class="print-link"
                   data-track="click"
                   data-track-action="view further reading article"
                   data-track-label="link:Gated spiking neural network using Iterative Free-Energy Optimization and rank-order coding for structure learning in memory sequences (INFERNO GATE)"
                   href="https://doi.org/10.1016/j.neunet.2019.09.023"
                   id="title-link-2">
                  Gated spiking neural network using Iterative Free-Energy Optimization and rank-order coding for structure learning in memory sequences (INFERNO GATE)
                </a>
              </h3>
              
                <ul data-test="author-list"
                    class="c-article-further-reading__authors-list js-list-authors-3 js-etal-collapsed" data-component-authors-activator="authors-list"
                    id="authors-2">
                  <li><span
                        class="js-separator"></span><span>Alexandre Pitti</span></li><li><span
                        class="js-separator">, </span><span>Mathias Quoy</span></li><li><span
                        class="js-separator">, </span><span>Catherine Lavandier</span></li><li><span
                        class="js-separator">&nbsp;&amp;&nbsp;</span><span>Sofiane Boucenna</span></li></ul>
              
              <p class="c-article-further-reading__journal-title" id="journal-name-2">
                <i>Neural Networks</i>
                (2020)
              </p>
            </li>
          
            <li class="c-article-further-reading__item js-ref-item" id="further-reading-item-3">
              <h3 class="c-article-further-reading__title">
                <a class="print-link"
                   data-track="click"
                   data-track-action="view further reading article"
                   data-track-label="link:Machine learning for active matter"
                   href="https://doi.org/10.1038/s42256-020-0146-9"
                   id="title-link-3">
                  Machine learning for active matter
                </a>
              </h3>
              
                <ul data-test="author-list"
                    class="c-article-further-reading__authors-list js-list-authors-3 js-etal-collapsed" data-component-authors-activator="authors-list"
                    id="authors-3">
                  <li><span
                        class="js-separator"></span><span>Frank Cichos</span></li><li><span
                        class="js-separator">, </span><span>Kristian Gustavsson</span></li><li><span
                        class="js-separator">, </span><span>Bernhard Mehlig</span></li><li><span
                        class="js-separator">&nbsp;&amp;&nbsp;</span><span>Giovanni Volpe</span></li></ul>
              
              <p class="c-article-further-reading__journal-title" id="journal-name-3">
                <i>Nature Machine Intelligence</i>
                (2020)
              </p>
            </li>
          
            <li class="c-article-further-reading__item js-ref-item" id="further-reading-item-4">
              <h3 class="c-article-further-reading__title">
                <a class="print-link"
                   data-track="click"
                   data-track-action="view further reading article"
                   data-track-label="link:Recent Progress in Synaptic Devices Based on 2D Materials"
                   href="https://doi.org/10.1002/aisy.201900167"
                   id="title-link-4">
                  Recent Progress in Synaptic Devices Based on 2D Materials
                </a>
              </h3>
              
                <ul data-test="author-list"
                    class="c-article-further-reading__authors-list js-list-authors-3 js-etal-collapsed" data-component-authors-activator="authors-list"
                    id="authors-4">
                  <li><span
                        class="js-separator"></span><span>Linfeng Sun</span></li><li><span
                        class="js-separator">, </span><span>Wei Wang</span></li><li><span
                        class="js-separator">&nbsp;&amp;&nbsp;</span><span>Heejun Yang</span></li></ul>
              
              <p class="c-article-further-reading__journal-title" id="journal-name-4">
                <i>Advanced Intelligent Systems</i>
                (2020)
              </p>
            </li>
          
            <li class="c-article-further-reading__item js-ref-item" id="further-reading-item-5">
              <h3 class="c-article-further-reading__title">
                <a class="print-link"
                   data-track="click"
                   data-track-action="view further reading article"
                   data-track-label="link:Primate Frontal Eye Field Neurons Selectively Signal the Reward Value of Prior Actions"
                   href="https://doi.org/10.1016/j.pneurobio.2020.101881"
                   id="title-link-5">
                  Primate Frontal Eye Field Neurons Selectively Signal the Reward Value of Prior Actions
                </a>
              </h3>
              
                <ul data-test="author-list"
                    class="c-article-further-reading__authors-list js-list-authors-3 js-etal-collapsed" data-component-authors-activator="authors-list"
                    id="authors-5">
                  <li><span
                        class="js-separator"></span><span>Xiaomo Chen</span></li><li><span
                        class="js-separator">, </span><span>Marc Zirnsak</span></li><li><span
                        class="js-separator">, </span><span>Gabriel M. Vega</span></li><li><span
                        class="js-separator">&nbsp;&amp;&nbsp;</span><span>Tirin Moore</span></li></ul>
              
              <p class="c-article-further-reading__journal-title" id="journal-name-5">
                <i>Progress in Neurobiology</i>
                (2020)
              </p>
            </li>
          
        </ul>
      </div>
    </div>
    </section>
  

                        

                        <span data-recommended="jobs"></span>
                    </div>
                </div>

                <div class="c-article-extras u-hide-print" role="complementary" data-container-type="reading-companion" data-track-component="reading companion">
                    
                        <noscript>
                            
<div class="c-nature-box c-nature-box--side " data-component="entitlement-box">
    
        <div class="js-access-button">
            <a href="https://wayf.springernature.com?redirect_uri&#x3D;https%3A%2F%2Fwww.nature.com%2Farticles%2Fs42256-019-0025-4" class="c-article__button" data-test="ra21" data-track="click" data-track-action="institution access" data-track-label="button">
                <svg class="u-icon" width="18" height="18"><use href="#global-icon-institution"></use></svg>
                <span class="c-article__button-text">Access through your institution</span>
            </a>
        </div>
         <div class="js-buy-button">
            <a href="#access-options" class="c-article__button c-article__button--inverted" data-test="ra21" data-track="click" data-track-action="buy or subscribe" data-track-label="button">
                <span>Buy or subscribe</span>
            </a>
        </div>
    
</div>

                        </noscript>
                        <div class="c-nature-box__wrapper c-nature-box__wrapper--placeholder">
                            <div class="c-nature-box c-nature-box--side u-display-none" aria-hidden="true" data-component="entitlement-box" id=entitlement-box-desktop>
    
        <p class="c-nature-box__text js-text u-display-none" aria-hidden="true"></p>
        
    
        
        <div class="c-pdf-download u-clear-both">
            <a href="/articles/s42256-019-0025-4.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="download-pdf" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="link">
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            </a>
        </div>
        
    

    
</div>

                        </div>
                    

                    
                        
                    

                    
        
            <aside>
                <div class="c-article-associated-content__container">
                    <h1 class="c-article-associated-content__title u-h3">Associated Content</h1>
                    
                        <div class="c-article-associated-content__collection special">
                            <section>
                                <p class="c-article-associated-content__collection-label u-sans-serif">Special</p>
                                <h3 class="c-article-associated-content__collection-title u-h3" itemprop="name headline"><a
                                        href="/collections/adjbhaagce"
                                        data-track="click"
                                        data-track-action="view special"
                                        data-track-label="link">
                                    One year anniversary collection
                                </a></h3>
                            </section>
                        </div>
                    
                        <div class="c-article-associated-content__collection nature outlook">
                            <section>
                                <p class="c-article-associated-content__collection-label u-sans-serif">Nature Outlook</p>
                                <h3 class="c-article-associated-content__collection-title u-h3" itemprop="name headline"><a
                                        href="/collections/jigfghaeje"
                                        data-track="click"
                                        data-track-action="view nature outlook"
                                        data-track-label="link">
                                    The brain
                                </a></h3>
                            </section>
                        </div>
                    
                    
                </div>
            </aside>
        
    

                    <div class="c-reading-companion">
                        <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky">
                            <div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections">
                                
    <div id="div-gpt-ad-right-2"
         class="div-gpt-ad advert medium-rectangle js-ad text-center hide-print grade-c-hide"
         data-ad-type="right"
         data-gpt-unitpath="/285/natmachintell.nature.com/article"
         data-gpt-sizes="300x250"
         data-gpt-targeting="type=article;pos=right;artid=s42256-019-0025-4;doi=10.1038/s42256-019-0025-4;subjmeta=114,116,2397,617,631,692;kwrd=Computational models,Computational neuroscience,Neurology">
        <noscript>
            <a href="//pubads.g.doubleclick.net/gampad/jump?iu=/285/natmachintell.nature.com/article&amp;sz=300x250&amp;c=1971604300&amp;t=pos%3Dright%26type%3Darticle%26artid%3Ds42256-019-0025-4%26doi%3D10.1038/s42256-019-0025-4%26subjmeta%3D114,116,2397,617,631,692%26kwrd%3DComputational models,Computational neuroscience,Neurology">
                <img data-test="gpt-advert-fallback-img"
                     src="//pubads.g.doubleclick.net/gampad/ad?iu=/285/natmachintell.nature.com/article&amp;sz=300x250&amp;c=1971604300&amp;t=pos%3Dright%26type%3Darticle%26artid%3Ds42256-019-0025-4%26doi%3D10.1038/s42256-019-0025-4%26subjmeta%3D114,116,2397,617,631,692%26kwrd%3DComputational models,Computational neuroscience,Neurology"
                     alt="Advertisement"
                     width="300"
                     height="250"></a>
        </noscript>
    </div>

                            </div>
                            <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures"></div>
                            <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references"></div>
                        </div>
                    </div>
                </div>
            </article>
        </div>
    </div>
</div>


<div class="hide-print js-header-menu menu mb20 z-index-50 composite-layer background-white border-bottom-2 border-gray-medium tighten-line-height" id="menu" data-track-component="menu">
    <div class="menu-inner content js-hide">
        <nav>
            <div class="cleared border-bottom-1 border-gray-medium ml20 mr20">
                <div class="grid-ng grid-1of4 mq875-grid-12">
                    <h3 class="h3 pa20 mq875-pa0 mq875-mt20">
                        Nature Machine Intelligence<span class="visually-hidden"> menu</span>
                    </h3>
                </div>
                
    
        
        <div class="grid-ng grid-1of4 text14 pa20 mq875-grid-4 mq640-grid-12">
            <ul class="clean-list mb0">
                
                <li class="mb4 pr10"><a href="/natmachintell/news-and-comment" data-track="click" data-track-action="news &amp; comment" data-track-label="link">News &amp; Comment</a></li>
                
                <li class="mb4 pr10"><a href="/natmachintell/research" data-track="click" data-track-action="research" data-track-label="link">Research</a></li>
                
            </ul>
        </div>
        

        
        <div class="grid-ng grid-1of4 text14 pa20 mq875-grid-4 mq640-grid-12">
            <ul class="clean-list mb0">
                
                <li class="mb4 pr10"><a href="/natmachintell/current-issue" data-track="click" data-track-action="current issue" data-track-label="link">Current Issue</a></li>
                
                <li class="mb4 pr10"><a href="/natmachintell/browse-issues" data-track="click" data-track-action="browse issues" data-track-label="link">Browse Issues</a></li>
                
                <li class="mb4 pr10"><a href="/natmachintell/articles" data-track="click" data-track-action="browse articles" data-track-label="link">Browse Articles</a></li>
                
                <li class="mb4 pr10"><a href="/natmachintell/collections" data-track="click" data-track-action="browse collections" data-track-label="link">Browse Collections</a></li>
                
            </ul>
        </div>
        

        
        <div class="grid-ng grid-1of4 text14 pa20 mq875-grid-4 mq640-grid-12">
            <ul class="clean-list mb0">
                
                <li class="mb4 pr10"><a href="/natmachintell/about" data-track="click" data-track-action="about the journal" data-track-label="link">About the Journal</a></li>
                
                <li class="mb4 pr10"><a href="/natmachintell/for-authors" data-track="click" data-track-action="for authors" data-track-label="link">For Authors</a></li>
                
                <li class="mb4 pr10"><a href="/natmachintell/for-reviewers" data-track="click" data-track-action="for reviewers" data-track-label="link">For Reviewers</a></li>
                
            </ul>
        </div>
        
    

            </div>
            <div class="cleared ml20 mr20">
                
    <div class="cleared pb10">
        <div class="grid-ng grid-1of4 mq875-grid-12">
            <h3 class="serif pa20 mq875-pa0 mq875-mt20">Nature Research<span class="visually-hidden"> menu</span></h3>
        </div>
        <div class="grid-ng grid-1of4 text14 pa20 mq875-grid-4 mq640-grid-12">
            <h4 class="mb4">Our Journals</h4>
            <ul class="clean-list mb0">
                <li class="mb4 pr10"><a href="/nature" data-track="click" data-track-action="nature" data-track-label="link">Nature</a></li>
                <li class="mb4 pr10"><a href="/ncomms" data-track="click" data-track-action="nature communications" data-track-label="link">Nature Communications</a></li>
                <li class="mb4 pr10"><a href="/nprot" data-track="click" data-track-action="nature protocols" data-track-label="link">Nature Protocols</a></li>
                <li class="mb4 pr10"><a href="/srep" data-track="click" data-track-action="scientific reports" data-track-label="link">Scientific Reports</a></li>
                <li class="mb4 pr10"><a href="/siteindex" data-track="click" data-track-action="journal a-z view all" data-track-label="link">View all journals</a></li>
            </ul>
        </div>
        <div class="grid-ng grid-1of4 text14 pa20 mq875-grid-4 mq640-grid-12">
            <h4 class="mb4">Subjects</h4>
            <ul class="clean-list mb0">
                <li class="mb4 pr10"><a href="/subjects/biological-sciences" data-track="click" data-track-action="biological sciences" data-track-label="link">Biological Sciences</a></li>
                <li class="mb4 pr10"><a href="/subjects/scientific-community-and-society" data-track="click" data-track-action="scientific community and society" data-track-label="link">Scientific Community &amp; Society</a></li>
                <li class="mb4 pr10"><a href="/subjects/earth-and-environmental-sciences" data-track="click" data-track-action="earth and environmental sciences" data-track-label="link">Earth &amp; Environmental Sciences</a></li>
                <li class="mb4 pr10"><a href="/subjects/health-sciences" data-track="click" data-track-action="health sciences" data-track-label="link">Health Sciences</a></li>
                <li class="mb4 pr10"><a href="/subjects/physical-sciences" data-track="click" data-track-action="physical sciences" data-track-label="link">Physical Sciences</a></li>
                <li class="mb4 pr10"><a href="/subjects" data-track="click" data-track-action="subjects view all" data-track-label="link">View all subjects</a></li>
            </ul>
        </div>
        <div class="grid-ng grid-1of4 text14 pa20 mq875-grid-4 mq640-grid-12">
            <h4 class="mb4">More</h4>
            <ul class="clean-list mb0">
                <li class="mb4 pr10"><a href="https://support.nature.com/support/home" data-track="click" data-track-action="subscriptions" data-track-label="link">Contact us</a></li>
                <li class="mb4 pr10"><a href="/authors/index.html" data-track="click" data-track-action="authors and referees" data-track-label="link">Authors &amp; Referees</a></li>
                <li class="mb4 pr10"><a href="/libraries/index.html" data-track="click" data-track-action="librarians" data-track-label="link">Librarians</a></li>
                <li class="mb4 pr10"><a href="/advertising/index.html" data-track="click" data-track-action="advertisers" data-track-label="link">Advertisers</a></li>
                <li class="mb4 pr10"><a href="/npg_/press_room/index.html" data-track="click" data-track-action="press" data-track-label="link">Press</a></li>
                <li class="mb4 pr10"><a href="/npg_/index_npg.html" data-track="click" data-track-action="about" data-track-label="link">About Nature Research</a></li>
            </ul>
        </div>
    </div>



            </div>
        </nav>
    </div>
</div>



    <div id="search-menu" class="menu pt20 mb20 z-index-50 composite-layer background-white border-gray-medium border-bottom-2" data-component="tray" data-track-component="header">
        <div class="menu-inner js-hide">
            <section>
                <h2 class="visually-hidden">Search</h2>
                <div class="hide-print content mb40 mq1200-padded position-relative" data-test="inline-search">
                    <div class="pt30 cleared background-white sans-serif">
                        <div class="grid grid-8 grid-left-2 mq875-grid-12 mq875-ml0 mq640-pb30">
                            <form action="/search" method="get" role="search" autocomplete="off" class="standard-space-below" data-track="submit" data-track-action="search" data-track-label="form">
                                <label for="keywords" class="block strong">Article search</label>
                                <div class="position-relative">
                                    <input type="search" id="keywords" class="border-gray border-all-1 equalize-line-height pa10 pr40 box-sizing grid-12" name="q" value="" placeholder="Search by keywords or author" data-test="search-keywords">
                                    <div class="position-absolute position-right position-top mt1 mr1">
                                        <button type="submit" class="icon icon-center search-btn kill-border" data-test="search-submit">
                                            Search
                                        </button>
                                    </div>
                                    <p class="mb0 mt4 text14"><a href="/search/advanced" data-track="click" data-track-action="advanced search" data-track-label="link">Advanced search</a></p>
                                </div>
                            </form>
                            <h3 class="h3 strong mb4 border-gray-medium border-bottom-1 sans-serif">Quick links</h3>
                            <ul class="clean-list mb0 text14">
                                <li class="grid-ng grid-1of2 mb6 mt6"><a href="/subjects" data-track="click" data-track-action="explore articles by subject" data-track-label="link">Explore articles by subject</a></li>
                                <li class="grid-ng grid-1of2 mb6 mt6"><a href="/naturecareers" data-track="click" data-track-action="find a job" data-track-label="link">Find a job</a></li>
                                <li class="grid-ng grid-1of2"><a href="/authors/index.html" data-track="click" data-track-action="guide to authors" data-track-label="link">Guide to authors</a></li>
                                <li class="grid-ng grid-1of2"><a href="/authors/editorial_policies/" data-track="click" data-track-action="editorial policies" data-track-label="link">Editorial policies</a></li>
                            </ul>
                        </div>
                    </div>
                </div>
            </section>
        </div>
    </div>




<footer role="contentinfo" class="composite-layer">
    
        
    


    

    <div itemscope itemtype="http://schema.org/Periodical">
        <meta itemprop="publisher" content="Springer Nature">
        
            <div class="container-footer mt40 cleared container-footer-full">
                
                    <div class="content pt20 pr30 pl30">
                        <div class="grid grid-12 last clear-float cleared mb15 text-gray-light text13 flex-box flex-wrap-reverse">
                            <h2 aria-level="2" class="text13 strong emphasis ma0 sans-serif" itemprop="name">Nature Machine Intelligence</h2>
                            <p class="pt0 pr10 pb0 pl10 ma0"><abbr title="International Standard Serial Number">ISSN</abbr> <span itemprop="issn">2522-5839</span> (online)</p>
                        </div>
                    </div>
                

                
    <div class="grid grid-12 pl30 pr30 last" id="footer">
        <h2 aria-level="2" class="hide">nature.com sitemap</h2>
    </div>

    <div class="pl30 pr30 content" data-track-component="footer">

        <div class="grid grid-12 last clear-float cleared mb5 pt20 text-gray-light flex-wrap">
            <div class="grid grid-3 mq875-grid-12 just-mq640-last just-mq875-last mb20"><img alt="Nature Research" src="/static/images/logos/nature research-colour-150.a383514567.svg" loading="lazy" width="255" height="33"></div>
            <ul class="grid grid-7 mq875-grid-8 mq640-grid-12 just-mq640-last clean-list mb10 mt10 text15 u-hide-print">
                <li class="grid grid-3 mq875-grid-3 mq640-grid-6 mq480-grid-12"><a href="https://www.nature.com/npg_/company_info/index.html" data-track="click" data-track-action="about us" class="text-gray-light" data-track-label="link">About us</a></li>
                <li class="grid grid-3 mq875-grid-3 mq640-grid-6 mq480-grid-12 just-mq640-last just-mq480-last"><a href="https://www.nature.com/npg_/press_room/press_releases.html" data-track="click" data-track-action="press releases" data-track-label="link" class="text-gray-light">Press releases</a></li>
                <li class="grid grid-3 mq875-grid-3 mq640-grid-6 mq480-grid-12"><a href="https://press.nature.com/" data-track="click" data-track-action="press office" data-track-label="link" class="text-gray-light">Press office</a></li>
                <li class="grid grid-3 mq875-grid-3 mq640-grid-6 mq480-grid-12 last"><a href="https://support.nature.com/support/home" data-track="click" data-track-action="contact us" class="text-gray-light" data-track-label="link">Contact us</a></li>
            </ul>
            <ul class="grid grid-2 mq875-grid-4 mq640-grid-12 last clean-list mb10 mt4 u-hide-print">
                <li class="grid grid-4 mq875-grid-3"><a href="https://www.facebook.com/nature/" data-track="click" data-track-action="facebook" data-track-label="link" class="text-gray-light"><img src="/static/images/fb.49ba391f15.svg" class="cleared text-footer-img" alt="Facebook"/></a></li>
                <li class="grid grid-4 mq875-grid-3"><a href="https://twitter.com/nresearchnews?lang=en" data-track="click" data-track-action="twitter" data-track-label="link" class="text-gray-light"><img src="/static/images/twitter.a90dbeb03a.svg" class="cleared text-footer-img" alt="Twitter"/></a></li>
                <li class="grid grid-4 mq875-grid-3 last"><a href="https://www.youtube.com/channel/UCvCLdSgYdSTpWcOgEJgi-ng" data-track="click" data-track-action="youtube" data-track-label="link" class="text-gray-light"><img src="/static/images/youtube.6071f6012c.svg" class="cleared text-footer-img" alt="Youtube"/></a></li>
            </ul>
        </div>

        <div class="cleared z-index-1 pt30 pb20 border-top-1 border-gray text-footer-mobile-bottom hide-print u-hide-print">
            <div class="grid grid-3 mq875-grid-4 mq640-grid-6 mq480-grid-12">
                <h3 class="text-footer-heading">Discover content</h3>
                <ul class="clean-list ma0 mb6 text15">
                    <li class="pb4"><a href="https://www.nature.com/siteindex" data-track="click" data-track-action="journals a-z" data-track-label="link" class="text-gray-light">Journals A-Z</a></li>
                    <li class="pb4"><a href="https://www.nature.com/subjects/" data-track="click" data-track-action="article by subject" data-track-label="link" class="text-gray-light">Articles by subject</a></li>
                    <li class="pb4"><a href="https://nano.nature.com/" data-track="click" data-track-action="nano" data-track-label="link" class="text-gray-light">Nano</a></li>
                    <li class="pb4"><a href="https://www.nature.com/protocolexchange/" data-track="click" data-track-action="protocol exchange" data-track-label="link" class="text-gray-light">Protocol Exchange</a></li>
                    <li class="pb4"><a href="https://www.natureindex.com/" data-track="click" data-track-action="nature index" data-track-label="link" class="text-gray-light">Nature Index</a></li>
                </ul>
            </div>

            <div class="grid grid-3 mq875-grid-4 mq640-grid-6 mq480-grid-12 just-mq640-last">
                <h3 class="text-footer-heading">Publish with us</h3>
                <ul class="clean-list ma0 mb6 text15">
                    <li class="pb4"><a href="https://www.nature.com/authors/author_resources/index.html" data-track="click" data-track-action="guide to authors" data-track-label="link" class="text-gray-light" >Guide to Authors</a></li>
                    <li class="pb4"><a href="https://www.nature.com/authors/peer_review/" data-track="click" data-track-action="guide to referees" data-track-label="link" class="text-gray-light">Guide to Referees</a></li>
                    <li class="pb4"><a href="https://www.nature.com/authors/editorial_policies/" data-track="click" data-track-action="editorial policies" data-track-label="link" class="text-gray-light">Editorial policies</a></li>
                    <li class="pb4"><a href="http://www.nature.com/openresearch/publishing-with-npg/" data-track="click" data-track-action="open access" data-track-label="link" class="text-gray-light">Open access</a></li>
                    <li class="pb4" ><a href="https://www.nature.com/reprints/" data-track="click" data-track-action="reprints and permissions" data-track-label="link" class="text-gray-light">Reprints &amp; permissions</a></li>
                </ul>
            </div>

            <div class="grid grid-3 mq875-grid-4 mq640-grid-6 mq480-grid-12 just-mq875-last">
                <h3 class="text-footer-heading">Researcher services</h3>
                <ul class="clean-list ma0 mb6 text15">
                    <li class="pb4"><a href="https://www.springernature.com/gp/authors/research-data" data-track="click" data-track-action="data research service" data-track-label="link" class="text-gray-light">Research data</a></li>
                    <li class="pb4"><a href="https://authorservices.springernature.com/go/nr" data-track="click" data-track-action="language editing" data-track-label="link" class="text-gray-light">Language editing</a></li>
                    <li class="pb4"><a href="https://authorservices.springernature.com/scientific-editing/" data-track="click" data-track-action="scientific editing" data-track-label="link" class="text-gray-light">Scientific editing</a></li>
                    <li class="pb4"><a href="https://masterclasses.nature.com/" data-track="click" data-track-action="nature masterclasses" data-track-label="link" class="text-gray-light">Nature Masterclasses</a></li>
                    <li class="pb4"><a href="https://partnerships.nature.com/product/researcher-training/" data-track="click" data-track-action="nature research academies" data-track-label="link" class="text-gray-light">Nature Research Academies</a></li>
                </ul>
            </div>

            <div class="grid grid-3 mq875-grid-4 mq640-grid-6 mq480-grid-12 just-mq875-clear just-mq640-last full-size-last just-mq1200-last just-mq875-last just-mq480-last">
                <h3 class="text-footer-heading">Libraries &amp; institutions</h3>
                <ul class="clean-list ma0 mb6 text15">
                    <li class="pb4"><a href="https://www.springernature.com/gp/librarians/tools-services" data-track="click" data-track-action="librarian service and tools" data-track-label="link" class="text-gray-light">Librarian service &amp; tools</a></li>
                    <li class="pb4"><a href="https://www.springernature.com/gp/librarians/manage-your-account/librarianportal" data-track="click" data-track-action="librarian portal" data-track-label="link" class="text-gray-light">Librarian portal</a></li>
                    <li class="pb4"><a href="http://www.nature.com/openresearch/about-open-access/information-for-institutions/" data-track="click" data-track-action="open research" data-track-label="link" class="text-gray-light">Open research</a></li>
                </ul>
            </div>
        </div>

        <div class="cleared z-index-1 pt30 pb20 border-top-1 border-gray text-footer-mobile-top hide-print u-hide-print">
            <div class="grid grid-3 mq875-grid-4 mq640-grid-6 mq480-grid-12">
                <h3 class="text-footer-heading">Advertising &amp; partnerships</h3>
                <ul class="clean-list ma0 mb6 text15">
                    <li class="pb4"><a href="https://partnerships.nature.com/product/digital-advertising/" data-track="click" data-track-action="advertising" data-track-label="link" class="text-gray-light">Advertising</a></li>
                    <li class="pb4"><a href="https://partnerships.nature.com/" data-track="click" data-track-action="partnerships and services" data-track-label="link" class="text-gray-light">Partnerships &amp; Services</a></li>
                    <li class="pb4"><a href="https://partnerships.nature.com/media-kits/" data-track="click" data-track-action="media kits" data-track-label="link" class="text-gray-light">Media kits</a></li>
                    <li class="pb4"><a href="https://partnerships.nature.com/product/branded-content-native-advertising/" data-track-action="branded content" data-track-label="link" class="text-gray-light">Branded content</a></li>
                </ul>
            </div>

            <div class="grid grid-3 mq875-grid-4 mq640-grid-6 mq480-grid-12 just-mq640-last just-mq875-last">
                <h3 class="text-footer-heading">Career development</h3>
                <ul class="clean-list ma0 mb6 text15">
                    <li class="pb4"><a href="https://www.nature.com/naturecareers" data-track="click" data-track-action="nature careers" data-track-label="link" class="text-gray-light">Nature Careers</a></li>
                    <li class="pb4"><a href="https://www.nature.com/natureconferences/" data-track="click" data-track-action="nature conferences" data-track-label="link" class="text-gray-light">Nature<span class="visually-hidden"> </span> Conferences</a></li>
                    <li class="pb4"><a href="https://www.nature.com/natureevents/" data-track="click" data-track-action="nature events" data-track-label="link" class="text-gray-light">Nature<span class="visually-hidden"> </span> events</a></li>
                </ul>
            </div>

            <div class="grid grid-3 mq875-grid-4 mq640-grid-6 mq480-grid-12">
                <h3 class="text-footer-heading">Regional websites</h3>
                <ul class="clean-list ma0 mb6 text15">
                    <li class="pb4"><a href="http://www.naturechina.com" data-track="click" data-track-action="nature china" data-track-label="link" class="text-gray-light">Nature China</a></li>
                    <li class="pb4"><a href="https://www.nature.com/nindia" data-track="click" data-track-action="nature india" data-track-label="link" class="text-gray-light">Nature India</a></li>
                    <li class="pb4"><a href="https://www.natureasia.com/ja-jp/" data-track="click" data-track-action="nature japan" data-track-label="link" class="text-gray-light">Nature Japan</a></li>
                    <li class="pb4"><a href="https://www.natureasia.com/ko-kr/" data-track="click" data-track-action="nature korea" data-track-label="link" class="text-gray-light">Nature Korea</a></li>
                    <li class="pb4"><a href="https://www.nature.com/nmiddleeast/" data-track="click" data-track-action="nature middle east" data-track-label="link" class="text-gray-light">Nature Middle East</a></li>
                </ul>
            </div>
        </div>
    </div>


            </div>
        
    </div>

    
    <div class="container-footer-box cleared text14 border-gray-medium border-top-1">
        <div class="content">
            <img src="/static/images/sn-logo.4368f3b177.svg" alt="Springer Nature" loading="lazy" width="140" height="14" class="cleared"/>
            <p class="mb10 text-gray-light">© 2020 Springer Nature Limited</p>
            <ul class="ma0 clean-list grid grid-12 hide-print u-hide-print">
                <li class="grid grid-2 mq875-grid-4 mq640-grid-6 mq480-grid-12 mb10 just-mq480-last"><a href="https://www.nature.com/info/privacy.html" data-track="click" data-track-action="privacy policy" data-track-label="link" class="text-gray-light">Privacy Policy</a></li>
                <li class="grid grid-2 mq875-grid-4 mq640-grid-6 mq480-grid-12 mb10 just-mq640-last just-mq480-last"><a href="https://www.nature.com/info/cookies.html" data-track="click" data-track-action="use of cookies" data-track-label="link" class="text-gray-light">Use of cookies</a></li>
                <li class="grid grid-2 mq875-grid-4 mq640-grid-6 mq480-grid-12 mb10 just-mq875-last just-mq480-last"><a href="javascript:;" class="optanon-toggle-display text-gray-light" data-track="click" data-track-action="manage cookies" data-track-label="link">Manage cookies</a></li>
                <li class="grid grid-2 mq875-grid-4 mq640-grid-6 mq480-grid-12 mb10 just-mq640-last just-mq480-last"><a href="https://www.nature.com/info/legal_notice.html" data-track="click" data-track-action="legal notice" data-track-label="link" class="text-gray-light">Legal notice</a></li>
                <li class="grid grid-2 mq875-grid-4 mq640-grid-6 mq480-grid-12 mb10 just-mq480-last"><a href="https://www.nature.com/info/accessibility_statement.html" data-track="click" data-track-action="accessibility statement" data-track-label="link" class="text-gray-light">Accessibility statement</a></li>
                <li class="grid grid-2 mq875-grid-4 mq640-grid-6 mq480-grid-12 mb10 last"><a href="https://www.nature.com/info/tandc.html" data-track="click" data-track-action="terms and conditions" data-track-label="link" class="text-gray-light">Terms &amp; Conditions</a></li>
            </ul>
        </div>
    </div>



    
    <svg class="u-hide hide">
        <symbol id="global-icon-chevron-right" viewBox="0 0 16 16">
            <path d="M7.782 7L5.3 4.518c-.393-.392-.4-1.022-.02-1.403a1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L7.782 9l1.013-.998z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-download" viewBox="0 0 16 16">
            <path d="M2 14c0-.556.449-1 1.002-1h9.996a.999.999 0 110 2H3.002A1.006 1.006 0 012 14zM9 2v6.8l2.482-2.482c.392-.392 1.022-.4 1.403-.02a1.001 1.001 0 010 1.417l-4.177 4.177a1.001 1.001 0 01-1.416 0L3.115 7.715a.991.991 0 01-.016-1.4 1 1 0 011.42.003L7 8.8V2c0-.55.444-.996 1-.996.552 0 1 .445 1 .996z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-email" viewBox="0 0 18 18">
            <path d="M1.995 2h14.01A2 2 0 0118 4.006v9.988A2 2 0 0116.005 16H1.995A2 2 0 010 13.994V4.006A2 2 0 011.995 2zM1 13.994A1 1 0 001.995 15h14.01A1 1 0 0017 13.994V4.006A1 1 0 0016.005 3H1.995A1 1 0 001 4.006zM9 11L2 7V5.557l7 4 7-4V7z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-institution" viewBox="0 0 18 18">
            <path d="M14 8a1 1 0 011 1v6h1.5a.5.5 0 01.5.5v.5h.5a.5.5 0 01.5.5V18H0v-1.5a.5.5 0 01.5-.5H1v-.5a.5.5 0 01.5-.5H3V9a1 1 0 112 0v6h8V9a1 1 0 011-1zM6 8l2 1v4l-2 1zm6 0v6l-2-1V9zM9.573.401l7.036 4.925A.92.92 0 0116.081 7H1.92a.92.92 0 01-.528-1.674L8.427.401a1 1 0 011.146 0zM9 2.441L5.345 5h7.31z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-search" viewBox="0 0 22 22">
            <path fill-rule="evenodd" d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z"/>
        </symbol>
    </svg>

</footer>




    
        <div class="c-site-messages message hide u-hide-print c-site-messages--nature-briefing c-site-messages--nature-briefing-email-variant sans-serif"
    data-component-id="nature-briefing-banner"
    data-component-expirydays="30"
    data-component-trigger-scroll-percentage="15"
    data-track="in-view"
    data-track-action="in-view"
    data-track-category="nature briefing"
    data-track-label="banner visible">

    <div class="">

        
        <div class="c-site-messages__banner-large">

            <div class="c-site-messages__close-container">
                <button class="c-site-messages__close"
                    data-track="click"
                    data-track-category="nature briefing"
                    data-track-label="banner dismiss">
                    <?xml version="1.0" encoding="UTF-8" standalone="no"?>
                    <svg width="24px" height="24px" focusable="false" aria-hidden="true" viewBox="0 0 24 24" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
                        <title>Close banner</title>
                        <defs></defs>
                        <g stroke="none" stroke-width="1" fill="none" fill-rule="evenodd">
                            <rect opacity="0" x="0" y="0" width="24" height="24"></rect>
                            <path d="M6.29679575,16.2772478 C5.90020818,16.6738354 5.90240728,17.3100587 6.29617427,17.7038257 C6.69268654,18.100338 7.32864195,18.0973145 7.72275218,17.7032043 L12,13.4259564 L16.2772478,17.7032043 C16.6738354,18.0997918 17.3100587,18.0975927 17.7038257,17.7038257 C18.100338,17.3073135 18.0973145,16.671358 17.7032043,16.2772478 L13.4259564,12 L17.7032043,7.72275218 C18.0997918,7.32616461 18.0975927,6.68994127 17.7038257,6.29617427 C17.3073135,5.89966201 16.671358,5.90268552 16.2772478,6.29679575 L12,10.5740436 L7.72275218,6.29679575 C7.32616461,5.90020818 6.68994127,5.90240728 6.29617427,6.29617427 C5.89966201,6.69268654 5.90268552,7.32864195 6.29679575,7.72275218 L10.5740436,12 L6.29679575,16.2772478 Z" fill="#006699"></path>
                        </g>
                    </svg>
                    <span class="visually-hidden">Close</span>
                </button>
            </div>

            <div class="c-site-messages__form-container">
                <form action="/briefing/signup/formfeedback" method="post" class="nature-briefing-banner__form box-sizing grid grid-12 last" data-location="banner" data-track="submit" data-track-action="transmit-form">
                    <div class="grid grid-6">
                        <div class="text13 grid grid-7">
                            <span class="block strong extra-tight-line-height sans-serif">
                                
                                Sign up for the <em>Nature Briefing</em> newsletter for a daily update on COVID-19 science.
                                
                            </span>
                        </div>

                        <div class="grid grid-5 last">
                            <input id="briefing-banner-signup-form-input-track-originReferralPoint" type="hidden" name="track_originReferralPoint" value="DirectEmailBannerCovid19">
                            <input id="briefing-banner-signup-form-input-track-formType" type="hidden" name="track_formType" value="DirectEmailBanner">
                            <label for="banner-EmailAddressInput" class="visually-hidden">Enter your email address</label>
                            <input class="nature-briefing-banner__email-input border-all-1 equalize-line-height pa10 box-sizing text13" type="email" id="banner-EmailAddressInput" name="email" value="" placeholder="Enter your email address" required="true" aria-required="true" data-test-element="briefing-emailbanner-email-input">
                        </div>
                    </div>

                    <div class="grid grid-6 last">
                        <div class="grid grid-9">
                            <div class="grid grid-12 last">
                                <input class="nature-briefing-banner__checkbox-checkbox" id="gdpr-briefing-banner-checkbox" type="checkbox" name="gdpr" value="1" data-test-element="briefing-emailbanner-gdpr-checkbox" required>
                                <label class="nature-briefing-banner__checkbox-label box-sizing text13 sans-serif block tighten-line-height" for="gdpr-briefing-banner-checkbox">I agree my information will be processed in accordance with the <em>Nature</em> and Springer Nature Limited <a href="https://www.nature.com/info/privacy">Privacy Policy</a>.</label>
                            </div>
                        </div>

                        <div class="grid grid-3 last">
                            <button type="submit" class="nature-briefing-banner__submit-button box-sizing" data-test-element="briefing-emailbanner-signup-button">Sign up</button>
                        </div>
                    </div>
                </form>
            </div>

        </div>

        
        <div class="c-site-messages__banner-small">

            <div class="c-site-messages__content text14">
                <span class="strong serif">Get the most important science stories of the day, free in your inbox.</span>
                <a class="nature-briefing__link text14 sans-serif"
                    data-track="click"
                    data-track-category="nature briefing"
                    data-track-label="banner CTA to site"
                    data-test-element="briefing-banner-link"
                    target="_blank"
                    rel="noreferrer noopener"
                    href="/briefing/signup/?origin=Nature&amp;originReferralPoint=EmailBanner">Sign up for Nature Briefing
                </a>
            </div>
            <div class="c-site-messages__close-container pin-right">
                <button class="c-site-messages__close"
                    data-track="click"
                    data-track-category="nature briefing"
                    data-track-label="banner dismiss">
                    <span class="icon--inline inline-block"><?xml version="1.0" encoding="UTF-8" standalone="no"?>
                        <svg width="24px" height="24px" focusable="false" aria-hidden="true" viewBox="0 0 24 24" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
                            <title>Close banner</title>
                            <defs></defs>
                            <g stroke="none" stroke-width="1" fill="none" fill-rule="evenodd">
                                <rect opacity="0" x="0" y="0" width="24" height="24"></rect>
                                <path d="M6.29679575,16.2772478 C5.90020818,16.6738354 5.90240728,17.3100587 6.29617427,17.7038257 C6.69268654,18.100338 7.32864195,18.0973145 7.72275218,17.7032043 L12,13.4259564 L16.2772478,17.7032043 C16.6738354,18.0997918 17.3100587,18.0975927 17.7038257,17.7038257 C18.100338,17.3073135 18.0973145,16.671358 17.7032043,16.2772478 L13.4259564,12 L17.7032043,7.72275218 C18.0997918,7.32616461 18.0975927,6.68994127 17.7038257,6.29617427 C17.3073135,5.89966201 16.671358,5.90268552 16.2772478,6.29679575 L12,10.5740436 L7.72275218,6.29679575 C7.32616461,5.90020818 6.68994127,5.90240728 6.29617427,6.29617427 C5.89966201,6.69268654 5.90268552,7.32864195 6.29679575,7.72275218 L10.5740436,12 L6.29679575,16.2772478 Z" fill="#006699"></path>
                            </g>
                        </svg>
                    </span>
                    <span class="visually-hidden">Close</span>
                </button>
            </div>

        </div>

    </div>

</div>

    






<noscript>
    <img hidden src="https://verify.nature.com/verify/nature.png" border="0" width="0" height="0" style="display: none" alt="">
</noscript>




<script src="//content.readcube.com/ping?doi=10.1038/s42256-019-0025-4&amp;format=js&amp;last_modified=2019-03-04" async></script>
<img src="/platform/track/article/s42256-019-0025-4" width="1" height="1" alt="" class="visually-hidden"/>

</body>
</html>
