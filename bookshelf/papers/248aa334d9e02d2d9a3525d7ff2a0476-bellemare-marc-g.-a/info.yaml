abstract: 'In this paper we argue for the fundamental importance of the value distribution:
  the distribution of the random return received by a reinforcement learning agent.
  This is in contrast to the common approach to reinforcement learning which models
  the expectation of this return, or value. Although there is an established body
  of literature studying the value distribution, thus far it has always been used
  for a specific purpose such as implementing risk-aware behaviour. We begin with
  theoretical results in both the policy evaluation and control settings, exposing
  a significant distributional instability in the latter. We then use the distributional
  perspective to design a new algorithm which applies Bellmanâ€™s equation to the learning
  of approximate value distributions. We evaluate our algorithm using the suite of
  games from the Arcade Learning Environment. We obtain both state-of-the-art results
  and anecdotal evidence demonstrating the importance of the value distribution in
  approximate reinforcement learning. Finally, we combine theoretical and empirical
  evidence to highlight the ways in which the value distribution impacts learning
  in the approximate setting.'
address: International Convention Centre, Sydney, Australia
author: Bellemare, Marc G. and Dabney, Will and Munos, R{\'e}mi
author_list:
- family: Bellemare
  given: Marc G.
- family: Dabney
  given: Will
- family: Munos
  given: R{\'e}mi
booktitle: Proceedings of the 34th International Conference on Machine Learning
editor: Doina Precup and Yee Whye Teh
month: 06--11 Aug
pages: 449--458
pdf: http://proceedings.mlr.press/v70/bellemare17a/bellemare17a.pdf
publisher: PMLR
ref: pmlr-v70-bellemare17a
series: Proceedings of Machine Learning Research
title: A Distributional Perspective on Reinforcement Learning
type: inproceedings
url: http://proceedings.mlr.press/v70/bellemare17a.html
volume: '70'
year: '2017'
