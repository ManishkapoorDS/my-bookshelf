abstract: Representation learning and option discovery are two of the biggest challenges
  in reinforcement learning (RL). Proto-value functions (PVFs) are a well-known approach
  for representation learning in MDPs. In this paper we address the option discovery
  problem by showing how PVFs implicitly define options. We do it by introducing eigenpurposes,
  intrinsic reward functions derived from the learned representations. The options
  discovered from eigenpurposes traverse the principal directions of the state space.
  They are useful for multiple tasks because they are discovered without taking the
  environment's rewards into consideration. Moreover, different options act at different
  time scales, making them helpful for exploration. We demonstrate features of eigenpurposes
  in traditional tabular domains as well as in Atari 2600 games.
archiveprefix: arXiv
author: Machado, Marlos C. and Bellemare, Marc G. and Bowling, Michael
author_list:
- family: Machado
  given: Marlos C.
- family: Bellemare
  given: Marc G.
- family: Bowling
  given: Michael
eprint: 1703.00956v2
file: 1703.00956v2.pdf
files:
- machado-marlos-c.-and-bellemare-marc-g.-and-bowling-michaela-laplacian-framework-for-option-discovery-in-reinforcement-learning2017.pdf
month: Mar
primaryclass: cs.LG
ref: 1703.00956v2
tags: deep-reinforcement-learning reinforcement-learning
title: A Laplacian Framework for Option Discovery in Reinforcement Learning
type: article
url: http://arxiv.org/abs/1703.00956v2
year: '2017'
